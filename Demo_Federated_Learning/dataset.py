import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import TensorDataset, Subset
from collections import Counter

# âœ… Tráº£ vá» sá»‘ lá»›p chuáº©n
def get_num_classes():
    df = pd.read_csv("IoTDIAD_sum.csv")
    print("CÃ¡c cá»™t trong file:", df.columns.tolist())  # In ra tÃªn cÃ¡c cá»™t Ä‘á»ƒ debug
    if "Label" in df.columns:
        labels = LabelEncoder().fit_transform(df["Label"])
        return len(set(labels))
    else:
        raise KeyError("KhÃ´ng tÃ¬m tháº¥y cá»™t 'Label' trong file CSV. CÃ¡c cá»™t hiá»‡n cÃ³: {}".format(df.columns.tolist()))

# âœ… Load dataset chuáº©n
def load_dataset(k_features=60, test_size=0.2, random_state=42):
    df = pd.read_csv("IoTDIAD_sum.csv")

    numeric_cols = [col for col in df.columns if df[col].dtype in ['int64', 'float64']]
    categorical_cols = [col for col in df.columns if col not in numeric_cols]
    print("Numeric columns:", numeric_cols)
    print("Categorical columns:", categorical_cols)
    
    object_cols = df.select_dtypes(include=['object']).columns.tolist()
    if 'Label' in object_cols:
        object_cols.remove('Label')
    for col in object_cols:
        df[col] = df[col].astype('category').cat.codes
    # MÃ£ hÃ³a nhÃ£n
    le = LabelEncoder()
    df["Label"] = le.fit_transform(df["Label"])
    # Xá»­ lÃ½ thá»i gian
    df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')
    df = df.sort_values(by='Timestamp').reset_index(drop=True)
    for col in ["Year","Month","Day","Hour","Minute","Second"]:
        df[col] = getattr(df["Timestamp"].dt, col.lower())
    df.drop(columns=["Timestamp"], inplace=True)
    object_cols = df.select_dtypes(include=['object']).columns.tolist()
    if 'type' in object_cols:
        object_cols.remove('type')

    print("--- Báº¯t Ä‘áº§u Ä‘iá»u tra cÃ¡c cá»™t 'object' ---")

    # Láº·p qua Tá»ªNG Cá»˜T trong danh sÃ¡ch
    for col in object_cols:
        # Cá»‘ gáº¯ng chuyá»ƒn Ä‘á»•i cá»™t hiá»‡n táº¡i thÃ nh sá»‘
        numeric_series = pd.to_numeric(df[col], errors='coerce')

        # Kiá»ƒm tra xem cÃ³ giÃ¡ trá»‹ NaN nÃ o Ä‘Æ°á»£c táº¡o ra khÃ´ng
        if numeric_series.isna().any():
            print(f"\n[!] Cá»˜T CÃ“ Váº¤N Äá»€: '{col}'")

            # TÃ¬m nhá»¯ng hÃ ng cÃ³ giÃ¡ trá»‹ gÃ¢y lá»—i
            problematic_rows = df[numeric_series.isna()]

            # In ra nhá»¯ng giÃ¡ trá»‹ rÃ¡c duy nháº¥t trong cá»™t Ä‘Ã³
            garbage_values = problematic_rows[col].unique()
            print(f"    -> CÃ¡c giÃ¡ trá»‹ rÃ¡c tÃ¬m tháº¥y: {garbage_values}")
        else:
            # Náº¿u khÃ´ng cÃ³ lá»—i, cá»™t nÃ y sáº¡ch
            print(f"\n[âœ“] Cá»™t '{col}' sáº¡ch, cÃ³ thá»ƒ chuyá»ƒn thÃ nh sá»‘.")

    print("\n--- Äiá»u tra hoÃ n táº¥t ---")

    X = df.drop(columns=['Label']).values
    y = df['Label'].values
    X = np.nan_to_num(X)
    X = np.clip(X, -1e10, 1e10)
    # Chia train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, stratify=y, random_state=random_state
    )
    # Chuáº©n hÃ³a
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    # ANOVA chá»n Ä‘áº·c trÆ°ng
    selector = SelectKBest(f_classif, k=min(k_features, X.shape[1]))
    X_train = selector.fit_transform(X_train, y_train)
    X_test = selector.transform(X_test)
    trainset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                             torch.tensor(y_train, dtype=torch.long))
    testset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),
                            torch.tensor(y_test, dtype=torch.long))
    return trainset, testset, len(np.unique(y))

# âœ… Partition Non-IID vá»›i Dirichlet
def partition_noniid(trainset, num_clients=5, num_classes=None, alpha=1, seed=42):
    np.random.seed(seed)
    labels = np.array([y.item() for _, y in trainset])
    idxs = np.arange(len(labels))
    if num_classes is None:
        num_classes = len(np.unique(labels))
    class_indices = [idxs[labels == c] for c in range(num_classes)]
    
    client_dict = {i: [] for i in range(num_clients)}

    for c in range(num_classes):
        np.random.shuffle(class_indices[c])
        proportions = np.random.dirichlet(alpha * np.ones(num_clients))
        split_points = (np.cumsum(proportions) * len(class_indices[c])).astype(int)
        split_class = np.split(class_indices[c], split_points[:-1])
        for cid, idx_split in enumerate(split_class):
            client_dict[cid].extend(idx_split)

    # ğŸ“Œ Thá»‘ng kÃª dá»¯ liá»‡u cho tá»«ng client
    print("\nğŸ“Š [Non-IID Partition Statistics]")
    for cid in client_dict:
        client_labels = labels[client_dict[cid]]
        unique, counts = np.unique(client_labels, return_counts=True)
        total = len(client_labels)
        dist = {int(u): round(c/total, 3) for u, c in zip(unique, counts)}
        print(f"Client {cid}: {total} samples | class distribution: {dist}")

    for cid in client_dict:
        np.random.shuffle(client_dict[cid])
    return client_dict


# âœ… Load partition theo client
def load_partition(client_id, num_clients=5, k_features=60, noniid=False, alpha=0.5):
    trainset, testset, num_classes = load_dataset(k_features=k_features)
    
    if noniid:
        parts = partition_noniid(trainset, num_clients=num_clients, num_classes=num_classes, alpha=alpha)
        client_trainset = Subset(trainset, parts[client_id])
    else:
        all_idx = np.arange(len(trainset))
        split = np.array_split(all_idx, num_clients)
        client_trainset = Subset(trainset, split[client_id])

    # âœ… Äáº¿m sá»‘ lÆ°á»£ng record vÃ  tá»· lá»‡ class
    labels = [trainset[i][1].item() for i in client_trainset.indices]
    class_dist = Counter(labels)
    total = len(labels)
    print(f"\nğŸ“Š [Client {client_id}] Records: {total}")
    for c, count in class_dist.items():
        print(f"  - Class {c}: {count} ({count/total:.2%})")
    
    return client_trainset, testset


