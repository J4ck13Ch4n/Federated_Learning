{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07552f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from diffusers import UNet2DModel, DDIMScheduler, DDPMScheduler, LMSDiscreteScheduler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aa0b9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flow ID</th>\n",
       "      <th>Src IP</th>\n",
       "      <th>Src Port</th>\n",
       "      <th>Dst IP</th>\n",
       "      <th>Dst Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packet</th>\n",
       "      <th>Total Bwd packets</th>\n",
       "      <th>...</th>\n",
       "      <th>Fwd Seg Size Min</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192.168.137.66-192.168.137.174-41082-80-6</td>\n",
       "      <td>192.168.137.66</td>\n",
       "      <td>41082.0</td>\n",
       "      <td>192.168.137.174</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>09/08/2022 11:27:38 AM</td>\n",
       "      <td>1527173.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192.168.137.66-192.168.137.254-55598-34287-6</td>\n",
       "      <td>192.168.137.66</td>\n",
       "      <td>55598.0</td>\n",
       "      <td>192.168.137.254</td>\n",
       "      <td>34287.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>09/08/2022 11:56:16 AM</td>\n",
       "      <td>9912071.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192.168.137.174-192.168.137.66-80-47994-6</td>\n",
       "      <td>192.168.137.174</td>\n",
       "      <td>80.0</td>\n",
       "      <td>192.168.137.66</td>\n",
       "      <td>47994.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>09/08/2022 11:29:10 AM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192.168.137.66-192.168.137.254-59336-8009-6</td>\n",
       "      <td>192.168.137.66</td>\n",
       "      <td>59336.0</td>\n",
       "      <td>192.168.137.254</td>\n",
       "      <td>8009.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>09/08/2022 11:50:16 AM</td>\n",
       "      <td>349868.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192.168.137.66-192.168.137.254-55662-8009-6</td>\n",
       "      <td>192.168.137.66</td>\n",
       "      <td>55662.0</td>\n",
       "      <td>192.168.137.254</td>\n",
       "      <td>8009.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>09/08/2022 11:49:16 AM</td>\n",
       "      <td>215841.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33613</th>\n",
       "      <td>192.168.137.79-255.255.255.255-49154-6667-17</td>\n",
       "      <td>192.168.137.79</td>\n",
       "      <td>49154.0</td>\n",
       "      <td>255.255.255.255</td>\n",
       "      <td>6667.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>07/10/2022 03:06:15 PM</td>\n",
       "      <td>119995219.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.165906e+07</td>\n",
       "      <td>6.061784e+06</td>\n",
       "      <td>19997148.0</td>\n",
       "      <td>4965723.0</td>\n",
       "      <td>5006391.25</td>\n",
       "      <td>13272.949739</td>\n",
       "      <td>5038906.0</td>\n",
       "      <td>5000281.0</td>\n",
       "      <td>BenignTraffic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33614</th>\n",
       "      <td>157.249.81.141-192.168.137.41-80-51746-6</td>\n",
       "      <td>157.249.81.141</td>\n",
       "      <td>80.0</td>\n",
       "      <td>192.168.137.41</td>\n",
       "      <td>51746.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>07/10/2022 06:00:51 PM</td>\n",
       "      <td>126325.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BenignTraffic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33615</th>\n",
       "      <td>192.168.137.186-192.168.137.1-42090-53-17</td>\n",
       "      <td>192.168.137.186</td>\n",
       "      <td>42090.0</td>\n",
       "      <td>192.168.137.1</td>\n",
       "      <td>53.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>07/10/2022 05:16:07 PM</td>\n",
       "      <td>85450.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BenignTraffic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33616</th>\n",
       "      <td>205.174.165.69-192.168.137.187-24421-63908-17</td>\n",
       "      <td>205.174.165.69</td>\n",
       "      <td>24421.0</td>\n",
       "      <td>192.168.137.187</td>\n",
       "      <td>63908.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>07/10/2022 09:11:08 PM</td>\n",
       "      <td>1115118.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BenignTraffic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33617</th>\n",
       "      <td>192.168.137.249-8.8.4.4-53-50021-17</td>\n",
       "      <td>192.168.137.249</td>\n",
       "      <td>53.0</td>\n",
       "      <td>8.8.4.4</td>\n",
       "      <td>50021.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>07/10/2022 03:35:59 PM</td>\n",
       "      <td>11998700.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BenignTraffic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33618 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Flow ID           Src IP  \\\n",
       "0          192.168.137.66-192.168.137.174-41082-80-6   192.168.137.66   \n",
       "1       192.168.137.66-192.168.137.254-55598-34287-6   192.168.137.66   \n",
       "2          192.168.137.174-192.168.137.66-80-47994-6  192.168.137.174   \n",
       "3        192.168.137.66-192.168.137.254-59336-8009-6   192.168.137.66   \n",
       "4        192.168.137.66-192.168.137.254-55662-8009-6   192.168.137.66   \n",
       "...                                              ...              ...   \n",
       "33613   192.168.137.79-255.255.255.255-49154-6667-17   192.168.137.79   \n",
       "33614       157.249.81.141-192.168.137.41-80-51746-6   157.249.81.141   \n",
       "33615      192.168.137.186-192.168.137.1-42090-53-17  192.168.137.186   \n",
       "33616  205.174.165.69-192.168.137.187-24421-63908-17   205.174.165.69   \n",
       "33617            192.168.137.249-8.8.4.4-53-50021-17  192.168.137.249   \n",
       "\n",
       "       Src Port           Dst IP  Dst Port  Protocol               Timestamp  \\\n",
       "0       41082.0  192.168.137.174      80.0       6.0  09/08/2022 11:27:38 AM   \n",
       "1       55598.0  192.168.137.254   34287.0       6.0  09/08/2022 11:56:16 AM   \n",
       "2          80.0   192.168.137.66   47994.0       6.0  09/08/2022 11:29:10 AM   \n",
       "3       59336.0  192.168.137.254    8009.0       6.0  09/08/2022 11:50:16 AM   \n",
       "4       55662.0  192.168.137.254    8009.0       6.0  09/08/2022 11:49:16 AM   \n",
       "...         ...              ...       ...       ...                     ...   \n",
       "33613   49154.0  255.255.255.255    6667.0      17.0  07/10/2022 03:06:15 PM   \n",
       "33614      80.0   192.168.137.41   51746.0       6.0  07/10/2022 06:00:51 PM   \n",
       "33615   42090.0    192.168.137.1      53.0      17.0  07/10/2022 05:16:07 PM   \n",
       "33616   24421.0  192.168.137.187   63908.0      17.0  07/10/2022 09:11:08 PM   \n",
       "33617      53.0          8.8.4.4   50021.0      17.0  07/10/2022 03:35:59 PM   \n",
       "\n",
       "       Flow Duration  Total Fwd Packet  Total Bwd packets  ...  \\\n",
       "0          1527173.0              16.0                1.0  ...   \n",
       "1          9912071.0               7.0                2.0  ...   \n",
       "2                0.0               2.0                0.0  ...   \n",
       "3           349868.0               1.0                1.0  ...   \n",
       "4           215841.0               1.0                1.0  ...   \n",
       "...              ...               ...                ...  ...   \n",
       "33613    119995219.0              25.0                0.0  ...   \n",
       "33614       126325.0               4.0                0.0  ...   \n",
       "33615        85450.0               2.0                2.0  ...   \n",
       "33616      1115118.0               7.0                0.0  ...   \n",
       "33617     11998700.0               5.0                0.0  ...   \n",
       "\n",
       "       Fwd Seg Size Min   Active Mean    Active Std  Active Max  Active Min  \\\n",
       "0                  20.0  0.000000e+00  0.000000e+00         0.0         0.0   \n",
       "1                  32.0  0.000000e+00  0.000000e+00         0.0         0.0   \n",
       "2                  20.0  0.000000e+00  0.000000e+00         0.0         0.0   \n",
       "3                  40.0  0.000000e+00  0.000000e+00         0.0         0.0   \n",
       "4                  40.0  0.000000e+00  0.000000e+00         0.0         0.0   \n",
       "...                 ...           ...           ...         ...         ...   \n",
       "33613               8.0  1.165906e+07  6.061784e+06  19997148.0   4965723.0   \n",
       "33614              32.0  0.000000e+00  0.000000e+00         0.0         0.0   \n",
       "33615               8.0  0.000000e+00  0.000000e+00         0.0         0.0   \n",
       "33616               8.0  0.000000e+00  0.000000e+00         0.0         0.0   \n",
       "33617               8.0  0.000000e+00  0.000000e+00         0.0         0.0   \n",
       "\n",
       "        Idle Mean      Idle Std   Idle Max   Idle Min          label  \n",
       "0            0.00      0.000000        0.0        0.0            DoS  \n",
       "1            0.00      0.000000        0.0        0.0            DoS  \n",
       "2            0.00      0.000000        0.0        0.0            DoS  \n",
       "3            0.00      0.000000        0.0        0.0            DoS  \n",
       "4            0.00      0.000000        0.0        0.0            DoS  \n",
       "...           ...           ...        ...        ...            ...  \n",
       "33613  5006391.25  13272.949739  5038906.0  5000281.0  BenignTraffic  \n",
       "33614        0.00      0.000000        0.0        0.0  BenignTraffic  \n",
       "33615        0.00      0.000000        0.0        0.0  BenignTraffic  \n",
       "33616        0.00      0.000000        0.0        0.0  BenignTraffic  \n",
       "33617        0.00      0.000000        0.0        0.0  BenignTraffic  \n",
       "\n",
       "[33618 rows x 84 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"IoTDIAD.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64470b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e192baa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DoS', 'Mirai', 'Recon', 'Spoofing', 'Web-based', 'Brute_Force',\n",
       "       'DDoS', 'BenignTraffic'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6612fe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (33618, 84)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Shape:', df.shape)\n",
    "df.head()\n",
    "df.isnull().sum()  # Kiểm tra missing values\n",
    "df['label'].value_counts().plot(kind='bar')\n",
    "plt.title('Label Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3176090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33618 entries, 0 to 33617\n",
      "Data columns (total 89 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Flow ID                     33618 non-null  int16  \n",
      " 1   Src IP                      33618 non-null  int16  \n",
      " 2   Src Port                    33618 non-null  float64\n",
      " 3   Dst IP                      33618 non-null  int16  \n",
      " 4   Dst Port                    33618 non-null  float64\n",
      " 5   Protocol                    33618 non-null  float64\n",
      " 6   Flow Duration               33618 non-null  float64\n",
      " 7   Total Fwd Packet            33618 non-null  float64\n",
      " 8   Total Bwd packets           33618 non-null  float64\n",
      " 9   Total Length of Fwd Packet  33618 non-null  float64\n",
      " 10  Total Length of Bwd Packet  33618 non-null  float64\n",
      " 11  Fwd Packet Length Max       33618 non-null  float64\n",
      " 12  Fwd Packet Length Min       33618 non-null  float64\n",
      " 13  Fwd Packet Length Mean      33618 non-null  float64\n",
      " 14  Fwd Packet Length Std       33618 non-null  float64\n",
      " 15  Bwd Packet Length Max       33618 non-null  float64\n",
      " 16  Bwd Packet Length Min       33618 non-null  float64\n",
      " 17  Bwd Packet Length Mean      33618 non-null  float64\n",
      " 18  Bwd Packet Length Std       33618 non-null  float64\n",
      " 19  Flow Bytes/s                33378 non-null  float64\n",
      " 20  Flow Packets/s              33618 non-null  float64\n",
      " 21  Flow IAT Mean               33618 non-null  float64\n",
      " 22  Flow IAT Std                33618 non-null  float64\n",
      " 23  Flow IAT Max                33618 non-null  float64\n",
      " 24  Flow IAT Min                33618 non-null  float64\n",
      " 25  Fwd IAT Total               33618 non-null  float64\n",
      " 26  Fwd IAT Mean                33618 non-null  float64\n",
      " 27  Fwd IAT Std                 33618 non-null  float64\n",
      " 28  Fwd IAT Max                 33618 non-null  float64\n",
      " 29  Fwd IAT Min                 33618 non-null  float64\n",
      " 30  Bwd IAT Total               33618 non-null  float64\n",
      " 31  Bwd IAT Mean                33618 non-null  float64\n",
      " 32  Bwd IAT Std                 33618 non-null  float64\n",
      " 33  Bwd IAT Max                 33618 non-null  float64\n",
      " 34  Bwd IAT Min                 33618 non-null  float64\n",
      " 35  Fwd PSH Flags               33618 non-null  float64\n",
      " 36  Bwd PSH Flags               33618 non-null  float64\n",
      " 37  Fwd URG Flags               33618 non-null  float64\n",
      " 38  Bwd URG Flags               33618 non-null  float64\n",
      " 39  Fwd Header Length           33618 non-null  float64\n",
      " 40  Bwd Header Length           33618 non-null  float64\n",
      " 41  Fwd Packets/s               33618 non-null  float64\n",
      " 42  Bwd Packets/s               33618 non-null  float64\n",
      " 43  Packet Length Min           33618 non-null  float64\n",
      " 44  Packet Length Max           33618 non-null  float64\n",
      " 45  Packet Length Mean          33618 non-null  float64\n",
      " 46  Packet Length Std           33618 non-null  float64\n",
      " 47  Packet Length Variance      33618 non-null  float64\n",
      " 48  FIN Flag Count              33618 non-null  float64\n",
      " 49  SYN Flag Count              33618 non-null  float64\n",
      " 50  RST Flag Count              33618 non-null  float64\n",
      " 51  PSH Flag Count              33618 non-null  float64\n",
      " 52  ACK Flag Count              33618 non-null  float64\n",
      " 53  URG Flag Count              33618 non-null  float64\n",
      " 54  CWR Flag Count              33618 non-null  float64\n",
      " 55  ECE Flag Count              33618 non-null  float64\n",
      " 56  Down/Up Ratio               33618 non-null  float64\n",
      " 57  Average Packet Size         33618 non-null  float64\n",
      " 58  Fwd Segment Size Avg        33618 non-null  float64\n",
      " 59  Bwd Segment Size Avg        33618 non-null  float64\n",
      " 60  Fwd Bytes/Bulk Avg          33618 non-null  float64\n",
      " 61  Fwd Packet/Bulk Avg         33618 non-null  float64\n",
      " 62  Fwd Bulk Rate Avg           33618 non-null  float64\n",
      " 63  Bwd Bytes/Bulk Avg          33618 non-null  float64\n",
      " 64  Bwd Packet/Bulk Avg         33618 non-null  float64\n",
      " 65  Bwd Bulk Rate Avg           33618 non-null  float64\n",
      " 66  Subflow Fwd Packets         33618 non-null  float64\n",
      " 67  Subflow Fwd Bytes           33618 non-null  float64\n",
      " 68  Subflow Bwd Packets         33618 non-null  float64\n",
      " 69  Subflow Bwd Bytes           33618 non-null  float64\n",
      " 70  FWD Init Win Bytes          33618 non-null  float64\n",
      " 71  Bwd Init Win Bytes          33618 non-null  float64\n",
      " 72  Fwd Act Data Pkts           33618 non-null  float64\n",
      " 73  Fwd Seg Size Min            33618 non-null  float64\n",
      " 74  Active Mean                 33618 non-null  float64\n",
      " 75  Active Std                  33618 non-null  float64\n",
      " 76  Active Max                  33618 non-null  float64\n",
      " 77  Active Min                  33618 non-null  float64\n",
      " 78  Idle Mean                   33618 non-null  float64\n",
      " 79  Idle Std                    33618 non-null  float64\n",
      " 80  Idle Max                    33618 non-null  float64\n",
      " 81  Idle Min                    33618 non-null  float64\n",
      " 82  label                       33618 non-null  int64  \n",
      " 83  Year                        23946 non-null  float64\n",
      " 84  Month                       23946 non-null  float64\n",
      " 85  Day                         23946 non-null  float64\n",
      " 86  Hour                        23946 non-null  float64\n",
      " 87  Minute                      23946 non-null  float64\n",
      " 88  Second                      23946 non-null  float64\n",
      "dtypes: float64(85), int16(3), int64(1)\n",
      "memory usage: 22.3 MB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "df[\"Year\"] = df[\"Timestamp\"].dt.year\n",
    "df[\"Month\"] = df[\"Timestamp\"].dt.month\n",
    "df[\"Day\"] = df[\"Timestamp\"].dt.day\n",
    "df[\"Hour\"] = df[\"Timestamp\"].dt.hour\n",
    "df[\"Minute\"] = df[\"Timestamp\"].dt.minute\n",
    "df[\"Second\"] = df[\"Timestamp\"].dt.second\n",
    "\n",
    "df.drop(columns=[\"Timestamp\"], inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"label\"])\n",
    "\n",
    "df[\"Flow ID\"] = df[\"Flow ID\"].astype(\"category\").cat.codes\n",
    "df[\"Src IP\"] = df[\"Src IP\"].astype(\"category\").cat.codes\n",
    "df[\"Dst IP\"] = df[\"Dst IP\"].astype(\"category\").cat.codes\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "121af579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (assuming 'df' is already loaded)\n",
    "y = df['label'].values  # Convert to numpy array\n",
    "X = df.drop(columns=['label']).values  # Convert to numpy array\n",
    "\n",
    "# Handle NaN and infinite values\n",
    "X = np.nan_to_num(X, nan=0, posinf=0, neginf=0)\n",
    "X = np.clip(X, -1e10, 1e10)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Modify the get_diffusion_model function to include the new models\n",
    "def get_diffusion_model(model_type='UNet'):\n",
    "    if model_type == 'UNet':\n",
    "        return UNet2DModel(sample_size=32, in_channels=1, out_channels=1, layers_per_block=2,\n",
    "                           block_out_channels=(64, 128, 256),\n",
    "                           down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n",
    "                           up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\"),\n",
    "                           norm_num_groups=8)\n",
    "    elif model_type == 'DDIM':\n",
    "        return DDIMScheduler(num_train_timesteps=1000, beta_start=0.00085, beta_end=0.012, beta_schedule=\"linear\")\n",
    "    elif model_type == 'DDPM':\n",
    "        return DDPMScheduler(num_train_timesteps=1000, beta_start=0.00085, beta_end=0.012, beta_schedule=\"linear\")\n",
    "    elif model_type == 'SBGM':\n",
    "        # Placeholder for Score-Based Generative Models\n",
    "        return \"Score-Based Generative Model - Placeholder\"\n",
    "    elif model_type == 'LDM':\n",
    "        # Placeholder for Latent Diffusion Models (LDM)\n",
    "        return \"Latent Diffusion Model - Placeholder\"\n",
    "    elif model_type == 'ConditionalDiffusion':\n",
    "        # Placeholder for Conditional Diffusion Models\n",
    "        return \"Conditional Diffusion Model - Placeholder\"\n",
    "    elif model_type == 'EBM':\n",
    "        # Placeholder for Score-Matching Energy-Based Models (EBMs)\n",
    "        return \"Score-Matching Energy-Based Model - Placeholder\"\n",
    "    elif model_type == 'DiffAutoencoder':\n",
    "        # Placeholder for Diffusion-based Autoencoders\n",
    "        return \"Diffusion-based Autoencoder - Placeholder\"\n",
    "    elif model_type == 'VariationalDiffusion':\n",
    "        # Placeholder for Variational Diffusion Models\n",
    "        return \"Variational Diffusion Model - Placeholder\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid diffusion model type specified.\")\n",
    "\n",
    "# Function to generate synthetic samples using Diffusion models\n",
    "def generate_samples_with_diffusion(X_train, y_train, model_type='UNet', threshold=4800):\n",
    "    print(f\"\\n Number of samples before augmentation: {X_train.shape[0]}\\n\")\n",
    "\n",
    "    # Print sample count for each class before augmentation\n",
    "    class_counts_before = Counter(y_train)\n",
    "    for class_label, count in class_counts_before.items():\n",
    "        print(f\"   ➤ Class {class_label}: {count} samples before augmentation\")\n",
    "\n",
    "    model = get_diffusion_model(model_type)  # Load the selected diffusion model\n",
    "    X_augmented = []\n",
    "    y_augmented = []\n",
    "\n",
    "    for class_label, count in class_counts_before.items():\n",
    "        if count < threshold:\n",
    "            samples_needed = threshold - count  # Calculate how many samples are needed\n",
    "            noise = torch.randn((samples_needed, X_train.shape[1]))  # Generate synthetic noise\n",
    "            generated_samples = noise.numpy()  # Replace with actual model inference\n",
    "            X_augmented.extend(generated_samples)\n",
    "            y_augmented.extend([class_label] * samples_needed)\n",
    "\n",
    "    # Combine original and generated samples\n",
    "    X_train_augmented = np.vstack([X_train, np.array(X_augmented)])\n",
    "    y_train_augmented = np.hstack([y_train, np.array(y_augmented)])\n",
    "\n",
    "    print(f\"Number of samples after augmentation: {X_train_augmented.shape[0]}\")\n",
    "\n",
    "    # Print sample count for each class after augmentation\n",
    "    class_counts_after = Counter(y_train_augmented)\n",
    "    for class_label, count in class_counts_after.items():\n",
    "        print(f\"  Class {class_label}: {count} samples after augmentation\")\n",
    "\n",
    "    return X_train_augmented, y_train_augmented, class_counts_before, class_counts_after\n",
    "# Feature selection using ANOVA (f_classif)\n",
    "def feature_selection(X_train, X_test, y_train, k=30):\n",
    "    # Using ANOVA to select the best k features\n",
    "    selector = SelectKBest(f_classif, k=min(k, X_train.shape[1]))\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)  # Apply feature selection to the training data\n",
    "    X_test_selected = selector.transform(X_test)  # Apply feature selection to the test data\n",
    "    selected_columns = [i for i, support in enumerate(selector.get_support()) if support]  # Get the selected columns\n",
    "    print(f\"Selected features (columns): {selected_columns}\")  # Print the selected feature columns\n",
    "    return X_train_selected, X_test_selected, selected_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79cd7643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN model\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "def train_birnn(X_train, y_train, X_test, y_test, input_size, hidden_size, num_classes, num_epochs=2000):\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    model = BiRNN(input_size, hidden_size, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "\n",
    "        precision = precision_score(y_test, predicted, average=None, zero_division=0)\n",
    "        recall = recall_score(y_test, predicted, average=None, zero_division=0)\n",
    "        f1 = f1_score(y_test, predicted, average=None, zero_division=0)\n",
    "\n",
    "    return accuracy, precision, recall, f1, num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "735c11ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "11.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)  # Nếu ra None → bản CPU\n",
    "print(torch.backends.cudnn.enabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb0ea33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 1\n",
      "Device name: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61792dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      " Number of samples before augmentation: 26894\n",
      "\n",
      "   ➤ Class 0: 4800 samples before augmentation\n",
      "   ➤ Class 6: 3200 samples before augmentation\n",
      "   ➤ Class 4: 3200 samples before augmentation\n",
      "   ➤ Class 1: 2894 samples before augmentation\n",
      "   ➤ Class 5: 3200 samples before augmentation\n",
      "   ➤ Class 3: 3200 samples before augmentation\n",
      "   ➤ Class 2: 3200 samples before augmentation\n",
      "   ➤ Class 7: 3200 samples before augmentation\n",
      "Number of samples after augmentation: 38400\n",
      "  Class 0: 4800 samples after augmentation\n",
      "  Class 6: 4800 samples after augmentation\n",
      "  Class 4: 4800 samples after augmentation\n",
      "  Class 1: 4800 samples after augmentation\n",
      "  Class 5: 4800 samples after augmentation\n",
      "  Class 3: 4800 samples after augmentation\n",
      "  Class 2: 4800 samples after augmentation\n",
      "  Class 7: 4800 samples after augmentation\n",
      "Selected features (columns): [0, 1, 2, 3, 4, 5, 12, 13, 21, 23, 24, 26, 28, 29, 43, 45, 48, 50, 57, 58, 70, 73, 78, 80, 82, 83, 84, 85, 86, 87]\n",
      "Epoch [1/2000] | Train Acc: 12.69% | Test Acc: 41.20% | Loss: 2.0844\n",
      "Epoch [2/2000] | Train Acc: 32.44% | Test Acc: 48.47% | Loss: 2.0271\n",
      "Epoch [3/2000] | Train Acc: 37.81% | Test Acc: 51.32% | Loss: 1.9733\n",
      "Epoch [4/2000] | Train Acc: 39.84% | Test Acc: 54.16% | Loss: 1.9209\n",
      "Epoch [5/2000] | Train Acc: 41.79% | Test Acc: 54.09% | Loss: 1.8687\n",
      "Epoch [6/2000] | Train Acc: 42.22% | Test Acc: 54.05% | Loss: 1.8161\n",
      "Epoch [7/2000] | Train Acc: 42.13% | Test Acc: 54.36% | Loss: 1.7633\n",
      "Epoch [8/2000] | Train Acc: 42.17% | Test Acc: 54.86% | Loss: 1.7107\n",
      "Epoch [9/2000] | Train Acc: 42.61% | Test Acc: 55.68% | Loss: 1.6593\n",
      "Epoch [10/2000] | Train Acc: 43.20% | Test Acc: 56.65% | Loss: 1.6104\n",
      "Epoch [11/2000] | Train Acc: 43.72% | Test Acc: 57.72% | Loss: 1.5652\n",
      "Epoch [12/2000] | Train Acc: 44.39% | Test Acc: 58.76% | Loss: 1.5245\n",
      "Epoch [13/2000] | Train Acc: 45.06% | Test Acc: 59.99% | Loss: 1.4890\n",
      "Epoch [14/2000] | Train Acc: 45.82% | Test Acc: 62.02% | Loss: 1.4588\n",
      "Epoch [15/2000] | Train Acc: 47.04% | Test Acc: 63.65% | Loss: 1.4337\n",
      "Epoch [16/2000] | Train Acc: 48.31% | Test Acc: 64.66% | Loss: 1.4130\n",
      "Epoch [17/2000] | Train Acc: 48.95% | Test Acc: 65.51% | Loss: 1.3959\n",
      "Epoch [18/2000] | Train Acc: 49.45% | Test Acc: 66.55% | Loss: 1.3814\n",
      "Epoch [19/2000] | Train Acc: 50.23% | Test Acc: 67.30% | Loss: 1.3685\n",
      "Epoch [20/2000] | Train Acc: 50.81% | Test Acc: 68.19% | Loss: 1.3563\n",
      "Epoch [21/2000] | Train Acc: 51.30% | Test Acc: 69.65% | Loss: 1.3440\n",
      "Epoch [22/2000] | Train Acc: 52.16% | Test Acc: 70.46% | Loss: 1.3314\n",
      "Epoch [23/2000] | Train Acc: 52.83% | Test Acc: 71.21% | Loss: 1.3184\n",
      "Epoch [24/2000] | Train Acc: 53.45% | Test Acc: 71.95% | Loss: 1.3052\n",
      "Epoch [25/2000] | Train Acc: 54.08% | Test Acc: 72.56% | Loss: 1.2921\n",
      "Epoch [26/2000] | Train Acc: 54.74% | Test Acc: 73.26% | Loss: 1.2793\n",
      "Epoch [27/2000] | Train Acc: 55.36% | Test Acc: 73.69% | Loss: 1.2670\n",
      "Epoch [28/2000] | Train Acc: 55.80% | Test Acc: 74.45% | Loss: 1.2552\n",
      "Epoch [29/2000] | Train Acc: 56.41% | Test Acc: 75.57% | Loss: 1.2439\n",
      "Epoch [30/2000] | Train Acc: 57.10% | Test Acc: 76.52% | Loss: 1.2329\n",
      "Epoch [31/2000] | Train Acc: 57.78% | Test Acc: 77.48% | Loss: 1.2223\n",
      "Epoch [32/2000] | Train Acc: 58.40% | Test Acc: 77.83% | Loss: 1.2120\n",
      "Epoch [33/2000] | Train Acc: 58.86% | Test Acc: 78.29% | Loss: 1.2021\n",
      "Epoch [34/2000] | Train Acc: 59.14% | Test Acc: 78.64% | Loss: 1.1925\n",
      "Epoch [35/2000] | Train Acc: 59.42% | Test Acc: 78.88% | Loss: 1.1833\n",
      "Epoch [36/2000] | Train Acc: 59.58% | Test Acc: 79.07% | Loss: 1.1743\n",
      "Epoch [37/2000] | Train Acc: 59.71% | Test Acc: 79.43% | Loss: 1.1656\n",
      "Epoch [38/2000] | Train Acc: 60.00% | Test Acc: 79.61% | Loss: 1.1572\n",
      "Epoch [39/2000] | Train Acc: 60.25% | Test Acc: 80.04% | Loss: 1.1490\n",
      "Epoch [40/2000] | Train Acc: 60.57% | Test Acc: 80.37% | Loss: 1.1410\n",
      "Epoch [41/2000] | Train Acc: 60.64% | Test Acc: 80.56% | Loss: 1.1331\n",
      "Epoch [42/2000] | Train Acc: 60.76% | Test Acc: 80.67% | Loss: 1.1254\n",
      "Epoch [43/2000] | Train Acc: 60.88% | Test Acc: 80.62% | Loss: 1.1179\n",
      "Epoch [44/2000] | Train Acc: 60.98% | Test Acc: 80.98% | Loss: 1.1107\n",
      "Epoch [45/2000] | Train Acc: 61.16% | Test Acc: 81.11% | Loss: 1.1037\n",
      "Epoch [46/2000] | Train Acc: 61.30% | Test Acc: 81.32% | Loss: 1.0969\n",
      "Epoch [47/2000] | Train Acc: 61.53% | Test Acc: 81.71% | Loss: 1.0902\n",
      "Epoch [48/2000] | Train Acc: 61.73% | Test Acc: 81.83% | Loss: 1.0836\n",
      "Epoch [49/2000] | Train Acc: 61.91% | Test Acc: 81.93% | Loss: 1.0771\n",
      "Epoch [50/2000] | Train Acc: 61.98% | Test Acc: 82.05% | Loss: 1.0707\n",
      "Epoch [51/2000] | Train Acc: 62.02% | Test Acc: 82.38% | Loss: 1.0644\n",
      "Epoch [52/2000] | Train Acc: 62.21% | Test Acc: 82.51% | Loss: 1.0581\n",
      "Epoch [53/2000] | Train Acc: 62.33% | Test Acc: 82.79% | Loss: 1.0519\n",
      "Epoch [54/2000] | Train Acc: 62.46% | Test Acc: 83.05% | Loss: 1.0458\n",
      "Epoch [55/2000] | Train Acc: 62.64% | Test Acc: 83.22% | Loss: 1.0397\n",
      "Epoch [56/2000] | Train Acc: 62.76% | Test Acc: 83.24% | Loss: 1.0338\n",
      "Epoch [57/2000] | Train Acc: 62.83% | Test Acc: 83.28% | Loss: 1.0279\n",
      "Epoch [58/2000] | Train Acc: 62.92% | Test Acc: 83.49% | Loss: 1.0222\n",
      "Epoch [59/2000] | Train Acc: 63.14% | Test Acc: 83.66% | Loss: 1.0166\n",
      "Epoch [60/2000] | Train Acc: 63.27% | Test Acc: 83.74% | Loss: 1.0111\n",
      "Epoch [61/2000] | Train Acc: 63.34% | Test Acc: 83.89% | Loss: 1.0057\n",
      "Epoch [62/2000] | Train Acc: 63.42% | Test Acc: 83.95% | Loss: 1.0004\n",
      "Epoch [63/2000] | Train Acc: 63.50% | Test Acc: 84.15% | Loss: 0.9951\n",
      "Epoch [64/2000] | Train Acc: 63.66% | Test Acc: 84.21% | Loss: 0.9900\n",
      "Epoch [65/2000] | Train Acc: 63.77% | Test Acc: 84.32% | Loss: 0.9849\n",
      "Epoch [66/2000] | Train Acc: 63.85% | Test Acc: 84.40% | Loss: 0.9799\n",
      "Epoch [67/2000] | Train Acc: 63.95% | Test Acc: 84.40% | Loss: 0.9750\n",
      "Epoch [68/2000] | Train Acc: 64.10% | Test Acc: 84.49% | Loss: 0.9701\n",
      "Epoch [69/2000] | Train Acc: 64.16% | Test Acc: 84.61% | Loss: 0.9653\n",
      "Epoch [70/2000] | Train Acc: 64.23% | Test Acc: 84.59% | Loss: 0.9606\n",
      "Epoch [71/2000] | Train Acc: 64.35% | Test Acc: 84.74% | Loss: 0.9559\n",
      "Epoch [72/2000] | Train Acc: 64.44% | Test Acc: 84.82% | Loss: 0.9513\n",
      "Epoch [73/2000] | Train Acc: 64.55% | Test Acc: 84.93% | Loss: 0.9468\n",
      "Epoch [74/2000] | Train Acc: 64.68% | Test Acc: 85.04% | Loss: 0.9423\n",
      "Epoch [75/2000] | Train Acc: 64.73% | Test Acc: 85.17% | Loss: 0.9379\n",
      "Epoch [76/2000] | Train Acc: 64.85% | Test Acc: 85.25% | Loss: 0.9335\n",
      "Epoch [77/2000] | Train Acc: 64.98% | Test Acc: 85.29% | Loss: 0.9292\n",
      "Epoch [78/2000] | Train Acc: 65.10% | Test Acc: 85.31% | Loss: 0.9249\n",
      "Epoch [79/2000] | Train Acc: 65.24% | Test Acc: 85.44% | Loss: 0.9207\n",
      "Epoch [80/2000] | Train Acc: 65.40% | Test Acc: 85.51% | Loss: 0.9165\n",
      "Epoch [81/2000] | Train Acc: 65.50% | Test Acc: 85.60% | Loss: 0.9124\n",
      "Epoch [82/2000] | Train Acc: 65.60% | Test Acc: 85.75% | Loss: 0.9083\n",
      "Epoch [83/2000] | Train Acc: 65.73% | Test Acc: 85.84% | Loss: 0.9042\n",
      "Epoch [84/2000] | Train Acc: 65.88% | Test Acc: 85.95% | Loss: 0.9002\n",
      "Epoch [85/2000] | Train Acc: 65.96% | Test Acc: 85.95% | Loss: 0.8963\n",
      "Epoch [86/2000] | Train Acc: 66.07% | Test Acc: 85.99% | Loss: 0.8923\n",
      "Epoch [87/2000] | Train Acc: 66.15% | Test Acc: 86.06% | Loss: 0.8885\n",
      "Epoch [88/2000] | Train Acc: 66.25% | Test Acc: 86.11% | Loss: 0.8846\n",
      "Epoch [89/2000] | Train Acc: 66.34% | Test Acc: 86.20% | Loss: 0.8808\n",
      "Epoch [90/2000] | Train Acc: 66.48% | Test Acc: 86.29% | Loss: 0.8770\n",
      "Epoch [91/2000] | Train Acc: 66.59% | Test Acc: 86.44% | Loss: 0.8732\n",
      "Epoch [92/2000] | Train Acc: 66.70% | Test Acc: 86.50% | Loss: 0.8694\n",
      "Epoch [93/2000] | Train Acc: 66.80% | Test Acc: 86.62% | Loss: 0.8657\n",
      "Epoch [94/2000] | Train Acc: 66.91% | Test Acc: 86.78% | Loss: 0.8620\n",
      "Epoch [95/2000] | Train Acc: 67.02% | Test Acc: 86.87% | Loss: 0.8583\n",
      "Epoch [96/2000] | Train Acc: 67.13% | Test Acc: 86.93% | Loss: 0.8547\n",
      "Epoch [97/2000] | Train Acc: 67.23% | Test Acc: 87.00% | Loss: 0.8510\n",
      "Epoch [98/2000] | Train Acc: 67.35% | Test Acc: 87.08% | Loss: 0.8474\n",
      "Epoch [99/2000] | Train Acc: 67.43% | Test Acc: 87.21% | Loss: 0.8438\n",
      "Epoch [100/2000] | Train Acc: 67.55% | Test Acc: 87.30% | Loss: 0.8403\n",
      "Epoch [101/2000] | Train Acc: 67.67% | Test Acc: 87.42% | Loss: 0.8367\n",
      "Epoch [102/2000] | Train Acc: 67.79% | Test Acc: 87.55% | Loss: 0.8332\n",
      "Epoch [103/2000] | Train Acc: 67.89% | Test Acc: 87.64% | Loss: 0.8297\n",
      "Epoch [104/2000] | Train Acc: 67.96% | Test Acc: 87.63% | Loss: 0.8262\n",
      "Epoch [105/2000] | Train Acc: 68.11% | Test Acc: 87.67% | Loss: 0.8227\n",
      "Epoch [106/2000] | Train Acc: 68.25% | Test Acc: 87.79% | Loss: 0.8192\n",
      "Epoch [107/2000] | Train Acc: 68.40% | Test Acc: 87.89% | Loss: 0.8158\n",
      "Epoch [108/2000] | Train Acc: 68.53% | Test Acc: 88.00% | Loss: 0.8123\n",
      "Epoch [109/2000] | Train Acc: 68.71% | Test Acc: 88.18% | Loss: 0.8089\n",
      "Epoch [110/2000] | Train Acc: 68.88% | Test Acc: 88.25% | Loss: 0.8055\n",
      "Epoch [111/2000] | Train Acc: 69.02% | Test Acc: 88.34% | Loss: 0.8022\n",
      "Epoch [112/2000] | Train Acc: 69.21% | Test Acc: 88.52% | Loss: 0.7988\n",
      "Epoch [113/2000] | Train Acc: 69.35% | Test Acc: 88.65% | Loss: 0.7955\n",
      "Epoch [114/2000] | Train Acc: 69.48% | Test Acc: 88.82% | Loss: 0.7922\n",
      "Epoch [115/2000] | Train Acc: 69.64% | Test Acc: 88.91% | Loss: 0.7889\n",
      "Epoch [116/2000] | Train Acc: 69.74% | Test Acc: 89.02% | Loss: 0.7856\n",
      "Epoch [117/2000] | Train Acc: 69.86% | Test Acc: 89.16% | Loss: 0.7824\n",
      "Epoch [118/2000] | Train Acc: 69.99% | Test Acc: 89.22% | Loss: 0.7791\n",
      "Epoch [119/2000] | Train Acc: 70.11% | Test Acc: 89.34% | Loss: 0.7759\n",
      "Epoch [120/2000] | Train Acc: 70.23% | Test Acc: 89.41% | Loss: 0.7727\n",
      "Epoch [121/2000] | Train Acc: 70.35% | Test Acc: 89.46% | Loss: 0.7696\n",
      "Epoch [122/2000] | Train Acc: 70.52% | Test Acc: 89.52% | Loss: 0.7665\n",
      "Epoch [123/2000] | Train Acc: 70.56% | Test Acc: 89.53% | Loss: 0.7633\n",
      "Epoch [124/2000] | Train Acc: 70.71% | Test Acc: 89.53% | Loss: 0.7603\n",
      "Epoch [125/2000] | Train Acc: 70.84% | Test Acc: 89.53% | Loss: 0.7572\n",
      "Epoch [126/2000] | Train Acc: 70.96% | Test Acc: 89.60% | Loss: 0.7542\n",
      "Epoch [127/2000] | Train Acc: 71.05% | Test Acc: 89.66% | Loss: 0.7511\n",
      "Epoch [128/2000] | Train Acc: 71.17% | Test Acc: 89.71% | Loss: 0.7482\n",
      "Epoch [129/2000] | Train Acc: 71.23% | Test Acc: 89.72% | Loss: 0.7452\n",
      "Epoch [130/2000] | Train Acc: 71.36% | Test Acc: 89.80% | Loss: 0.7423\n",
      "Epoch [131/2000] | Train Acc: 71.43% | Test Acc: 89.83% | Loss: 0.7393\n",
      "Epoch [132/2000] | Train Acc: 71.52% | Test Acc: 89.86% | Loss: 0.7364\n",
      "Epoch [133/2000] | Train Acc: 71.66% | Test Acc: 89.90% | Loss: 0.7336\n",
      "Epoch [134/2000] | Train Acc: 71.79% | Test Acc: 89.95% | Loss: 0.7307\n",
      "Epoch [135/2000] | Train Acc: 71.88% | Test Acc: 90.01% | Loss: 0.7279\n",
      "Epoch [136/2000] | Train Acc: 71.98% | Test Acc: 90.02% | Loss: 0.7251\n",
      "Epoch [137/2000] | Train Acc: 72.10% | Test Acc: 90.02% | Loss: 0.7223\n",
      "Epoch [138/2000] | Train Acc: 72.22% | Test Acc: 90.07% | Loss: 0.7195\n",
      "Epoch [139/2000] | Train Acc: 72.31% | Test Acc: 90.10% | Loss: 0.7167\n",
      "Epoch [140/2000] | Train Acc: 72.46% | Test Acc: 90.10% | Loss: 0.7140\n",
      "Epoch [141/2000] | Train Acc: 72.50% | Test Acc: 90.11% | Loss: 0.7112\n",
      "Epoch [142/2000] | Train Acc: 72.59% | Test Acc: 90.10% | Loss: 0.7085\n",
      "Epoch [143/2000] | Train Acc: 72.69% | Test Acc: 90.10% | Loss: 0.7058\n",
      "Epoch [144/2000] | Train Acc: 72.86% | Test Acc: 90.08% | Loss: 0.7031\n",
      "Epoch [145/2000] | Train Acc: 72.97% | Test Acc: 90.11% | Loss: 0.7004\n",
      "Epoch [146/2000] | Train Acc: 73.07% | Test Acc: 90.18% | Loss: 0.6978\n",
      "Epoch [147/2000] | Train Acc: 73.13% | Test Acc: 90.21% | Loss: 0.6951\n",
      "Epoch [148/2000] | Train Acc: 73.23% | Test Acc: 90.26% | Loss: 0.6925\n",
      "Epoch [149/2000] | Train Acc: 73.33% | Test Acc: 90.29% | Loss: 0.6898\n",
      "Epoch [150/2000] | Train Acc: 73.47% | Test Acc: 90.38% | Loss: 0.6872\n",
      "Epoch [151/2000] | Train Acc: 73.53% | Test Acc: 90.45% | Loss: 0.6846\n",
      "Epoch [152/2000] | Train Acc: 73.63% | Test Acc: 90.50% | Loss: 0.6820\n",
      "Epoch [153/2000] | Train Acc: 73.71% | Test Acc: 90.53% | Loss: 0.6794\n",
      "Epoch [154/2000] | Train Acc: 73.82% | Test Acc: 90.53% | Loss: 0.6768\n",
      "Epoch [155/2000] | Train Acc: 73.91% | Test Acc: 90.59% | Loss: 0.6742\n",
      "Epoch [156/2000] | Train Acc: 73.99% | Test Acc: 90.63% | Loss: 0.6716\n",
      "Epoch [157/2000] | Train Acc: 74.15% | Test Acc: 90.65% | Loss: 0.6690\n",
      "Epoch [158/2000] | Train Acc: 74.28% | Test Acc: 90.69% | Loss: 0.6665\n",
      "Epoch [159/2000] | Train Acc: 74.34% | Test Acc: 90.73% | Loss: 0.6639\n",
      "Epoch [160/2000] | Train Acc: 74.45% | Test Acc: 90.79% | Loss: 0.6614\n",
      "Epoch [161/2000] | Train Acc: 74.55% | Test Acc: 90.81% | Loss: 0.6588\n",
      "Epoch [162/2000] | Train Acc: 74.70% | Test Acc: 90.81% | Loss: 0.6563\n",
      "Epoch [163/2000] | Train Acc: 74.83% | Test Acc: 90.84% | Loss: 0.6537\n",
      "Epoch [164/2000] | Train Acc: 74.93% | Test Acc: 90.87% | Loss: 0.6512\n",
      "Epoch [165/2000] | Train Acc: 75.06% | Test Acc: 90.91% | Loss: 0.6487\n",
      "Epoch [166/2000] | Train Acc: 75.18% | Test Acc: 90.96% | Loss: 0.6461\n",
      "Epoch [167/2000] | Train Acc: 75.25% | Test Acc: 90.97% | Loss: 0.6436\n",
      "Epoch [168/2000] | Train Acc: 75.39% | Test Acc: 91.03% | Loss: 0.6411\n",
      "Epoch [169/2000] | Train Acc: 75.53% | Test Acc: 91.06% | Loss: 0.6386\n",
      "Epoch [170/2000] | Train Acc: 75.63% | Test Acc: 91.12% | Loss: 0.6361\n",
      "Epoch [171/2000] | Train Acc: 75.71% | Test Acc: 91.11% | Loss: 0.6336\n",
      "Epoch [172/2000] | Train Acc: 75.84% | Test Acc: 91.12% | Loss: 0.6311\n",
      "Epoch [173/2000] | Train Acc: 75.97% | Test Acc: 91.14% | Loss: 0.6286\n",
      "Epoch [174/2000] | Train Acc: 76.08% | Test Acc: 91.18% | Loss: 0.6261\n",
      "Epoch [175/2000] | Train Acc: 76.20% | Test Acc: 91.21% | Loss: 0.6236\n",
      "Epoch [176/2000] | Train Acc: 76.33% | Test Acc: 91.27% | Loss: 0.6211\n",
      "Epoch [177/2000] | Train Acc: 76.44% | Test Acc: 91.30% | Loss: 0.6187\n",
      "Epoch [178/2000] | Train Acc: 76.57% | Test Acc: 91.36% | Loss: 0.6162\n",
      "Epoch [179/2000] | Train Acc: 76.70% | Test Acc: 91.40% | Loss: 0.6137\n",
      "Epoch [180/2000] | Train Acc: 76.81% | Test Acc: 91.48% | Loss: 0.6113\n",
      "Epoch [181/2000] | Train Acc: 76.97% | Test Acc: 91.55% | Loss: 0.6088\n",
      "Epoch [182/2000] | Train Acc: 77.13% | Test Acc: 91.64% | Loss: 0.6064\n",
      "Epoch [183/2000] | Train Acc: 77.26% | Test Acc: 91.69% | Loss: 0.6039\n",
      "Epoch [184/2000] | Train Acc: 77.38% | Test Acc: 91.69% | Loss: 0.6015\n",
      "Epoch [185/2000] | Train Acc: 77.51% | Test Acc: 91.70% | Loss: 0.5990\n",
      "Epoch [186/2000] | Train Acc: 77.63% | Test Acc: 91.76% | Loss: 0.5966\n",
      "Epoch [187/2000] | Train Acc: 77.78% | Test Acc: 91.81% | Loss: 0.5942\n",
      "Epoch [188/2000] | Train Acc: 77.89% | Test Acc: 91.85% | Loss: 0.5918\n",
      "Epoch [189/2000] | Train Acc: 77.98% | Test Acc: 91.86% | Loss: 0.5893\n",
      "Epoch [190/2000] | Train Acc: 78.09% | Test Acc: 91.97% | Loss: 0.5869\n",
      "Epoch [191/2000] | Train Acc: 78.18% | Test Acc: 91.97% | Loss: 0.5845\n",
      "Epoch [192/2000] | Train Acc: 78.34% | Test Acc: 91.98% | Loss: 0.5821\n",
      "Epoch [193/2000] | Train Acc: 78.47% | Test Acc: 91.98% | Loss: 0.5797\n",
      "Epoch [194/2000] | Train Acc: 78.57% | Test Acc: 92.01% | Loss: 0.5774\n",
      "Epoch [195/2000] | Train Acc: 78.66% | Test Acc: 92.07% | Loss: 0.5750\n",
      "Epoch [196/2000] | Train Acc: 78.79% | Test Acc: 92.10% | Loss: 0.5726\n",
      "Epoch [197/2000] | Train Acc: 78.92% | Test Acc: 92.15% | Loss: 0.5702\n",
      "Epoch [198/2000] | Train Acc: 79.02% | Test Acc: 92.19% | Loss: 0.5679\n",
      "Epoch [199/2000] | Train Acc: 79.12% | Test Acc: 92.25% | Loss: 0.5655\n",
      "Epoch [200/2000] | Train Acc: 79.24% | Test Acc: 92.28% | Loss: 0.5631\n",
      "Epoch [201/2000] | Train Acc: 79.35% | Test Acc: 92.33% | Loss: 0.5608\n",
      "Epoch [202/2000] | Train Acc: 79.44% | Test Acc: 92.37% | Loss: 0.5585\n",
      "Epoch [203/2000] | Train Acc: 79.55% | Test Acc: 92.39% | Loss: 0.5561\n",
      "Epoch [204/2000] | Train Acc: 79.64% | Test Acc: 92.42% | Loss: 0.5538\n",
      "Epoch [205/2000] | Train Acc: 79.78% | Test Acc: 92.43% | Loss: 0.5515\n",
      "Epoch [206/2000] | Train Acc: 79.90% | Test Acc: 92.43% | Loss: 0.5491\n",
      "Epoch [207/2000] | Train Acc: 79.97% | Test Acc: 92.42% | Loss: 0.5468\n",
      "Epoch [208/2000] | Train Acc: 80.09% | Test Acc: 92.43% | Loss: 0.5445\n",
      "Epoch [209/2000] | Train Acc: 80.18% | Test Acc: 92.49% | Loss: 0.5422\n",
      "Epoch [210/2000] | Train Acc: 80.29% | Test Acc: 92.53% | Loss: 0.5399\n",
      "Epoch [211/2000] | Train Acc: 80.44% | Test Acc: 92.55% | Loss: 0.5376\n",
      "Epoch [212/2000] | Train Acc: 80.53% | Test Acc: 92.62% | Loss: 0.5353\n",
      "Epoch [213/2000] | Train Acc: 80.63% | Test Acc: 92.64% | Loss: 0.5330\n",
      "Epoch [214/2000] | Train Acc: 80.72% | Test Acc: 92.64% | Loss: 0.5307\n",
      "Epoch [215/2000] | Train Acc: 80.82% | Test Acc: 92.70% | Loss: 0.5284\n",
      "Epoch [216/2000] | Train Acc: 80.93% | Test Acc: 92.67% | Loss: 0.5262\n",
      "Epoch [217/2000] | Train Acc: 81.02% | Test Acc: 92.80% | Loss: 0.5239\n",
      "Epoch [218/2000] | Train Acc: 81.16% | Test Acc: 92.71% | Loss: 0.5216\n",
      "Epoch [219/2000] | Train Acc: 81.21% | Test Acc: 92.91% | Loss: 0.5193\n",
      "Epoch [220/2000] | Train Acc: 81.42% | Test Acc: 92.58% | Loss: 0.5171\n",
      "Epoch [221/2000] | Train Acc: 81.27% | Test Acc: 93.05% | Loss: 0.5149\n",
      "Epoch [222/2000] | Train Acc: 81.70% | Test Acc: 92.16% | Loss: 0.5127\n",
      "Epoch [223/2000] | Train Acc: 81.24% | Test Acc: 93.29% | Loss: 0.5108\n",
      "Epoch [224/2000] | Train Acc: 82.08% | Test Acc: 92.19% | Loss: 0.5089\n",
      "Epoch [225/2000] | Train Acc: 81.48% | Test Acc: 92.86% | Loss: 0.5063\n",
      "Epoch [226/2000] | Train Acc: 82.02% | Test Acc: 93.23% | Loss: 0.5037\n",
      "Epoch [227/2000] | Train Acc: 82.33% | Test Acc: 92.21% | Loss: 0.5019\n",
      "Epoch [228/2000] | Train Acc: 81.74% | Test Acc: 92.95% | Loss: 0.4998\n",
      "Epoch [229/2000] | Train Acc: 82.36% | Test Acc: 93.23% | Loss: 0.4971\n",
      "Epoch [230/2000] | Train Acc: 82.60% | Test Acc: 92.30% | Loss: 0.4952\n",
      "Epoch [231/2000] | Train Acc: 82.07% | Test Acc: 93.02% | Loss: 0.4931\n",
      "Epoch [232/2000] | Train Acc: 82.63% | Test Acc: 93.23% | Loss: 0.4906\n",
      "Epoch [233/2000] | Train Acc: 82.82% | Test Acc: 92.39% | Loss: 0.4886\n",
      "Epoch [234/2000] | Train Acc: 82.36% | Test Acc: 93.05% | Loss: 0.4866\n",
      "Epoch [235/2000] | Train Acc: 82.92% | Test Acc: 93.29% | Loss: 0.4841\n",
      "Epoch [236/2000] | Train Acc: 83.10% | Test Acc: 92.50% | Loss: 0.4822\n",
      "Epoch [237/2000] | Train Acc: 82.77% | Test Acc: 93.10% | Loss: 0.4800\n",
      "Epoch [238/2000] | Train Acc: 83.24% | Test Acc: 93.34% | Loss: 0.4777\n",
      "Epoch [239/2000] | Train Acc: 83.38% | Test Acc: 92.77% | Loss: 0.4757\n",
      "Epoch [240/2000] | Train Acc: 83.14% | Test Acc: 93.14% | Loss: 0.4736\n",
      "Epoch [241/2000] | Train Acc: 83.49% | Test Acc: 93.37% | Loss: 0.4713\n",
      "Epoch [242/2000] | Train Acc: 83.64% | Test Acc: 92.98% | Loss: 0.4693\n",
      "Epoch [243/2000] | Train Acc: 83.52% | Test Acc: 93.20% | Loss: 0.4672\n",
      "Epoch [244/2000] | Train Acc: 83.79% | Test Acc: 93.37% | Loss: 0.4650\n",
      "Epoch [245/2000] | Train Acc: 83.91% | Test Acc: 93.10% | Loss: 0.4630\n",
      "Epoch [246/2000] | Train Acc: 83.85% | Test Acc: 93.26% | Loss: 0.4609\n",
      "Epoch [247/2000] | Train Acc: 84.05% | Test Acc: 93.40% | Loss: 0.4587\n",
      "Epoch [248/2000] | Train Acc: 84.20% | Test Acc: 93.14% | Loss: 0.4567\n",
      "Epoch [249/2000] | Train Acc: 84.18% | Test Acc: 93.32% | Loss: 0.4546\n",
      "Epoch [250/2000] | Train Acc: 84.38% | Test Acc: 93.31% | Loss: 0.4525\n",
      "Epoch [251/2000] | Train Acc: 84.45% | Test Acc: 93.31% | Loss: 0.4504\n",
      "Epoch [252/2000] | Train Acc: 84.46% | Test Acc: 93.31% | Loss: 0.4484\n",
      "Epoch [253/2000] | Train Acc: 84.64% | Test Acc: 93.31% | Loss: 0.4463\n",
      "Epoch [254/2000] | Train Acc: 84.73% | Test Acc: 93.38% | Loss: 0.4442\n",
      "Epoch [255/2000] | Train Acc: 84.79% | Test Acc: 93.40% | Loss: 0.4422\n",
      "Epoch [256/2000] | Train Acc: 84.90% | Test Acc: 93.35% | Loss: 0.4402\n",
      "Epoch [257/2000] | Train Acc: 84.98% | Test Acc: 93.41% | Loss: 0.4381\n",
      "Epoch [258/2000] | Train Acc: 85.06% | Test Acc: 93.43% | Loss: 0.4361\n",
      "Epoch [259/2000] | Train Acc: 85.17% | Test Acc: 93.44% | Loss: 0.4340\n",
      "Epoch [260/2000] | Train Acc: 85.25% | Test Acc: 93.41% | Loss: 0.4320\n",
      "Epoch [261/2000] | Train Acc: 85.34% | Test Acc: 93.49% | Loss: 0.4299\n",
      "Epoch [262/2000] | Train Acc: 85.43% | Test Acc: 93.44% | Loss: 0.4280\n",
      "Epoch [263/2000] | Train Acc: 85.51% | Test Acc: 93.50% | Loss: 0.4260\n",
      "Epoch [264/2000] | Train Acc: 85.63% | Test Acc: 93.47% | Loss: 0.4239\n",
      "Epoch [265/2000] | Train Acc: 85.69% | Test Acc: 93.47% | Loss: 0.4219\n",
      "Epoch [266/2000] | Train Acc: 85.80% | Test Acc: 93.53% | Loss: 0.4199\n",
      "Epoch [267/2000] | Train Acc: 85.90% | Test Acc: 93.49% | Loss: 0.4179\n",
      "Epoch [268/2000] | Train Acc: 85.95% | Test Acc: 93.50% | Loss: 0.4160\n",
      "Epoch [269/2000] | Train Acc: 86.04% | Test Acc: 93.52% | Loss: 0.4140\n",
      "Epoch [270/2000] | Train Acc: 86.12% | Test Acc: 93.52% | Loss: 0.4120\n",
      "Epoch [271/2000] | Train Acc: 86.24% | Test Acc: 93.58% | Loss: 0.4100\n",
      "Epoch [272/2000] | Train Acc: 86.39% | Test Acc: 93.50% | Loss: 0.4081\n",
      "Epoch [273/2000] | Train Acc: 86.43% | Test Acc: 93.62% | Loss: 0.4061\n",
      "Epoch [274/2000] | Train Acc: 86.57% | Test Acc: 93.53% | Loss: 0.4041\n",
      "Epoch [275/2000] | Train Acc: 86.62% | Test Acc: 93.55% | Loss: 0.4022\n",
      "Epoch [276/2000] | Train Acc: 86.71% | Test Acc: 93.62% | Loss: 0.4002\n",
      "Epoch [277/2000] | Train Acc: 86.83% | Test Acc: 93.55% | Loss: 0.3983\n",
      "Epoch [278/2000] | Train Acc: 86.90% | Test Acc: 93.63% | Loss: 0.3964\n",
      "Epoch [279/2000] | Train Acc: 87.05% | Test Acc: 93.50% | Loss: 0.3945\n",
      "Epoch [280/2000] | Train Acc: 87.11% | Test Acc: 93.65% | Loss: 0.3925\n",
      "Epoch [281/2000] | Train Acc: 87.24% | Test Acc: 93.52% | Loss: 0.3906\n",
      "Epoch [282/2000] | Train Acc: 87.29% | Test Acc: 93.66% | Loss: 0.3887\n",
      "Epoch [283/2000] | Train Acc: 87.44% | Test Acc: 93.50% | Loss: 0.3868\n",
      "Epoch [284/2000] | Train Acc: 87.44% | Test Acc: 93.62% | Loss: 0.3849\n",
      "Epoch [285/2000] | Train Acc: 87.61% | Test Acc: 93.56% | Loss: 0.3831\n",
      "Epoch [286/2000] | Train Acc: 87.60% | Test Acc: 93.81% | Loss: 0.3812\n",
      "Epoch [287/2000] | Train Acc: 87.81% | Test Acc: 93.56% | Loss: 0.3794\n",
      "Epoch [288/2000] | Train Acc: 87.77% | Test Acc: 93.81% | Loss: 0.3776\n",
      "Epoch [289/2000] | Train Acc: 88.06% | Test Acc: 93.28% | Loss: 0.3758\n",
      "Epoch [290/2000] | Train Acc: 87.75% | Test Acc: 93.87% | Loss: 0.3742\n",
      "Epoch [291/2000] | Train Acc: 88.28% | Test Acc: 92.95% | Loss: 0.3725\n",
      "Epoch [292/2000] | Train Acc: 87.66% | Test Acc: 93.90% | Loss: 0.3709\n",
      "Epoch [293/2000] | Train Acc: 88.43% | Test Acc: 93.60% | Loss: 0.3688\n",
      "Epoch [294/2000] | Train Acc: 88.30% | Test Acc: 93.66% | Loss: 0.3666\n",
      "Epoch [295/2000] | Train Acc: 88.49% | Test Acc: 93.80% | Loss: 0.3646\n",
      "Epoch [296/2000] | Train Acc: 88.66% | Test Acc: 93.60% | Loss: 0.3630\n",
      "Epoch [297/2000] | Train Acc: 88.50% | Test Acc: 93.83% | Loss: 0.3615\n",
      "Epoch [298/2000] | Train Acc: 88.83% | Test Acc: 93.60% | Loss: 0.3597\n",
      "Epoch [299/2000] | Train Acc: 88.67% | Test Acc: 93.77% | Loss: 0.3578\n",
      "Epoch [300/2000] | Train Acc: 88.91% | Test Acc: 93.81% | Loss: 0.3558\n",
      "Epoch [301/2000] | Train Acc: 89.02% | Test Acc: 93.58% | Loss: 0.3541\n",
      "Epoch [302/2000] | Train Acc: 88.93% | Test Acc: 93.83% | Loss: 0.3526\n",
      "Epoch [303/2000] | Train Acc: 89.22% | Test Acc: 93.59% | Loss: 0.3509\n",
      "Epoch [304/2000] | Train Acc: 89.08% | Test Acc: 93.86% | Loss: 0.3491\n",
      "Epoch [305/2000] | Train Acc: 89.31% | Test Acc: 93.86% | Loss: 0.3472\n",
      "Epoch [306/2000] | Train Acc: 89.40% | Test Acc: 93.62% | Loss: 0.3455\n",
      "Epoch [307/2000] | Train Acc: 89.36% | Test Acc: 93.83% | Loss: 0.3439\n",
      "Epoch [308/2000] | Train Acc: 89.62% | Test Acc: 93.62% | Loss: 0.3423\n",
      "Epoch [309/2000] | Train Acc: 89.53% | Test Acc: 93.89% | Loss: 0.3406\n",
      "Epoch [310/2000] | Train Acc: 89.79% | Test Acc: 93.81% | Loss: 0.3388\n",
      "Epoch [311/2000] | Train Acc: 89.83% | Test Acc: 93.86% | Loss: 0.3371\n",
      "Epoch [312/2000] | Train Acc: 89.86% | Test Acc: 93.93% | Loss: 0.3354\n",
      "Epoch [313/2000] | Train Acc: 90.04% | Test Acc: 93.66% | Loss: 0.3338\n",
      "Epoch [314/2000] | Train Acc: 89.97% | Test Acc: 93.93% | Loss: 0.3322\n",
      "Epoch [315/2000] | Train Acc: 90.20% | Test Acc: 93.78% | Loss: 0.3306\n",
      "Epoch [316/2000] | Train Acc: 90.14% | Test Acc: 94.01% | Loss: 0.3289\n",
      "Epoch [317/2000] | Train Acc: 90.32% | Test Acc: 93.92% | Loss: 0.3272\n",
      "Epoch [318/2000] | Train Acc: 90.35% | Test Acc: 93.89% | Loss: 0.3256\n",
      "Epoch [319/2000] | Train Acc: 90.42% | Test Acc: 94.01% | Loss: 0.3240\n",
      "Epoch [320/2000] | Train Acc: 90.57% | Test Acc: 93.87% | Loss: 0.3224\n",
      "Epoch [321/2000] | Train Acc: 90.54% | Test Acc: 93.98% | Loss: 0.3208\n",
      "Epoch [322/2000] | Train Acc: 90.70% | Test Acc: 93.81% | Loss: 0.3193\n",
      "Epoch [323/2000] | Train Acc: 90.65% | Test Acc: 93.99% | Loss: 0.3177\n",
      "Epoch [324/2000] | Train Acc: 90.82% | Test Acc: 93.80% | Loss: 0.3161\n",
      "Epoch [325/2000] | Train Acc: 90.75% | Test Acc: 93.99% | Loss: 0.3146\n",
      "Epoch [326/2000] | Train Acc: 90.94% | Test Acc: 93.83% | Loss: 0.3130\n",
      "Epoch [327/2000] | Train Acc: 90.89% | Test Acc: 93.99% | Loss: 0.3115\n",
      "Epoch [328/2000] | Train Acc: 91.08% | Test Acc: 93.84% | Loss: 0.3099\n",
      "Epoch [329/2000] | Train Acc: 90.99% | Test Acc: 94.01% | Loss: 0.3084\n",
      "Epoch [330/2000] | Train Acc: 91.20% | Test Acc: 93.89% | Loss: 0.3069\n",
      "Epoch [331/2000] | Train Acc: 91.12% | Test Acc: 94.02% | Loss: 0.3054\n",
      "Epoch [332/2000] | Train Acc: 91.33% | Test Acc: 93.90% | Loss: 0.3038\n",
      "Epoch [333/2000] | Train Acc: 91.27% | Test Acc: 94.01% | Loss: 0.3023\n",
      "Epoch [334/2000] | Train Acc: 91.43% | Test Acc: 93.95% | Loss: 0.3008\n",
      "Epoch [335/2000] | Train Acc: 91.42% | Test Acc: 94.02% | Loss: 0.2993\n",
      "Epoch [336/2000] | Train Acc: 91.57% | Test Acc: 93.95% | Loss: 0.2979\n",
      "Epoch [337/2000] | Train Acc: 91.56% | Test Acc: 94.04% | Loss: 0.2964\n",
      "Epoch [338/2000] | Train Acc: 91.72% | Test Acc: 93.92% | Loss: 0.2950\n",
      "Epoch [339/2000] | Train Acc: 91.70% | Test Acc: 94.05% | Loss: 0.2935\n",
      "Epoch [340/2000] | Train Acc: 91.85% | Test Acc: 93.95% | Loss: 0.2921\n",
      "Epoch [341/2000] | Train Acc: 91.79% | Test Acc: 94.07% | Loss: 0.2907\n",
      "Epoch [342/2000] | Train Acc: 91.97% | Test Acc: 93.90% | Loss: 0.2893\n",
      "Epoch [343/2000] | Train Acc: 91.95% | Test Acc: 94.07% | Loss: 0.2879\n",
      "Epoch [344/2000] | Train Acc: 92.13% | Test Acc: 93.96% | Loss: 0.2864\n",
      "Epoch [345/2000] | Train Acc: 92.07% | Test Acc: 94.07% | Loss: 0.2850\n",
      "Epoch [346/2000] | Train Acc: 92.21% | Test Acc: 94.01% | Loss: 0.2835\n",
      "Epoch [347/2000] | Train Acc: 92.27% | Test Acc: 94.08% | Loss: 0.2821\n",
      "Epoch [348/2000] | Train Acc: 92.36% | Test Acc: 94.07% | Loss: 0.2806\n",
      "Epoch [349/2000] | Train Acc: 92.41% | Test Acc: 94.07% | Loss: 0.2792\n",
      "Epoch [350/2000] | Train Acc: 92.46% | Test Acc: 94.07% | Loss: 0.2778\n",
      "Epoch [351/2000] | Train Acc: 92.52% | Test Acc: 94.07% | Loss: 0.2764\n",
      "Epoch [352/2000] | Train Acc: 92.59% | Test Acc: 94.05% | Loss: 0.2751\n",
      "Epoch [353/2000] | Train Acc: 92.63% | Test Acc: 94.13% | Loss: 0.2737\n",
      "Epoch [354/2000] | Train Acc: 92.69% | Test Acc: 94.14% | Loss: 0.2724\n",
      "Epoch [355/2000] | Train Acc: 92.78% | Test Acc: 94.07% | Loss: 0.2711\n",
      "Epoch [356/2000] | Train Acc: 92.80% | Test Acc: 94.13% | Loss: 0.2698\n",
      "Epoch [357/2000] | Train Acc: 92.86% | Test Acc: 94.02% | Loss: 0.2686\n",
      "Epoch [358/2000] | Train Acc: 92.85% | Test Acc: 94.05% | Loss: 0.2675\n",
      "Epoch [359/2000] | Train Acc: 92.94% | Test Acc: 93.87% | Loss: 0.2666\n",
      "Epoch [360/2000] | Train Acc: 92.70% | Test Acc: 93.98% | Loss: 0.2660\n",
      "Epoch [361/2000] | Train Acc: 92.98% | Test Acc: 93.69% | Loss: 0.2650\n",
      "Epoch [362/2000] | Train Acc: 92.67% | Test Acc: 94.08% | Loss: 0.2637\n",
      "Epoch [363/2000] | Train Acc: 93.12% | Test Acc: 94.07% | Loss: 0.2612\n",
      "Epoch [364/2000] | Train Acc: 93.19% | Test Acc: 94.01% | Loss: 0.2594\n",
      "Epoch [365/2000] | Train Acc: 93.14% | Test Acc: 94.08% | Loss: 0.2586\n",
      "Epoch [366/2000] | Train Acc: 93.24% | Test Acc: 93.99% | Loss: 0.2580\n",
      "Epoch [367/2000] | Train Acc: 93.13% | Test Acc: 94.10% | Loss: 0.2567\n",
      "Epoch [368/2000] | Train Acc: 93.36% | Test Acc: 94.10% | Loss: 0.2547\n",
      "Epoch [369/2000] | Train Acc: 93.44% | Test Acc: 94.07% | Loss: 0.2534\n",
      "Epoch [370/2000] | Train Acc: 93.40% | Test Acc: 94.13% | Loss: 0.2526\n",
      "Epoch [371/2000] | Train Acc: 93.51% | Test Acc: 94.11% | Loss: 0.2516\n",
      "Epoch [372/2000] | Train Acc: 93.57% | Test Acc: 94.11% | Loss: 0.2500\n",
      "Epoch [373/2000] | Train Acc: 93.60% | Test Acc: 94.11% | Loss: 0.2486\n",
      "Epoch [374/2000] | Train Acc: 93.63% | Test Acc: 94.10% | Loss: 0.2476\n",
      "Epoch [375/2000] | Train Acc: 93.62% | Test Acc: 94.10% | Loss: 0.2467\n",
      "Epoch [376/2000] | Train Acc: 93.73% | Test Acc: 94.19% | Loss: 0.2454\n",
      "Epoch [377/2000] | Train Acc: 93.80% | Test Acc: 94.14% | Loss: 0.2440\n",
      "Epoch [378/2000] | Train Acc: 93.81% | Test Acc: 94.10% | Loss: 0.2428\n",
      "Epoch [379/2000] | Train Acc: 93.85% | Test Acc: 94.11% | Loss: 0.2419\n",
      "Epoch [380/2000] | Train Acc: 93.88% | Test Acc: 94.08% | Loss: 0.2408\n",
      "Epoch [381/2000] | Train Acc: 93.94% | Test Acc: 94.19% | Loss: 0.2395\n",
      "Epoch [382/2000] | Train Acc: 93.98% | Test Acc: 94.21% | Loss: 0.2383\n",
      "Epoch [383/2000] | Train Acc: 94.03% | Test Acc: 94.10% | Loss: 0.2373\n",
      "Epoch [384/2000] | Train Acc: 94.05% | Test Acc: 94.21% | Loss: 0.2363\n",
      "Epoch [385/2000] | Train Acc: 94.08% | Test Acc: 94.16% | Loss: 0.2351\n",
      "Epoch [386/2000] | Train Acc: 94.13% | Test Acc: 94.19% | Loss: 0.2339\n",
      "Epoch [387/2000] | Train Acc: 94.16% | Test Acc: 94.20% | Loss: 0.2328\n",
      "Epoch [388/2000] | Train Acc: 94.20% | Test Acc: 94.11% | Loss: 0.2317\n",
      "Epoch [389/2000] | Train Acc: 94.26% | Test Acc: 94.19% | Loss: 0.2307\n",
      "Epoch [390/2000] | Train Acc: 94.32% | Test Acc: 94.10% | Loss: 0.2297\n",
      "Epoch [391/2000] | Train Acc: 94.34% | Test Acc: 94.16% | Loss: 0.2286\n",
      "Epoch [392/2000] | Train Acc: 94.39% | Test Acc: 94.16% | Loss: 0.2274\n",
      "Epoch [393/2000] | Train Acc: 94.43% | Test Acc: 94.16% | Loss: 0.2264\n",
      "Epoch [394/2000] | Train Acc: 94.49% | Test Acc: 94.20% | Loss: 0.2254\n",
      "Epoch [395/2000] | Train Acc: 94.55% | Test Acc: 94.11% | Loss: 0.2244\n",
      "Epoch [396/2000] | Train Acc: 94.57% | Test Acc: 94.20% | Loss: 0.2233\n",
      "Epoch [397/2000] | Train Acc: 94.59% | Test Acc: 94.17% | Loss: 0.2223\n",
      "Epoch [398/2000] | Train Acc: 94.64% | Test Acc: 94.21% | Loss: 0.2212\n",
      "Epoch [399/2000] | Train Acc: 94.68% | Test Acc: 94.17% | Loss: 0.2202\n",
      "Epoch [400/2000] | Train Acc: 94.70% | Test Acc: 94.17% | Loss: 0.2192\n",
      "Epoch [401/2000] | Train Acc: 94.76% | Test Acc: 94.20% | Loss: 0.2182\n",
      "Epoch [402/2000] | Train Acc: 94.77% | Test Acc: 94.19% | Loss: 0.2172\n",
      "Epoch [403/2000] | Train Acc: 94.83% | Test Acc: 94.21% | Loss: 0.2162\n",
      "Epoch [404/2000] | Train Acc: 94.85% | Test Acc: 94.17% | Loss: 0.2152\n",
      "Epoch [405/2000] | Train Acc: 94.89% | Test Acc: 94.19% | Loss: 0.2143\n",
      "Epoch [406/2000] | Train Acc: 94.94% | Test Acc: 94.14% | Loss: 0.2133\n",
      "Epoch [407/2000] | Train Acc: 94.99% | Test Acc: 94.20% | Loss: 0.2124\n",
      "Epoch [408/2000] | Train Acc: 95.01% | Test Acc: 94.13% | Loss: 0.2114\n",
      "Epoch [409/2000] | Train Acc: 95.05% | Test Acc: 94.27% | Loss: 0.2105\n",
      "Epoch [410/2000] | Train Acc: 95.10% | Test Acc: 94.08% | Loss: 0.2096\n",
      "Epoch [411/2000] | Train Acc: 95.11% | Test Acc: 94.26% | Loss: 0.2087\n",
      "Epoch [412/2000] | Train Acc: 95.13% | Test Acc: 94.07% | Loss: 0.2079\n",
      "Epoch [413/2000] | Train Acc: 95.18% | Test Acc: 94.23% | Loss: 0.2071\n",
      "Epoch [414/2000] | Train Acc: 95.13% | Test Acc: 94.05% | Loss: 0.2063\n",
      "Epoch [415/2000] | Train Acc: 95.24% | Test Acc: 94.21% | Loss: 0.2054\n",
      "Epoch [416/2000] | Train Acc: 95.21% | Test Acc: 94.04% | Loss: 0.2045\n",
      "Epoch [417/2000] | Train Acc: 95.32% | Test Acc: 94.21% | Loss: 0.2033\n",
      "Epoch [418/2000] | Train Acc: 95.36% | Test Acc: 94.17% | Loss: 0.2021\n",
      "Epoch [419/2000] | Train Acc: 95.39% | Test Acc: 94.23% | Loss: 0.2011\n",
      "Epoch [420/2000] | Train Acc: 95.39% | Test Acc: 94.23% | Loss: 0.2001\n",
      "Epoch [421/2000] | Train Acc: 95.43% | Test Acc: 94.14% | Loss: 0.1993\n",
      "Epoch [422/2000] | Train Acc: 95.49% | Test Acc: 94.24% | Loss: 0.1985\n",
      "Epoch [423/2000] | Train Acc: 95.52% | Test Acc: 94.08% | Loss: 0.1978\n",
      "Epoch [424/2000] | Train Acc: 95.57% | Test Acc: 94.30% | Loss: 0.1970\n",
      "Epoch [425/2000] | Train Acc: 95.56% | Test Acc: 94.07% | Loss: 0.1962\n",
      "Epoch [426/2000] | Train Acc: 95.63% | Test Acc: 94.26% | Loss: 0.1952\n",
      "Epoch [427/2000] | Train Acc: 95.65% | Test Acc: 94.17% | Loss: 0.1943\n",
      "Epoch [428/2000] | Train Acc: 95.69% | Test Acc: 94.27% | Loss: 0.1933\n",
      "Epoch [429/2000] | Train Acc: 95.69% | Test Acc: 94.26% | Loss: 0.1924\n",
      "Epoch [430/2000] | Train Acc: 95.71% | Test Acc: 94.21% | Loss: 0.1915\n",
      "Epoch [431/2000] | Train Acc: 95.74% | Test Acc: 94.27% | Loss: 0.1907\n",
      "Epoch [432/2000] | Train Acc: 95.78% | Test Acc: 94.19% | Loss: 0.1899\n",
      "Epoch [433/2000] | Train Acc: 95.83% | Test Acc: 94.26% | Loss: 0.1892\n",
      "Epoch [434/2000] | Train Acc: 95.79% | Test Acc: 94.14% | Loss: 0.1884\n",
      "Epoch [435/2000] | Train Acc: 95.85% | Test Acc: 94.30% | Loss: 0.1877\n",
      "Epoch [436/2000] | Train Acc: 95.83% | Test Acc: 94.13% | Loss: 0.1870\n",
      "Epoch [437/2000] | Train Acc: 95.88% | Test Acc: 94.32% | Loss: 0.1863\n",
      "Epoch [438/2000] | Train Acc: 95.89% | Test Acc: 94.13% | Loss: 0.1855\n",
      "Epoch [439/2000] | Train Acc: 95.94% | Test Acc: 94.30% | Loss: 0.1847\n",
      "Epoch [440/2000] | Train Acc: 95.95% | Test Acc: 94.17% | Loss: 0.1839\n",
      "Epoch [441/2000] | Train Acc: 96.01% | Test Acc: 94.27% | Loss: 0.1830\n",
      "Epoch [442/2000] | Train Acc: 95.99% | Test Acc: 94.21% | Loss: 0.1821\n",
      "Epoch [443/2000] | Train Acc: 96.05% | Test Acc: 94.23% | Loss: 0.1812\n",
      "Epoch [444/2000] | Train Acc: 96.04% | Test Acc: 94.26% | Loss: 0.1803\n",
      "Epoch [445/2000] | Train Acc: 96.08% | Test Acc: 94.26% | Loss: 0.1795\n",
      "Epoch [446/2000] | Train Acc: 96.10% | Test Acc: 94.23% | Loss: 0.1788\n",
      "Epoch [447/2000] | Train Acc: 96.13% | Test Acc: 94.20% | Loss: 0.1781\n",
      "Epoch [448/2000] | Train Acc: 96.19% | Test Acc: 94.29% | Loss: 0.1774\n",
      "Epoch [449/2000] | Train Acc: 96.18% | Test Acc: 94.19% | Loss: 0.1767\n",
      "Epoch [450/2000] | Train Acc: 96.24% | Test Acc: 94.32% | Loss: 0.1761\n",
      "Epoch [451/2000] | Train Acc: 96.22% | Test Acc: 94.08% | Loss: 0.1755\n",
      "Epoch [452/2000] | Train Acc: 96.24% | Test Acc: 94.33% | Loss: 0.1750\n",
      "Epoch [453/2000] | Train Acc: 96.25% | Test Acc: 94.16% | Loss: 0.1745\n",
      "Epoch [454/2000] | Train Acc: 96.25% | Test Acc: 94.29% | Loss: 0.1739\n",
      "Epoch [455/2000] | Train Acc: 96.29% | Test Acc: 94.10% | Loss: 0.1733\n",
      "Epoch [456/2000] | Train Acc: 96.29% | Test Acc: 94.36% | Loss: 0.1724\n",
      "Epoch [457/2000] | Train Acc: 96.34% | Test Acc: 94.19% | Loss: 0.1714\n",
      "Epoch [458/2000] | Train Acc: 96.36% | Test Acc: 94.23% | Loss: 0.1703\n",
      "Epoch [459/2000] | Train Acc: 96.37% | Test Acc: 94.26% | Loss: 0.1694\n",
      "Epoch [460/2000] | Train Acc: 96.40% | Test Acc: 94.23% | Loss: 0.1688\n",
      "Epoch [461/2000] | Train Acc: 96.42% | Test Acc: 94.39% | Loss: 0.1683\n",
      "Epoch [462/2000] | Train Acc: 96.45% | Test Acc: 94.13% | Loss: 0.1678\n",
      "Epoch [463/2000] | Train Acc: 96.45% | Test Acc: 94.36% | Loss: 0.1673\n",
      "Epoch [464/2000] | Train Acc: 96.48% | Test Acc: 94.14% | Loss: 0.1666\n",
      "Epoch [465/2000] | Train Acc: 96.48% | Test Acc: 94.32% | Loss: 0.1657\n",
      "Epoch [466/2000] | Train Acc: 96.52% | Test Acc: 94.30% | Loss: 0.1649\n",
      "Epoch [467/2000] | Train Acc: 96.52% | Test Acc: 94.29% | Loss: 0.1641\n",
      "Epoch [468/2000] | Train Acc: 96.53% | Test Acc: 94.35% | Loss: 0.1634\n",
      "Epoch [469/2000] | Train Acc: 96.53% | Test Acc: 94.26% | Loss: 0.1628\n",
      "Epoch [470/2000] | Train Acc: 96.56% | Test Acc: 94.39% | Loss: 0.1623\n",
      "Epoch [471/2000] | Train Acc: 96.57% | Test Acc: 94.16% | Loss: 0.1618\n",
      "Epoch [472/2000] | Train Acc: 96.60% | Test Acc: 94.38% | Loss: 0.1612\n",
      "Epoch [473/2000] | Train Acc: 96.62% | Test Acc: 94.19% | Loss: 0.1606\n",
      "Epoch [474/2000] | Train Acc: 96.64% | Test Acc: 94.38% | Loss: 0.1599\n",
      "Epoch [475/2000] | Train Acc: 96.67% | Test Acc: 94.32% | Loss: 0.1592\n",
      "Epoch [476/2000] | Train Acc: 96.67% | Test Acc: 94.32% | Loss: 0.1585\n",
      "Epoch [477/2000] | Train Acc: 96.67% | Test Acc: 94.26% | Loss: 0.1578\n",
      "Epoch [478/2000] | Train Acc: 96.70% | Test Acc: 94.27% | Loss: 0.1571\n",
      "Epoch [479/2000] | Train Acc: 96.70% | Test Acc: 94.30% | Loss: 0.1565\n",
      "Epoch [480/2000] | Train Acc: 96.70% | Test Acc: 94.30% | Loss: 0.1559\n",
      "Epoch [481/2000] | Train Acc: 96.72% | Test Acc: 94.36% | Loss: 0.1554\n",
      "Epoch [482/2000] | Train Acc: 96.75% | Test Acc: 94.24% | Loss: 0.1548\n",
      "Epoch [483/2000] | Train Acc: 96.77% | Test Acc: 94.38% | Loss: 0.1543\n",
      "Epoch [484/2000] | Train Acc: 96.76% | Test Acc: 94.19% | Loss: 0.1539\n",
      "Epoch [485/2000] | Train Acc: 96.80% | Test Acc: 94.42% | Loss: 0.1534\n",
      "Epoch [486/2000] | Train Acc: 96.82% | Test Acc: 94.24% | Loss: 0.1531\n",
      "Epoch [487/2000] | Train Acc: 96.78% | Test Acc: 94.45% | Loss: 0.1527\n",
      "Epoch [488/2000] | Train Acc: 96.83% | Test Acc: 94.26% | Loss: 0.1525\n",
      "Epoch [489/2000] | Train Acc: 96.82% | Test Acc: 94.45% | Loss: 0.1520\n",
      "Epoch [490/2000] | Train Acc: 96.85% | Test Acc: 94.23% | Loss: 0.1514\n",
      "Epoch [491/2000] | Train Acc: 96.84% | Test Acc: 94.38% | Loss: 0.1504\n",
      "Epoch [492/2000] | Train Acc: 96.87% | Test Acc: 94.32% | Loss: 0.1494\n",
      "Epoch [493/2000] | Train Acc: 96.91% | Test Acc: 94.30% | Loss: 0.1485\n",
      "Epoch [494/2000] | Train Acc: 96.93% | Test Acc: 94.39% | Loss: 0.1480\n",
      "Epoch [495/2000] | Train Acc: 96.91% | Test Acc: 94.19% | Loss: 0.1477\n",
      "Epoch [496/2000] | Train Acc: 96.94% | Test Acc: 94.39% | Loss: 0.1474\n",
      "Epoch [497/2000] | Train Acc: 96.95% | Test Acc: 94.20% | Loss: 0.1470\n",
      "Epoch [498/2000] | Train Acc: 96.95% | Test Acc: 94.41% | Loss: 0.1464\n",
      "Epoch [499/2000] | Train Acc: 96.97% | Test Acc: 94.30% | Loss: 0.1457\n",
      "Epoch [500/2000] | Train Acc: 96.97% | Test Acc: 94.38% | Loss: 0.1449\n",
      "Epoch [501/2000] | Train Acc: 96.95% | Test Acc: 94.36% | Loss: 0.1443\n",
      "Epoch [502/2000] | Train Acc: 96.99% | Test Acc: 94.27% | Loss: 0.1438\n",
      "Epoch [503/2000] | Train Acc: 96.99% | Test Acc: 94.38% | Loss: 0.1434\n",
      "Epoch [504/2000] | Train Acc: 97.02% | Test Acc: 94.26% | Loss: 0.1431\n",
      "Epoch [505/2000] | Train Acc: 97.00% | Test Acc: 94.39% | Loss: 0.1426\n",
      "Epoch [506/2000] | Train Acc: 97.06% | Test Acc: 94.24% | Loss: 0.1421\n",
      "Epoch [507/2000] | Train Acc: 97.04% | Test Acc: 94.41% | Loss: 0.1415\n",
      "Epoch [508/2000] | Train Acc: 97.08% | Test Acc: 94.29% | Loss: 0.1409\n",
      "Epoch [509/2000] | Train Acc: 97.08% | Test Acc: 94.36% | Loss: 0.1403\n",
      "Epoch [510/2000] | Train Acc: 97.06% | Test Acc: 94.39% | Loss: 0.1398\n",
      "Epoch [511/2000] | Train Acc: 97.06% | Test Acc: 94.33% | Loss: 0.1393\n",
      "Epoch [512/2000] | Train Acc: 97.11% | Test Acc: 94.42% | Loss: 0.1389\n",
      "Epoch [513/2000] | Train Acc: 97.10% | Test Acc: 94.27% | Loss: 0.1385\n",
      "Epoch [514/2000] | Train Acc: 97.11% | Test Acc: 94.38% | Loss: 0.1381\n",
      "Epoch [515/2000] | Train Acc: 97.11% | Test Acc: 94.24% | Loss: 0.1377\n",
      "Epoch [516/2000] | Train Acc: 97.12% | Test Acc: 94.47% | Loss: 0.1372\n",
      "Epoch [517/2000] | Train Acc: 97.15% | Test Acc: 94.30% | Loss: 0.1368\n",
      "Epoch [518/2000] | Train Acc: 97.13% | Test Acc: 94.41% | Loss: 0.1363\n",
      "Epoch [519/2000] | Train Acc: 97.15% | Test Acc: 94.30% | Loss: 0.1358\n",
      "Epoch [520/2000] | Train Acc: 97.14% | Test Acc: 94.44% | Loss: 0.1353\n",
      "Epoch [521/2000] | Train Acc: 97.17% | Test Acc: 94.33% | Loss: 0.1349\n",
      "Epoch [522/2000] | Train Acc: 97.18% | Test Acc: 94.44% | Loss: 0.1344\n",
      "Epoch [523/2000] | Train Acc: 97.17% | Test Acc: 94.38% | Loss: 0.1339\n",
      "Epoch [524/2000] | Train Acc: 97.21% | Test Acc: 94.48% | Loss: 0.1334\n",
      "Epoch [525/2000] | Train Acc: 97.18% | Test Acc: 94.39% | Loss: 0.1330\n",
      "Epoch [526/2000] | Train Acc: 97.21% | Test Acc: 94.47% | Loss: 0.1325\n",
      "Epoch [527/2000] | Train Acc: 97.20% | Test Acc: 94.41% | Loss: 0.1321\n",
      "Epoch [528/2000] | Train Acc: 97.23% | Test Acc: 94.47% | Loss: 0.1317\n",
      "Epoch [529/2000] | Train Acc: 97.21% | Test Acc: 94.42% | Loss: 0.1312\n",
      "Epoch [530/2000] | Train Acc: 97.25% | Test Acc: 94.47% | Loss: 0.1308\n",
      "Epoch [531/2000] | Train Acc: 97.24% | Test Acc: 94.36% | Loss: 0.1305\n",
      "Epoch [532/2000] | Train Acc: 97.24% | Test Acc: 94.51% | Loss: 0.1301\n",
      "Epoch [533/2000] | Train Acc: 97.27% | Test Acc: 94.33% | Loss: 0.1298\n",
      "Epoch [534/2000] | Train Acc: 97.23% | Test Acc: 94.53% | Loss: 0.1296\n",
      "Epoch [535/2000] | Train Acc: 97.27% | Test Acc: 94.32% | Loss: 0.1296\n",
      "Epoch [536/2000] | Train Acc: 97.21% | Test Acc: 94.63% | Loss: 0.1297\n",
      "Epoch [537/2000] | Train Acc: 97.30% | Test Acc: 94.32% | Loss: 0.1300\n",
      "Epoch [538/2000] | Train Acc: 97.22% | Test Acc: 94.60% | Loss: 0.1297\n",
      "Epoch [539/2000] | Train Acc: 97.29% | Test Acc: 94.33% | Loss: 0.1291\n",
      "Epoch [540/2000] | Train Acc: 97.23% | Test Acc: 94.47% | Loss: 0.1275\n",
      "Epoch [541/2000] | Train Acc: 97.27% | Test Acc: 94.50% | Loss: 0.1264\n",
      "Epoch [542/2000] | Train Acc: 97.29% | Test Acc: 94.41% | Loss: 0.1260\n",
      "Epoch [543/2000] | Train Acc: 97.27% | Test Acc: 94.56% | Loss: 0.1262\n",
      "Epoch [544/2000] | Train Acc: 97.33% | Test Acc: 94.42% | Loss: 0.1264\n",
      "Epoch [545/2000] | Train Acc: 97.26% | Test Acc: 94.53% | Loss: 0.1258\n",
      "Epoch [546/2000] | Train Acc: 97.35% | Test Acc: 94.53% | Loss: 0.1248\n",
      "Epoch [547/2000] | Train Acc: 97.30% | Test Acc: 94.54% | Loss: 0.1241\n",
      "Epoch [548/2000] | Train Acc: 97.35% | Test Acc: 94.59% | Loss: 0.1238\n",
      "Epoch [549/2000] | Train Acc: 97.36% | Test Acc: 94.44% | Loss: 0.1239\n",
      "Epoch [550/2000] | Train Acc: 97.30% | Test Acc: 94.56% | Loss: 0.1236\n",
      "Epoch [551/2000] | Train Acc: 97.37% | Test Acc: 94.54% | Loss: 0.1231\n",
      "Epoch [552/2000] | Train Acc: 97.36% | Test Acc: 94.51% | Loss: 0.1223\n",
      "Epoch [553/2000] | Train Acc: 97.31% | Test Acc: 94.53% | Loss: 0.1219\n",
      "Epoch [554/2000] | Train Acc: 97.34% | Test Acc: 94.51% | Loss: 0.1217\n",
      "Epoch [555/2000] | Train Acc: 97.34% | Test Acc: 94.54% | Loss: 0.1216\n",
      "Epoch [556/2000] | Train Acc: 97.38% | Test Acc: 94.51% | Loss: 0.1212\n",
      "Epoch [557/2000] | Train Acc: 97.36% | Test Acc: 94.51% | Loss: 0.1207\n",
      "Epoch [558/2000] | Train Acc: 97.34% | Test Acc: 94.53% | Loss: 0.1202\n",
      "Epoch [559/2000] | Train Acc: 97.35% | Test Acc: 94.57% | Loss: 0.1198\n",
      "Epoch [560/2000] | Train Acc: 97.37% | Test Acc: 94.54% | Loss: 0.1196\n",
      "Epoch [561/2000] | Train Acc: 97.40% | Test Acc: 94.54% | Loss: 0.1194\n",
      "Epoch [562/2000] | Train Acc: 97.36% | Test Acc: 94.51% | Loss: 0.1190\n",
      "Epoch [563/2000] | Train Acc: 97.38% | Test Acc: 94.51% | Loss: 0.1186\n",
      "Epoch [564/2000] | Train Acc: 97.35% | Test Acc: 94.53% | Loss: 0.1182\n",
      "Epoch [565/2000] | Train Acc: 97.35% | Test Acc: 94.48% | Loss: 0.1178\n",
      "Epoch [566/2000] | Train Acc: 97.37% | Test Acc: 94.57% | Loss: 0.1175\n",
      "Epoch [567/2000] | Train Acc: 97.38% | Test Acc: 94.51% | Loss: 0.1173\n",
      "Epoch [568/2000] | Train Acc: 97.40% | Test Acc: 94.53% | Loss: 0.1170\n",
      "Epoch [569/2000] | Train Acc: 97.38% | Test Acc: 94.45% | Loss: 0.1166\n",
      "Epoch [570/2000] | Train Acc: 97.39% | Test Acc: 94.53% | Loss: 0.1163\n",
      "Epoch [571/2000] | Train Acc: 97.36% | Test Acc: 94.54% | Loss: 0.1159\n",
      "Epoch [572/2000] | Train Acc: 97.37% | Test Acc: 94.54% | Loss: 0.1156\n",
      "Epoch [573/2000] | Train Acc: 97.38% | Test Acc: 94.51% | Loss: 0.1153\n",
      "Epoch [574/2000] | Train Acc: 97.37% | Test Acc: 94.47% | Loss: 0.1150\n",
      "Epoch [575/2000] | Train Acc: 97.40% | Test Acc: 94.51% | Loss: 0.1147\n",
      "Epoch [576/2000] | Train Acc: 97.40% | Test Acc: 94.51% | Loss: 0.1144\n",
      "Epoch [577/2000] | Train Acc: 97.41% | Test Acc: 94.53% | Loss: 0.1141\n",
      "Epoch [578/2000] | Train Acc: 97.38% | Test Acc: 94.56% | Loss: 0.1138\n",
      "Epoch [579/2000] | Train Acc: 97.41% | Test Acc: 94.51% | Loss: 0.1135\n",
      "Epoch [580/2000] | Train Acc: 97.39% | Test Acc: 94.54% | Loss: 0.1132\n",
      "Epoch [581/2000] | Train Acc: 97.39% | Test Acc: 94.54% | Loss: 0.1129\n",
      "Epoch [582/2000] | Train Acc: 97.40% | Test Acc: 94.53% | Loss: 0.1125\n",
      "Epoch [583/2000] | Train Acc: 97.40% | Test Acc: 94.51% | Loss: 0.1122\n",
      "Epoch [584/2000] | Train Acc: 97.40% | Test Acc: 94.54% | Loss: 0.1119\n",
      "Epoch [585/2000] | Train Acc: 97.40% | Test Acc: 94.54% | Loss: 0.1117\n",
      "Epoch [586/2000] | Train Acc: 97.41% | Test Acc: 94.54% | Loss: 0.1114\n",
      "Epoch [587/2000] | Train Acc: 97.42% | Test Acc: 94.54% | Loss: 0.1111\n",
      "Epoch [588/2000] | Train Acc: 97.41% | Test Acc: 94.57% | Loss: 0.1108\n",
      "Epoch [589/2000] | Train Acc: 97.45% | Test Acc: 94.54% | Loss: 0.1106\n",
      "Epoch [590/2000] | Train Acc: 97.45% | Test Acc: 94.59% | Loss: 0.1103\n",
      "Epoch [591/2000] | Train Acc: 97.43% | Test Acc: 94.54% | Loss: 0.1101\n",
      "Epoch [592/2000] | Train Acc: 97.48% | Test Acc: 94.54% | Loss: 0.1098\n",
      "Epoch [593/2000] | Train Acc: 97.45% | Test Acc: 94.51% | Loss: 0.1096\n",
      "Epoch [594/2000] | Train Acc: 97.48% | Test Acc: 94.44% | Loss: 0.1095\n",
      "Epoch [595/2000] | Train Acc: 97.44% | Test Acc: 94.48% | Loss: 0.1095\n",
      "Epoch [596/2000] | Train Acc: 97.48% | Test Acc: 94.36% | Loss: 0.1097\n",
      "Epoch [597/2000] | Train Acc: 97.43% | Test Acc: 94.57% | Loss: 0.1099\n",
      "Epoch [598/2000] | Train Acc: 97.50% | Test Acc: 94.26% | Loss: 0.1103\n",
      "Epoch [599/2000] | Train Acc: 97.44% | Test Acc: 94.57% | Loss: 0.1100\n",
      "Epoch [600/2000] | Train Acc: 97.49% | Test Acc: 94.39% | Loss: 0.1094\n",
      "Epoch [601/2000] | Train Acc: 97.44% | Test Acc: 94.54% | Loss: 0.1081\n",
      "Epoch [602/2000] | Train Acc: 97.47% | Test Acc: 94.56% | Loss: 0.1071\n",
      "Epoch [603/2000] | Train Acc: 97.48% | Test Acc: 94.44% | Loss: 0.1069\n",
      "Epoch [604/2000] | Train Acc: 97.47% | Test Acc: 94.51% | Loss: 0.1071\n",
      "Epoch [605/2000] | Train Acc: 97.49% | Test Acc: 94.42% | Loss: 0.1074\n",
      "Epoch [606/2000] | Train Acc: 97.47% | Test Acc: 94.51% | Loss: 0.1071\n",
      "Epoch [607/2000] | Train Acc: 97.50% | Test Acc: 94.54% | Loss: 0.1064\n",
      "Epoch [608/2000] | Train Acc: 97.46% | Test Acc: 94.54% | Loss: 0.1056\n",
      "Epoch [609/2000] | Train Acc: 97.47% | Test Acc: 94.56% | Loss: 0.1053\n",
      "Epoch [610/2000] | Train Acc: 97.51% | Test Acc: 94.51% | Loss: 0.1054\n",
      "Epoch [611/2000] | Train Acc: 97.49% | Test Acc: 94.51% | Loss: 0.1054\n",
      "Epoch [612/2000] | Train Acc: 97.51% | Test Acc: 94.51% | Loss: 0.1052\n",
      "Epoch [613/2000] | Train Acc: 97.47% | Test Acc: 94.54% | Loss: 0.1046\n",
      "Epoch [614/2000] | Train Acc: 97.49% | Test Acc: 94.59% | Loss: 0.1041\n",
      "Epoch [615/2000] | Train Acc: 97.49% | Test Acc: 94.56% | Loss: 0.1039\n",
      "Epoch [616/2000] | Train Acc: 97.48% | Test Acc: 94.57% | Loss: 0.1039\n",
      "Epoch [617/2000] | Train Acc: 97.52% | Test Acc: 94.51% | Loss: 0.1038\n",
      "Epoch [618/2000] | Train Acc: 97.48% | Test Acc: 94.59% | Loss: 0.1035\n",
      "Epoch [619/2000] | Train Acc: 97.52% | Test Acc: 94.54% | Loss: 0.1031\n",
      "Epoch [620/2000] | Train Acc: 97.51% | Test Acc: 94.57% | Loss: 0.1028\n",
      "Epoch [621/2000] | Train Acc: 97.50% | Test Acc: 94.66% | Loss: 0.1025\n",
      "Epoch [622/2000] | Train Acc: 97.53% | Test Acc: 94.56% | Loss: 0.1024\n",
      "Epoch [623/2000] | Train Acc: 97.51% | Test Acc: 94.60% | Loss: 0.1022\n",
      "Epoch [624/2000] | Train Acc: 97.51% | Test Acc: 94.56% | Loss: 0.1021\n",
      "Epoch [625/2000] | Train Acc: 97.51% | Test Acc: 94.65% | Loss: 0.1018\n",
      "Epoch [626/2000] | Train Acc: 97.53% | Test Acc: 94.57% | Loss: 0.1015\n",
      "Epoch [627/2000] | Train Acc: 97.51% | Test Acc: 94.57% | Loss: 0.1012\n",
      "Epoch [628/2000] | Train Acc: 97.52% | Test Acc: 94.62% | Loss: 0.1010\n",
      "Epoch [629/2000] | Train Acc: 97.52% | Test Acc: 94.59% | Loss: 0.1008\n",
      "Epoch [630/2000] | Train Acc: 97.51% | Test Acc: 94.66% | Loss: 0.1006\n",
      "Epoch [631/2000] | Train Acc: 97.54% | Test Acc: 94.60% | Loss: 0.1004\n",
      "Epoch [632/2000] | Train Acc: 97.52% | Test Acc: 94.68% | Loss: 0.1002\n",
      "Epoch [633/2000] | Train Acc: 97.55% | Test Acc: 94.60% | Loss: 0.1000\n",
      "Epoch [634/2000] | Train Acc: 97.53% | Test Acc: 94.62% | Loss: 0.0997\n",
      "Epoch [635/2000] | Train Acc: 97.52% | Test Acc: 94.60% | Loss: 0.0995\n",
      "Epoch [636/2000] | Train Acc: 97.55% | Test Acc: 94.62% | Loss: 0.0993\n",
      "Epoch [637/2000] | Train Acc: 97.54% | Test Acc: 94.62% | Loss: 0.0991\n",
      "Epoch [638/2000] | Train Acc: 97.56% | Test Acc: 94.62% | Loss: 0.0989\n",
      "Epoch [639/2000] | Train Acc: 97.54% | Test Acc: 94.65% | Loss: 0.0987\n",
      "Epoch [640/2000] | Train Acc: 97.56% | Test Acc: 94.59% | Loss: 0.0985\n",
      "Epoch [641/2000] | Train Acc: 97.55% | Test Acc: 94.65% | Loss: 0.0983\n",
      "Epoch [642/2000] | Train Acc: 97.57% | Test Acc: 94.62% | Loss: 0.0981\n",
      "Epoch [643/2000] | Train Acc: 97.54% | Test Acc: 94.63% | Loss: 0.0979\n",
      "Epoch [644/2000] | Train Acc: 97.58% | Test Acc: 94.57% | Loss: 0.0977\n",
      "Epoch [645/2000] | Train Acc: 97.55% | Test Acc: 94.57% | Loss: 0.0975\n",
      "Epoch [646/2000] | Train Acc: 97.58% | Test Acc: 94.57% | Loss: 0.0973\n",
      "Epoch [647/2000] | Train Acc: 97.56% | Test Acc: 94.59% | Loss: 0.0971\n",
      "Epoch [648/2000] | Train Acc: 97.56% | Test Acc: 94.56% | Loss: 0.0969\n",
      "Epoch [649/2000] | Train Acc: 97.56% | Test Acc: 94.57% | Loss: 0.0967\n",
      "Epoch [650/2000] | Train Acc: 97.57% | Test Acc: 94.56% | Loss: 0.0965\n",
      "Epoch [651/2000] | Train Acc: 97.57% | Test Acc: 94.59% | Loss: 0.0963\n",
      "Epoch [652/2000] | Train Acc: 97.58% | Test Acc: 94.56% | Loss: 0.0961\n",
      "Epoch [653/2000] | Train Acc: 97.57% | Test Acc: 94.57% | Loss: 0.0959\n",
      "Epoch [654/2000] | Train Acc: 97.58% | Test Acc: 94.56% | Loss: 0.0957\n",
      "Epoch [655/2000] | Train Acc: 97.58% | Test Acc: 94.56% | Loss: 0.0955\n",
      "Epoch [656/2000] | Train Acc: 97.59% | Test Acc: 94.56% | Loss: 0.0954\n",
      "Epoch [657/2000] | Train Acc: 97.58% | Test Acc: 94.59% | Loss: 0.0952\n",
      "Epoch [658/2000] | Train Acc: 97.61% | Test Acc: 94.54% | Loss: 0.0950\n",
      "Epoch [659/2000] | Train Acc: 97.58% | Test Acc: 94.63% | Loss: 0.0948\n",
      "Epoch [660/2000] | Train Acc: 97.61% | Test Acc: 94.54% | Loss: 0.0947\n",
      "Epoch [661/2000] | Train Acc: 97.57% | Test Acc: 94.62% | Loss: 0.0945\n",
      "Epoch [662/2000] | Train Acc: 97.60% | Test Acc: 94.48% | Loss: 0.0945\n",
      "Epoch [663/2000] | Train Acc: 97.59% | Test Acc: 94.57% | Loss: 0.0945\n",
      "Epoch [664/2000] | Train Acc: 97.63% | Test Acc: 94.33% | Loss: 0.0947\n",
      "Epoch [665/2000] | Train Acc: 97.56% | Test Acc: 94.74% | Loss: 0.0952\n",
      "Epoch [666/2000] | Train Acc: 97.69% | Test Acc: 94.26% | Loss: 0.0963\n",
      "Epoch [667/2000] | Train Acc: 97.51% | Test Acc: 94.75% | Loss: 0.0968\n",
      "Epoch [668/2000] | Train Acc: 97.61% | Test Acc: 94.26% | Loss: 0.0974\n",
      "Epoch [669/2000] | Train Acc: 97.53% | Test Acc: 94.60% | Loss: 0.0953\n",
      "Epoch [670/2000] | Train Acc: 97.61% | Test Acc: 94.59% | Loss: 0.0934\n",
      "Epoch [671/2000] | Train Acc: 97.61% | Test Acc: 94.41% | Loss: 0.0928\n",
      "Epoch [672/2000] | Train Acc: 97.58% | Test Acc: 94.75% | Loss: 0.0936\n",
      "Epoch [673/2000] | Train Acc: 97.69% | Test Acc: 94.36% | Loss: 0.0946\n",
      "Epoch [674/2000] | Train Acc: 97.57% | Test Acc: 94.62% | Loss: 0.0938\n",
      "Epoch [675/2000] | Train Acc: 97.61% | Test Acc: 94.65% | Loss: 0.0925\n",
      "Epoch [676/2000] | Train Acc: 97.61% | Test Acc: 94.47% | Loss: 0.0920\n",
      "Epoch [677/2000] | Train Acc: 97.58% | Test Acc: 94.63% | Loss: 0.0925\n",
      "Epoch [678/2000] | Train Acc: 97.65% | Test Acc: 94.47% | Loss: 0.0930\n",
      "Epoch [679/2000] | Train Acc: 97.58% | Test Acc: 94.59% | Loss: 0.0922\n",
      "Epoch [680/2000] | Train Acc: 97.63% | Test Acc: 94.66% | Loss: 0.0914\n",
      "Epoch [681/2000] | Train Acc: 97.63% | Test Acc: 94.50% | Loss: 0.0913\n",
      "Epoch [682/2000] | Train Acc: 97.61% | Test Acc: 94.56% | Loss: 0.0917\n",
      "Epoch [683/2000] | Train Acc: 97.64% | Test Acc: 94.48% | Loss: 0.0917\n",
      "Epoch [684/2000] | Train Acc: 97.61% | Test Acc: 94.54% | Loss: 0.0910\n",
      "Epoch [685/2000] | Train Acc: 97.63% | Test Acc: 94.60% | Loss: 0.0905\n",
      "Epoch [686/2000] | Train Acc: 97.65% | Test Acc: 94.50% | Loss: 0.0906\n",
      "Epoch [687/2000] | Train Acc: 97.61% | Test Acc: 94.59% | Loss: 0.0907\n",
      "Epoch [688/2000] | Train Acc: 97.63% | Test Acc: 94.48% | Loss: 0.0905\n",
      "Epoch [689/2000] | Train Acc: 97.61% | Test Acc: 94.50% | Loss: 0.0900\n",
      "Epoch [690/2000] | Train Acc: 97.61% | Test Acc: 94.62% | Loss: 0.0898\n",
      "Epoch [691/2000] | Train Acc: 97.65% | Test Acc: 94.45% | Loss: 0.0898\n",
      "Epoch [692/2000] | Train Acc: 97.64% | Test Acc: 94.62% | Loss: 0.0898\n",
      "Epoch [693/2000] | Train Acc: 97.64% | Test Acc: 94.48% | Loss: 0.0895\n",
      "Epoch [694/2000] | Train Acc: 97.62% | Test Acc: 94.53% | Loss: 0.0892\n",
      "Epoch [695/2000] | Train Acc: 97.62% | Test Acc: 94.60% | Loss: 0.0890\n",
      "Epoch [696/2000] | Train Acc: 97.67% | Test Acc: 94.50% | Loss: 0.0890\n",
      "Epoch [697/2000] | Train Acc: 97.65% | Test Acc: 94.60% | Loss: 0.0889\n",
      "Epoch [698/2000] | Train Acc: 97.65% | Test Acc: 94.50% | Loss: 0.0887\n",
      "Epoch [699/2000] | Train Acc: 97.63% | Test Acc: 94.53% | Loss: 0.0884\n",
      "Epoch [700/2000] | Train Acc: 97.63% | Test Acc: 94.62% | Loss: 0.0883\n",
      "Epoch [701/2000] | Train Acc: 97.67% | Test Acc: 94.51% | Loss: 0.0883\n",
      "Epoch [702/2000] | Train Acc: 97.64% | Test Acc: 94.60% | Loss: 0.0881\n",
      "Epoch [703/2000] | Train Acc: 97.66% | Test Acc: 94.53% | Loss: 0.0879\n",
      "Epoch [704/2000] | Train Acc: 97.63% | Test Acc: 94.51% | Loss: 0.0877\n",
      "Epoch [705/2000] | Train Acc: 97.64% | Test Acc: 94.62% | Loss: 0.0876\n",
      "Epoch [706/2000] | Train Acc: 97.68% | Test Acc: 94.53% | Loss: 0.0875\n",
      "Epoch [707/2000] | Train Acc: 97.66% | Test Acc: 94.60% | Loss: 0.0874\n",
      "Epoch [708/2000] | Train Acc: 97.66% | Test Acc: 94.53% | Loss: 0.0872\n",
      "Epoch [709/2000] | Train Acc: 97.65% | Test Acc: 94.56% | Loss: 0.0871\n",
      "Epoch [710/2000] | Train Acc: 97.68% | Test Acc: 94.59% | Loss: 0.0869\n",
      "Epoch [711/2000] | Train Acc: 97.68% | Test Acc: 94.54% | Loss: 0.0868\n",
      "Epoch [712/2000] | Train Acc: 97.65% | Test Acc: 94.57% | Loss: 0.0867\n",
      "Epoch [713/2000] | Train Acc: 97.68% | Test Acc: 94.51% | Loss: 0.0865\n",
      "Epoch [714/2000] | Train Acc: 97.65% | Test Acc: 94.59% | Loss: 0.0864\n",
      "Epoch [715/2000] | Train Acc: 97.68% | Test Acc: 94.57% | Loss: 0.0862\n",
      "Epoch [716/2000] | Train Acc: 97.68% | Test Acc: 94.56% | Loss: 0.0861\n",
      "Epoch [717/2000] | Train Acc: 97.66% | Test Acc: 94.57% | Loss: 0.0860\n",
      "Epoch [718/2000] | Train Acc: 97.68% | Test Acc: 94.54% | Loss: 0.0858\n",
      "Epoch [719/2000] | Train Acc: 97.66% | Test Acc: 94.57% | Loss: 0.0857\n",
      "Epoch [720/2000] | Train Acc: 97.68% | Test Acc: 94.53% | Loss: 0.0856\n",
      "Epoch [721/2000] | Train Acc: 97.67% | Test Acc: 94.57% | Loss: 0.0854\n",
      "Epoch [722/2000] | Train Acc: 97.71% | Test Acc: 94.57% | Loss: 0.0853\n",
      "Epoch [723/2000] | Train Acc: 97.69% | Test Acc: 94.51% | Loss: 0.0852\n",
      "Epoch [724/2000] | Train Acc: 97.67% | Test Acc: 94.57% | Loss: 0.0851\n",
      "Epoch [725/2000] | Train Acc: 97.70% | Test Acc: 94.51% | Loss: 0.0849\n",
      "Epoch [726/2000] | Train Acc: 97.67% | Test Acc: 94.60% | Loss: 0.0848\n",
      "Epoch [727/2000] | Train Acc: 97.71% | Test Acc: 94.50% | Loss: 0.0847\n",
      "Epoch [728/2000] | Train Acc: 97.68% | Test Acc: 94.59% | Loss: 0.0846\n",
      "Epoch [729/2000] | Train Acc: 97.71% | Test Acc: 94.57% | Loss: 0.0844\n",
      "Epoch [730/2000] | Train Acc: 97.70% | Test Acc: 94.57% | Loss: 0.0843\n",
      "Epoch [731/2000] | Train Acc: 97.71% | Test Acc: 94.57% | Loss: 0.0842\n",
      "Epoch [732/2000] | Train Acc: 97.71% | Test Acc: 94.54% | Loss: 0.0840\n",
      "Epoch [733/2000] | Train Acc: 97.69% | Test Acc: 94.59% | Loss: 0.0839\n",
      "Epoch [734/2000] | Train Acc: 97.71% | Test Acc: 94.54% | Loss: 0.0838\n",
      "Epoch [735/2000] | Train Acc: 97.68% | Test Acc: 94.56% | Loss: 0.0837\n",
      "Epoch [736/2000] | Train Acc: 97.71% | Test Acc: 94.54% | Loss: 0.0836\n",
      "Epoch [737/2000] | Train Acc: 97.69% | Test Acc: 94.57% | Loss: 0.0834\n",
      "Epoch [738/2000] | Train Acc: 97.71% | Test Acc: 94.54% | Loss: 0.0833\n",
      "Epoch [739/2000] | Train Acc: 97.70% | Test Acc: 94.57% | Loss: 0.0832\n",
      "Epoch [740/2000] | Train Acc: 97.71% | Test Acc: 94.54% | Loss: 0.0831\n",
      "Epoch [741/2000] | Train Acc: 97.71% | Test Acc: 94.57% | Loss: 0.0830\n",
      "Epoch [742/2000] | Train Acc: 97.71% | Test Acc: 94.56% | Loss: 0.0828\n",
      "Epoch [743/2000] | Train Acc: 97.72% | Test Acc: 94.56% | Loss: 0.0827\n",
      "Epoch [744/2000] | Train Acc: 97.72% | Test Acc: 94.56% | Loss: 0.0826\n",
      "Epoch [745/2000] | Train Acc: 97.72% | Test Acc: 94.56% | Loss: 0.0825\n",
      "Epoch [746/2000] | Train Acc: 97.72% | Test Acc: 94.56% | Loss: 0.0824\n",
      "Epoch [747/2000] | Train Acc: 97.72% | Test Acc: 94.56% | Loss: 0.0823\n",
      "Epoch [748/2000] | Train Acc: 97.73% | Test Acc: 94.57% | Loss: 0.0821\n",
      "Epoch [749/2000] | Train Acc: 97.72% | Test Acc: 94.56% | Loss: 0.0820\n",
      "Epoch [750/2000] | Train Acc: 97.73% | Test Acc: 94.57% | Loss: 0.0819\n",
      "Epoch [751/2000] | Train Acc: 97.73% | Test Acc: 94.54% | Loss: 0.0818\n",
      "Epoch [752/2000] | Train Acc: 97.73% | Test Acc: 94.57% | Loss: 0.0817\n",
      "Epoch [753/2000] | Train Acc: 97.73% | Test Acc: 94.53% | Loss: 0.0816\n",
      "Epoch [754/2000] | Train Acc: 97.71% | Test Acc: 94.57% | Loss: 0.0815\n",
      "Epoch [755/2000] | Train Acc: 97.74% | Test Acc: 94.53% | Loss: 0.0814\n",
      "Epoch [756/2000] | Train Acc: 97.71% | Test Acc: 94.62% | Loss: 0.0813\n",
      "Epoch [757/2000] | Train Acc: 97.73% | Test Acc: 94.51% | Loss: 0.0812\n",
      "Epoch [758/2000] | Train Acc: 97.70% | Test Acc: 94.62% | Loss: 0.0811\n",
      "Epoch [759/2000] | Train Acc: 97.72% | Test Acc: 94.48% | Loss: 0.0811\n",
      "Epoch [760/2000] | Train Acc: 97.70% | Test Acc: 94.51% | Loss: 0.0811\n",
      "Epoch [761/2000] | Train Acc: 97.74% | Test Acc: 94.47% | Loss: 0.0812\n",
      "Epoch [762/2000] | Train Acc: 97.67% | Test Acc: 94.68% | Loss: 0.0815\n",
      "Epoch [763/2000] | Train Acc: 97.77% | Test Acc: 94.24% | Loss: 0.0821\n",
      "Epoch [764/2000] | Train Acc: 97.59% | Test Acc: 94.66% | Loss: 0.0827\n",
      "Epoch [765/2000] | Train Acc: 97.74% | Test Acc: 94.17% | Loss: 0.0839\n",
      "Epoch [766/2000] | Train Acc: 97.59% | Test Acc: 94.68% | Loss: 0.0835\n",
      "Epoch [767/2000] | Train Acc: 97.77% | Test Acc: 94.47% | Loss: 0.0827\n",
      "Epoch [768/2000] | Train Acc: 97.67% | Test Acc: 94.53% | Loss: 0.0807\n",
      "Epoch [769/2000] | Train Acc: 97.71% | Test Acc: 94.54% | Loss: 0.0799\n",
      "Epoch [770/2000] | Train Acc: 97.76% | Test Acc: 94.35% | Loss: 0.0804\n",
      "Epoch [771/2000] | Train Acc: 97.65% | Test Acc: 94.65% | Loss: 0.0812\n",
      "Epoch [772/2000] | Train Acc: 97.77% | Test Acc: 94.42% | Loss: 0.0815\n",
      "Epoch [773/2000] | Train Acc: 97.66% | Test Acc: 94.56% | Loss: 0.0804\n",
      "Epoch [774/2000] | Train Acc: 97.74% | Test Acc: 94.56% | Loss: 0.0795\n",
      "Epoch [775/2000] | Train Acc: 97.77% | Test Acc: 94.50% | Loss: 0.0794\n",
      "Epoch [776/2000] | Train Acc: 97.70% | Test Acc: 94.59% | Loss: 0.0800\n",
      "Epoch [777/2000] | Train Acc: 97.75% | Test Acc: 94.48% | Loss: 0.0803\n",
      "Epoch [778/2000] | Train Acc: 97.69% | Test Acc: 94.59% | Loss: 0.0796\n",
      "Epoch [779/2000] | Train Acc: 97.77% | Test Acc: 94.62% | Loss: 0.0790\n",
      "Epoch [780/2000] | Train Acc: 97.78% | Test Acc: 94.51% | Loss: 0.0789\n",
      "Epoch [781/2000] | Train Acc: 97.71% | Test Acc: 94.54% | Loss: 0.0792\n",
      "Epoch [782/2000] | Train Acc: 97.76% | Test Acc: 94.53% | Loss: 0.0793\n",
      "Epoch [783/2000] | Train Acc: 97.70% | Test Acc: 94.62% | Loss: 0.0789\n",
      "Epoch [784/2000] | Train Acc: 97.76% | Test Acc: 94.57% | Loss: 0.0785\n",
      "Epoch [785/2000] | Train Acc: 97.79% | Test Acc: 94.48% | Loss: 0.0783\n",
      "Epoch [786/2000] | Train Acc: 97.72% | Test Acc: 94.51% | Loss: 0.0785\n",
      "Epoch [787/2000] | Train Acc: 97.76% | Test Acc: 94.48% | Loss: 0.0786\n",
      "Epoch [788/2000] | Train Acc: 97.70% | Test Acc: 94.63% | Loss: 0.0783\n",
      "Epoch [789/2000] | Train Acc: 97.78% | Test Acc: 94.63% | Loss: 0.0780\n",
      "Epoch [790/2000] | Train Acc: 97.79% | Test Acc: 94.53% | Loss: 0.0778\n",
      "Epoch [791/2000] | Train Acc: 97.74% | Test Acc: 94.54% | Loss: 0.0779\n",
      "Epoch [792/2000] | Train Acc: 97.78% | Test Acc: 94.50% | Loss: 0.0779\n",
      "Epoch [793/2000] | Train Acc: 97.72% | Test Acc: 94.62% | Loss: 0.0778\n",
      "Epoch [794/2000] | Train Acc: 97.77% | Test Acc: 94.60% | Loss: 0.0776\n",
      "Epoch [795/2000] | Train Acc: 97.78% | Test Acc: 94.50% | Loss: 0.0774\n",
      "Epoch [796/2000] | Train Acc: 97.74% | Test Acc: 94.62% | Loss: 0.0773\n",
      "Epoch [797/2000] | Train Acc: 97.78% | Test Acc: 94.47% | Loss: 0.0773\n",
      "Epoch [798/2000] | Train Acc: 97.72% | Test Acc: 94.62% | Loss: 0.0773\n",
      "Epoch [799/2000] | Train Acc: 97.78% | Test Acc: 94.53% | Loss: 0.0771\n",
      "Epoch [800/2000] | Train Acc: 97.76% | Test Acc: 94.57% | Loss: 0.0770\n",
      "Epoch [801/2000] | Train Acc: 97.79% | Test Acc: 94.65% | Loss: 0.0768\n",
      "Epoch [802/2000] | Train Acc: 97.78% | Test Acc: 94.53% | Loss: 0.0768\n",
      "Epoch [803/2000] | Train Acc: 97.75% | Test Acc: 94.63% | Loss: 0.0767\n",
      "Epoch [804/2000] | Train Acc: 97.78% | Test Acc: 94.54% | Loss: 0.0767\n",
      "Epoch [805/2000] | Train Acc: 97.76% | Test Acc: 94.63% | Loss: 0.0765\n",
      "Epoch [806/2000] | Train Acc: 97.78% | Test Acc: 94.56% | Loss: 0.0764\n",
      "Epoch [807/2000] | Train Acc: 97.77% | Test Acc: 94.57% | Loss: 0.0763\n",
      "Epoch [808/2000] | Train Acc: 97.78% | Test Acc: 94.63% | Loss: 0.0762\n",
      "Epoch [809/2000] | Train Acc: 97.79% | Test Acc: 94.54% | Loss: 0.0762\n",
      "Epoch [810/2000] | Train Acc: 97.76% | Test Acc: 94.66% | Loss: 0.0761\n",
      "Epoch [811/2000] | Train Acc: 97.79% | Test Acc: 94.56% | Loss: 0.0760\n",
      "Epoch [812/2000] | Train Acc: 97.77% | Test Acc: 94.62% | Loss: 0.0759\n",
      "Epoch [813/2000] | Train Acc: 97.79% | Test Acc: 94.57% | Loss: 0.0758\n",
      "Epoch [814/2000] | Train Acc: 97.79% | Test Acc: 94.57% | Loss: 0.0757\n",
      "Epoch [815/2000] | Train Acc: 97.80% | Test Acc: 94.63% | Loss: 0.0756\n",
      "Epoch [816/2000] | Train Acc: 97.78% | Test Acc: 94.54% | Loss: 0.0755\n",
      "Epoch [817/2000] | Train Acc: 97.78% | Test Acc: 94.59% | Loss: 0.0755\n",
      "Epoch [818/2000] | Train Acc: 97.79% | Test Acc: 94.53% | Loss: 0.0754\n",
      "Epoch [819/2000] | Train Acc: 97.79% | Test Acc: 94.57% | Loss: 0.0753\n",
      "Epoch [820/2000] | Train Acc: 97.79% | Test Acc: 94.57% | Loss: 0.0752\n",
      "Epoch [821/2000] | Train Acc: 97.79% | Test Acc: 94.59% | Loss: 0.0751\n",
      "Epoch [822/2000] | Train Acc: 97.80% | Test Acc: 94.57% | Loss: 0.0750\n",
      "Epoch [823/2000] | Train Acc: 97.80% | Test Acc: 94.53% | Loss: 0.0749\n",
      "Epoch [824/2000] | Train Acc: 97.80% | Test Acc: 94.57% | Loss: 0.0749\n",
      "Epoch [825/2000] | Train Acc: 97.80% | Test Acc: 94.54% | Loss: 0.0748\n",
      "Epoch [826/2000] | Train Acc: 97.78% | Test Acc: 94.57% | Loss: 0.0747\n",
      "Epoch [827/2000] | Train Acc: 97.79% | Test Acc: 94.54% | Loss: 0.0746\n",
      "Epoch [828/2000] | Train Acc: 97.80% | Test Acc: 94.57% | Loss: 0.0745\n",
      "Epoch [829/2000] | Train Acc: 97.80% | Test Acc: 94.56% | Loss: 0.0745\n",
      "Epoch [830/2000] | Train Acc: 97.80% | Test Acc: 94.57% | Loss: 0.0744\n",
      "Epoch [831/2000] | Train Acc: 97.80% | Test Acc: 94.57% | Loss: 0.0743\n",
      "Epoch [832/2000] | Train Acc: 97.80% | Test Acc: 94.57% | Loss: 0.0742\n",
      "Epoch [833/2000] | Train Acc: 97.81% | Test Acc: 94.60% | Loss: 0.0741\n",
      "Epoch [834/2000] | Train Acc: 97.81% | Test Acc: 94.59% | Loss: 0.0740\n",
      "Epoch [835/2000] | Train Acc: 97.81% | Test Acc: 94.60% | Loss: 0.0740\n",
      "Epoch [836/2000] | Train Acc: 97.81% | Test Acc: 94.57% | Loss: 0.0739\n",
      "Epoch [837/2000] | Train Acc: 97.81% | Test Acc: 94.57% | Loss: 0.0738\n",
      "Epoch [838/2000] | Train Acc: 97.82% | Test Acc: 94.59% | Loss: 0.0737\n",
      "Epoch [839/2000] | Train Acc: 97.81% | Test Acc: 94.57% | Loss: 0.0737\n",
      "Epoch [840/2000] | Train Acc: 97.81% | Test Acc: 94.63% | Loss: 0.0736\n",
      "Epoch [841/2000] | Train Acc: 97.82% | Test Acc: 94.59% | Loss: 0.0735\n",
      "Epoch [842/2000] | Train Acc: 97.82% | Test Acc: 94.60% | Loss: 0.0734\n",
      "Epoch [843/2000] | Train Acc: 97.81% | Test Acc: 94.59% | Loss: 0.0734\n",
      "Epoch [844/2000] | Train Acc: 97.80% | Test Acc: 94.57% | Loss: 0.0733\n",
      "Epoch [845/2000] | Train Acc: 97.80% | Test Acc: 94.59% | Loss: 0.0732\n",
      "Epoch [846/2000] | Train Acc: 97.81% | Test Acc: 94.51% | Loss: 0.0731\n",
      "Epoch [847/2000] | Train Acc: 97.81% | Test Acc: 94.56% | Loss: 0.0731\n",
      "Epoch [848/2000] | Train Acc: 97.82% | Test Acc: 94.56% | Loss: 0.0730\n",
      "Epoch [849/2000] | Train Acc: 97.80% | Test Acc: 94.54% | Loss: 0.0730\n",
      "Epoch [850/2000] | Train Acc: 97.81% | Test Acc: 94.53% | Loss: 0.0730\n",
      "Epoch [851/2000] | Train Acc: 97.76% | Test Acc: 94.51% | Loss: 0.0730\n",
      "Epoch [852/2000] | Train Acc: 97.83% | Test Acc: 94.51% | Loss: 0.0731\n",
      "Epoch [853/2000] | Train Acc: 97.75% | Test Acc: 94.68% | Loss: 0.0733\n",
      "Epoch [854/2000] | Train Acc: 97.84% | Test Acc: 94.30% | Loss: 0.0737\n",
      "Epoch [855/2000] | Train Acc: 97.72% | Test Acc: 94.74% | Loss: 0.0742\n",
      "Epoch [856/2000] | Train Acc: 97.84% | Test Acc: 94.32% | Loss: 0.0751\n",
      "Epoch [857/2000] | Train Acc: 97.67% | Test Acc: 94.75% | Loss: 0.0751\n",
      "Epoch [858/2000] | Train Acc: 97.84% | Test Acc: 94.35% | Loss: 0.0752\n",
      "Epoch [859/2000] | Train Acc: 97.72% | Test Acc: 94.51% | Loss: 0.0737\n",
      "Epoch [860/2000] | Train Acc: 97.82% | Test Acc: 94.60% | Loss: 0.0725\n",
      "Epoch [861/2000] | Train Acc: 97.84% | Test Acc: 94.56% | Loss: 0.0720\n",
      "Epoch [862/2000] | Train Acc: 97.76% | Test Acc: 94.68% | Loss: 0.0724\n",
      "Epoch [863/2000] | Train Acc: 97.85% | Test Acc: 94.45% | Loss: 0.0732\n",
      "Epoch [864/2000] | Train Acc: 97.73% | Test Acc: 94.59% | Loss: 0.0732\n",
      "Epoch [865/2000] | Train Acc: 97.84% | Test Acc: 94.48% | Loss: 0.0728\n",
      "Epoch [866/2000] | Train Acc: 97.76% | Test Acc: 94.57% | Loss: 0.0719\n",
      "Epoch [867/2000] | Train Acc: 97.84% | Test Acc: 94.54% | Loss: 0.0716\n",
      "Epoch [868/2000] | Train Acc: 97.82% | Test Acc: 94.51% | Loss: 0.0718\n",
      "Epoch [869/2000] | Train Acc: 97.76% | Test Acc: 94.59% | Loss: 0.0722\n",
      "Epoch [870/2000] | Train Acc: 97.85% | Test Acc: 94.51% | Loss: 0.0722\n",
      "Epoch [871/2000] | Train Acc: 97.77% | Test Acc: 94.59% | Loss: 0.0718\n",
      "Epoch [872/2000] | Train Acc: 97.83% | Test Acc: 94.62% | Loss: 0.0714\n",
      "Epoch [873/2000] | Train Acc: 97.83% | Test Acc: 94.57% | Loss: 0.0712\n",
      "Epoch [874/2000] | Train Acc: 97.80% | Test Acc: 94.57% | Loss: 0.0713\n",
      "Epoch [875/2000] | Train Acc: 97.84% | Test Acc: 94.53% | Loss: 0.0715\n",
      "Epoch [876/2000] | Train Acc: 97.77% | Test Acc: 94.54% | Loss: 0.0714\n",
      "Epoch [877/2000] | Train Acc: 97.84% | Test Acc: 94.62% | Loss: 0.0712\n",
      "Epoch [878/2000] | Train Acc: 97.85% | Test Acc: 94.63% | Loss: 0.0709\n",
      "Epoch [879/2000] | Train Acc: 97.85% | Test Acc: 94.60% | Loss: 0.0708\n",
      "Epoch [880/2000] | Train Acc: 97.84% | Test Acc: 94.54% | Loss: 0.0709\n",
      "Epoch [881/2000] | Train Acc: 97.79% | Test Acc: 94.53% | Loss: 0.0709\n",
      "Epoch [882/2000] | Train Acc: 97.84% | Test Acc: 94.57% | Loss: 0.0709\n",
      "Epoch [883/2000] | Train Acc: 97.81% | Test Acc: 94.60% | Loss: 0.0707\n",
      "Epoch [884/2000] | Train Acc: 97.83% | Test Acc: 94.63% | Loss: 0.0705\n",
      "Epoch [885/2000] | Train Acc: 97.85% | Test Acc: 94.62% | Loss: 0.0704\n",
      "Epoch [886/2000] | Train Acc: 97.85% | Test Acc: 94.63% | Loss: 0.0704\n",
      "Epoch [887/2000] | Train Acc: 97.83% | Test Acc: 94.59% | Loss: 0.0704\n",
      "Epoch [888/2000] | Train Acc: 97.83% | Test Acc: 94.62% | Loss: 0.0704\n",
      "Epoch [889/2000] | Train Acc: 97.84% | Test Acc: 94.63% | Loss: 0.0703\n",
      "Epoch [890/2000] | Train Acc: 97.86% | Test Acc: 94.62% | Loss: 0.0701\n",
      "Epoch [891/2000] | Train Acc: 97.83% | Test Acc: 94.66% | Loss: 0.0700\n",
      "Epoch [892/2000] | Train Acc: 97.85% | Test Acc: 94.65% | Loss: 0.0700\n",
      "Epoch [893/2000] | Train Acc: 97.86% | Test Acc: 94.65% | Loss: 0.0699\n",
      "Epoch [894/2000] | Train Acc: 97.84% | Test Acc: 94.68% | Loss: 0.0699\n",
      "Epoch [895/2000] | Train Acc: 97.85% | Test Acc: 94.65% | Loss: 0.0698\n",
      "Epoch [896/2000] | Train Acc: 97.84% | Test Acc: 94.65% | Loss: 0.0698\n",
      "Epoch [897/2000] | Train Acc: 97.85% | Test Acc: 94.66% | Loss: 0.0697\n",
      "Epoch [898/2000] | Train Acc: 97.84% | Test Acc: 94.65% | Loss: 0.0696\n",
      "Epoch [899/2000] | Train Acc: 97.86% | Test Acc: 94.66% | Loss: 0.0695\n",
      "Epoch [900/2000] | Train Acc: 97.86% | Test Acc: 94.63% | Loss: 0.0695\n",
      "Epoch [901/2000] | Train Acc: 97.84% | Test Acc: 94.65% | Loss: 0.0694\n",
      "Epoch [902/2000] | Train Acc: 97.87% | Test Acc: 94.62% | Loss: 0.0694\n",
      "Epoch [903/2000] | Train Acc: 97.85% | Test Acc: 94.65% | Loss: 0.0693\n",
      "Epoch [904/2000] | Train Acc: 97.87% | Test Acc: 94.60% | Loss: 0.0692\n",
      "Epoch [905/2000] | Train Acc: 97.85% | Test Acc: 94.63% | Loss: 0.0692\n",
      "Epoch [906/2000] | Train Acc: 97.87% | Test Acc: 94.62% | Loss: 0.0691\n",
      "Epoch [907/2000] | Train Acc: 97.85% | Test Acc: 94.63% | Loss: 0.0690\n",
      "Epoch [908/2000] | Train Acc: 97.86% | Test Acc: 94.63% | Loss: 0.0690\n",
      "Epoch [909/2000] | Train Acc: 97.85% | Test Acc: 94.60% | Loss: 0.0689\n",
      "Epoch [910/2000] | Train Acc: 97.85% | Test Acc: 94.62% | Loss: 0.0688\n",
      "Epoch [911/2000] | Train Acc: 97.86% | Test Acc: 94.63% | Loss: 0.0688\n",
      "Epoch [912/2000] | Train Acc: 97.85% | Test Acc: 94.65% | Loss: 0.0687\n",
      "Epoch [913/2000] | Train Acc: 97.86% | Test Acc: 94.60% | Loss: 0.0687\n",
      "Epoch [914/2000] | Train Acc: 97.85% | Test Acc: 94.65% | Loss: 0.0686\n",
      "Epoch [915/2000] | Train Acc: 97.87% | Test Acc: 94.63% | Loss: 0.0686\n",
      "Epoch [916/2000] | Train Acc: 97.85% | Test Acc: 94.65% | Loss: 0.0685\n",
      "Epoch [917/2000] | Train Acc: 97.88% | Test Acc: 94.62% | Loss: 0.0684\n",
      "Epoch [918/2000] | Train Acc: 97.85% | Test Acc: 94.65% | Loss: 0.0684\n",
      "Epoch [919/2000] | Train Acc: 97.88% | Test Acc: 94.62% | Loss: 0.0683\n",
      "Epoch [920/2000] | Train Acc: 97.86% | Test Acc: 94.63% | Loss: 0.0683\n",
      "Epoch [921/2000] | Train Acc: 97.87% | Test Acc: 94.63% | Loss: 0.0682\n",
      "Epoch [922/2000] | Train Acc: 97.86% | Test Acc: 94.66% | Loss: 0.0682\n",
      "Epoch [923/2000] | Train Acc: 97.88% | Test Acc: 94.63% | Loss: 0.0681\n",
      "Epoch [924/2000] | Train Acc: 97.86% | Test Acc: 94.63% | Loss: 0.0681\n",
      "Epoch [925/2000] | Train Acc: 97.87% | Test Acc: 94.65% | Loss: 0.0680\n",
      "Epoch [926/2000] | Train Acc: 97.86% | Test Acc: 94.57% | Loss: 0.0680\n",
      "Epoch [927/2000] | Train Acc: 97.85% | Test Acc: 94.63% | Loss: 0.0680\n",
      "Epoch [928/2000] | Train Acc: 97.86% | Test Acc: 94.57% | Loss: 0.0680\n",
      "Epoch [929/2000] | Train Acc: 97.84% | Test Acc: 94.59% | Loss: 0.0680\n",
      "Epoch [930/2000] | Train Acc: 97.86% | Test Acc: 94.51% | Loss: 0.0680\n",
      "Epoch [931/2000] | Train Acc: 97.83% | Test Acc: 94.63% | Loss: 0.0681\n",
      "Epoch [932/2000] | Train Acc: 97.90% | Test Acc: 94.45% | Loss: 0.0683\n",
      "Epoch [933/2000] | Train Acc: 97.79% | Test Acc: 94.68% | Loss: 0.0684\n",
      "Epoch [934/2000] | Train Acc: 97.91% | Test Acc: 94.41% | Loss: 0.0688\n",
      "Epoch [935/2000] | Train Acc: 97.78% | Test Acc: 94.71% | Loss: 0.0689\n",
      "Epoch [936/2000] | Train Acc: 97.92% | Test Acc: 94.41% | Loss: 0.0691\n",
      "Epoch [937/2000] | Train Acc: 97.78% | Test Acc: 94.65% | Loss: 0.0688\n",
      "Epoch [938/2000] | Train Acc: 97.89% | Test Acc: 94.47% | Loss: 0.0684\n",
      "Epoch [939/2000] | Train Acc: 97.81% | Test Acc: 94.65% | Loss: 0.0677\n",
      "Epoch [940/2000] | Train Acc: 97.88% | Test Acc: 94.65% | Loss: 0.0673\n",
      "Epoch [941/2000] | Train Acc: 97.86% | Test Acc: 94.59% | Loss: 0.0671\n",
      "Epoch [942/2000] | Train Acc: 97.86% | Test Acc: 94.62% | Loss: 0.0672\n",
      "Epoch [943/2000] | Train Acc: 97.89% | Test Acc: 94.51% | Loss: 0.0674\n",
      "Epoch [944/2000] | Train Acc: 97.82% | Test Acc: 94.62% | Loss: 0.0676\n",
      "Epoch [945/2000] | Train Acc: 97.91% | Test Acc: 94.51% | Loss: 0.0676\n",
      "Epoch [946/2000] | Train Acc: 97.83% | Test Acc: 94.60% | Loss: 0.0674\n",
      "Epoch [947/2000] | Train Acc: 97.87% | Test Acc: 94.57% | Loss: 0.0671\n",
      "Epoch [948/2000] | Train Acc: 97.87% | Test Acc: 94.63% | Loss: 0.0668\n",
      "Epoch [949/2000] | Train Acc: 97.88% | Test Acc: 94.66% | Loss: 0.0666\n",
      "Epoch [950/2000] | Train Acc: 97.87% | Test Acc: 94.59% | Loss: 0.0666\n",
      "Epoch [951/2000] | Train Acc: 97.87% | Test Acc: 94.57% | Loss: 0.0667\n",
      "Epoch [952/2000] | Train Acc: 97.88% | Test Acc: 94.60% | Loss: 0.0668\n",
      "Epoch [953/2000] | Train Acc: 97.86% | Test Acc: 94.62% | Loss: 0.0668\n",
      "Epoch [954/2000] | Train Acc: 97.89% | Test Acc: 94.57% | Loss: 0.0667\n",
      "Epoch [955/2000] | Train Acc: 97.87% | Test Acc: 94.63% | Loss: 0.0666\n",
      "Epoch [956/2000] | Train Acc: 97.87% | Test Acc: 94.65% | Loss: 0.0664\n",
      "Epoch [957/2000] | Train Acc: 97.89% | Test Acc: 94.65% | Loss: 0.0663\n",
      "Epoch [958/2000] | Train Acc: 97.87% | Test Acc: 94.66% | Loss: 0.0662\n",
      "Epoch [959/2000] | Train Acc: 97.89% | Test Acc: 94.68% | Loss: 0.0661\n",
      "Epoch [960/2000] | Train Acc: 97.90% | Test Acc: 94.63% | Loss: 0.0661\n",
      "Epoch [961/2000] | Train Acc: 97.89% | Test Acc: 94.56% | Loss: 0.0661\n",
      "Epoch [962/2000] | Train Acc: 97.88% | Test Acc: 94.62% | Loss: 0.0661\n",
      "Epoch [963/2000] | Train Acc: 97.88% | Test Acc: 94.56% | Loss: 0.0661\n",
      "Epoch [964/2000] | Train Acc: 97.88% | Test Acc: 94.63% | Loss: 0.0660\n",
      "Epoch [965/2000] | Train Acc: 97.88% | Test Acc: 94.66% | Loss: 0.0659\n",
      "Epoch [966/2000] | Train Acc: 97.90% | Test Acc: 94.66% | Loss: 0.0658\n",
      "Epoch [967/2000] | Train Acc: 97.89% | Test Acc: 94.63% | Loss: 0.0658\n",
      "Epoch [968/2000] | Train Acc: 97.90% | Test Acc: 94.63% | Loss: 0.0657\n",
      "Epoch [969/2000] | Train Acc: 97.88% | Test Acc: 94.63% | Loss: 0.0656\n",
      "Epoch [970/2000] | Train Acc: 97.89% | Test Acc: 94.63% | Loss: 0.0656\n",
      "Epoch [971/2000] | Train Acc: 97.90% | Test Acc: 94.66% | Loss: 0.0655\n",
      "Epoch [972/2000] | Train Acc: 97.90% | Test Acc: 94.63% | Loss: 0.0655\n",
      "Epoch [973/2000] | Train Acc: 97.91% | Test Acc: 94.68% | Loss: 0.0655\n",
      "Epoch [974/2000] | Train Acc: 97.89% | Test Acc: 94.65% | Loss: 0.0654\n",
      "Epoch [975/2000] | Train Acc: 97.91% | Test Acc: 94.65% | Loss: 0.0654\n",
      "Epoch [976/2000] | Train Acc: 97.89% | Test Acc: 94.63% | Loss: 0.0654\n",
      "Epoch [977/2000] | Train Acc: 97.90% | Test Acc: 94.62% | Loss: 0.0653\n",
      "Epoch [978/2000] | Train Acc: 97.90% | Test Acc: 94.65% | Loss: 0.0653\n",
      "Epoch [979/2000] | Train Acc: 97.89% | Test Acc: 94.63% | Loss: 0.0653\n",
      "Epoch [980/2000] | Train Acc: 97.90% | Test Acc: 94.63% | Loss: 0.0652\n",
      "Epoch [981/2000] | Train Acc: 97.89% | Test Acc: 94.60% | Loss: 0.0652\n",
      "Epoch [982/2000] | Train Acc: 97.90% | Test Acc: 94.63% | Loss: 0.0652\n",
      "Epoch [983/2000] | Train Acc: 97.88% | Test Acc: 94.57% | Loss: 0.0651\n",
      "Epoch [984/2000] | Train Acc: 97.90% | Test Acc: 94.62% | Loss: 0.0651\n",
      "Epoch [985/2000] | Train Acc: 97.89% | Test Acc: 94.60% | Loss: 0.0651\n",
      "Epoch [986/2000] | Train Acc: 97.90% | Test Acc: 94.63% | Loss: 0.0651\n",
      "Epoch [987/2000] | Train Acc: 97.89% | Test Acc: 94.60% | Loss: 0.0651\n",
      "Epoch [988/2000] | Train Acc: 97.92% | Test Acc: 94.59% | Loss: 0.0652\n",
      "Epoch [989/2000] | Train Acc: 97.88% | Test Acc: 94.63% | Loss: 0.0652\n",
      "Epoch [990/2000] | Train Acc: 97.92% | Test Acc: 94.56% | Loss: 0.0652\n",
      "Epoch [991/2000] | Train Acc: 97.87% | Test Acc: 94.60% | Loss: 0.0652\n",
      "Epoch [992/2000] | Train Acc: 97.92% | Test Acc: 94.54% | Loss: 0.0652\n",
      "Epoch [993/2000] | Train Acc: 97.86% | Test Acc: 94.63% | Loss: 0.0651\n",
      "Epoch [994/2000] | Train Acc: 97.92% | Test Acc: 94.59% | Loss: 0.0650\n",
      "Epoch [995/2000] | Train Acc: 97.89% | Test Acc: 94.63% | Loss: 0.0649\n",
      "Epoch [996/2000] | Train Acc: 97.91% | Test Acc: 94.63% | Loss: 0.0647\n",
      "Epoch [997/2000] | Train Acc: 97.89% | Test Acc: 94.63% | Loss: 0.0645\n",
      "Epoch [998/2000] | Train Acc: 97.90% | Test Acc: 94.68% | Loss: 0.0644\n",
      "Epoch [999/2000] | Train Acc: 97.92% | Test Acc: 94.71% | Loss: 0.0642\n",
      "Epoch [1000/2000] | Train Acc: 97.91% | Test Acc: 94.69% | Loss: 0.0641\n",
      "Epoch [1001/2000] | Train Acc: 97.91% | Test Acc: 94.68% | Loss: 0.0641\n",
      "Epoch [1002/2000] | Train Acc: 97.93% | Test Acc: 94.65% | Loss: 0.0641\n",
      "Epoch [1003/2000] | Train Acc: 97.91% | Test Acc: 94.69% | Loss: 0.0640\n",
      "Epoch [1004/2000] | Train Acc: 97.92% | Test Acc: 94.63% | Loss: 0.0640\n",
      "Epoch [1005/2000] | Train Acc: 97.91% | Test Acc: 94.66% | Loss: 0.0641\n",
      "Epoch [1006/2000] | Train Acc: 97.90% | Test Acc: 94.60% | Loss: 0.0641\n",
      "Epoch [1007/2000] | Train Acc: 97.92% | Test Acc: 94.63% | Loss: 0.0641\n",
      "Epoch [1008/2000] | Train Acc: 97.91% | Test Acc: 94.65% | Loss: 0.0641\n",
      "Epoch [1009/2000] | Train Acc: 97.92% | Test Acc: 94.68% | Loss: 0.0641\n",
      "Epoch [1010/2000] | Train Acc: 97.91% | Test Acc: 94.63% | Loss: 0.0641\n",
      "Epoch [1011/2000] | Train Acc: 97.93% | Test Acc: 94.65% | Loss: 0.0642\n",
      "Epoch [1012/2000] | Train Acc: 97.90% | Test Acc: 94.63% | Loss: 0.0641\n",
      "Epoch [1013/2000] | Train Acc: 97.93% | Test Acc: 94.65% | Loss: 0.0641\n",
      "Epoch [1014/2000] | Train Acc: 97.90% | Test Acc: 94.65% | Loss: 0.0641\n",
      "Epoch [1015/2000] | Train Acc: 97.93% | Test Acc: 94.66% | Loss: 0.0640\n",
      "Epoch [1016/2000] | Train Acc: 97.91% | Test Acc: 94.62% | Loss: 0.0639\n",
      "Epoch [1017/2000] | Train Acc: 97.92% | Test Acc: 94.66% | Loss: 0.0637\n",
      "Epoch [1018/2000] | Train Acc: 97.91% | Test Acc: 94.63% | Loss: 0.0636\n",
      "Epoch [1019/2000] | Train Acc: 97.92% | Test Acc: 94.72% | Loss: 0.0634\n",
      "Epoch [1020/2000] | Train Acc: 97.92% | Test Acc: 94.69% | Loss: 0.0633\n",
      "Epoch [1021/2000] | Train Acc: 97.92% | Test Acc: 94.69% | Loss: 0.0632\n",
      "Epoch [1022/2000] | Train Acc: 97.94% | Test Acc: 94.68% | Loss: 0.0632\n",
      "Epoch [1023/2000] | Train Acc: 97.94% | Test Acc: 94.71% | Loss: 0.0631\n",
      "Epoch [1024/2000] | Train Acc: 97.91% | Test Acc: 94.69% | Loss: 0.0631\n",
      "Epoch [1025/2000] | Train Acc: 97.94% | Test Acc: 94.72% | Loss: 0.0631\n",
      "Epoch [1026/2000] | Train Acc: 97.92% | Test Acc: 94.66% | Loss: 0.0631\n",
      "Epoch [1027/2000] | Train Acc: 97.93% | Test Acc: 94.68% | Loss: 0.0631\n",
      "Epoch [1028/2000] | Train Acc: 97.92% | Test Acc: 94.66% | Loss: 0.0631\n",
      "Epoch [1029/2000] | Train Acc: 97.91% | Test Acc: 94.65% | Loss: 0.0631\n",
      "Epoch [1030/2000] | Train Acc: 97.94% | Test Acc: 94.71% | Loss: 0.0631\n",
      "Epoch [1031/2000] | Train Acc: 97.91% | Test Acc: 94.69% | Loss: 0.0631\n",
      "Epoch [1032/2000] | Train Acc: 97.94% | Test Acc: 94.66% | Loss: 0.0632\n",
      "Epoch [1033/2000] | Train Acc: 97.93% | Test Acc: 94.68% | Loss: 0.0633\n",
      "Epoch [1034/2000] | Train Acc: 97.96% | Test Acc: 94.59% | Loss: 0.0634\n",
      "Epoch [1035/2000] | Train Acc: 97.88% | Test Acc: 94.66% | Loss: 0.0634\n",
      "Epoch [1036/2000] | Train Acc: 97.96% | Test Acc: 94.51% | Loss: 0.0635\n",
      "Epoch [1037/2000] | Train Acc: 97.87% | Test Acc: 94.68% | Loss: 0.0634\n",
      "Epoch [1038/2000] | Train Acc: 97.95% | Test Acc: 94.62% | Loss: 0.0634\n",
      "Epoch [1039/2000] | Train Acc: 97.90% | Test Acc: 94.66% | Loss: 0.0631\n",
      "Epoch [1040/2000] | Train Acc: 97.96% | Test Acc: 94.68% | Loss: 0.0629\n",
      "Epoch [1041/2000] | Train Acc: 97.92% | Test Acc: 94.71% | Loss: 0.0626\n",
      "Epoch [1042/2000] | Train Acc: 97.93% | Test Acc: 94.68% | Loss: 0.0624\n",
      "Epoch [1043/2000] | Train Acc: 97.95% | Test Acc: 94.68% | Loss: 0.0623\n",
      "Epoch [1044/2000] | Train Acc: 97.95% | Test Acc: 94.72% | Loss: 0.0623\n",
      "Epoch [1045/2000] | Train Acc: 97.94% | Test Acc: 94.66% | Loss: 0.0623\n",
      "Epoch [1046/2000] | Train Acc: 97.93% | Test Acc: 94.71% | Loss: 0.0623\n",
      "Epoch [1047/2000] | Train Acc: 97.97% | Test Acc: 94.68% | Loss: 0.0624\n",
      "Epoch [1048/2000] | Train Acc: 97.91% | Test Acc: 94.72% | Loss: 0.0624\n",
      "Epoch [1049/2000] | Train Acc: 97.95% | Test Acc: 94.69% | Loss: 0.0625\n",
      "Epoch [1050/2000] | Train Acc: 97.92% | Test Acc: 94.72% | Loss: 0.0624\n",
      "Epoch [1051/2000] | Train Acc: 97.95% | Test Acc: 94.68% | Loss: 0.0624\n",
      "Epoch [1052/2000] | Train Acc: 97.92% | Test Acc: 94.71% | Loss: 0.0623\n",
      "Epoch [1053/2000] | Train Acc: 97.96% | Test Acc: 94.68% | Loss: 0.0622\n",
      "Epoch [1054/2000] | Train Acc: 97.92% | Test Acc: 94.71% | Loss: 0.0621\n",
      "Epoch [1055/2000] | Train Acc: 97.95% | Test Acc: 94.72% | Loss: 0.0619\n",
      "Epoch [1056/2000] | Train Acc: 97.96% | Test Acc: 94.72% | Loss: 0.0618\n",
      "Epoch [1057/2000] | Train Acc: 97.95% | Test Acc: 94.71% | Loss: 0.0618\n",
      "Epoch [1058/2000] | Train Acc: 97.96% | Test Acc: 94.71% | Loss: 0.0617\n",
      "Epoch [1059/2000] | Train Acc: 97.96% | Test Acc: 94.71% | Loss: 0.0616\n",
      "Epoch [1060/2000] | Train Acc: 97.95% | Test Acc: 94.72% | Loss: 0.0616\n",
      "Epoch [1061/2000] | Train Acc: 97.96% | Test Acc: 94.74% | Loss: 0.0616\n",
      "Epoch [1062/2000] | Train Acc: 97.94% | Test Acc: 94.72% | Loss: 0.0616\n",
      "Epoch [1063/2000] | Train Acc: 97.96% | Test Acc: 94.72% | Loss: 0.0616\n",
      "Epoch [1064/2000] | Train Acc: 97.94% | Test Acc: 94.68% | Loss: 0.0616\n",
      "Epoch [1065/2000] | Train Acc: 97.94% | Test Acc: 94.72% | Loss: 0.0616\n",
      "Epoch [1066/2000] | Train Acc: 97.97% | Test Acc: 94.65% | Loss: 0.0616\n",
      "Epoch [1067/2000] | Train Acc: 97.94% | Test Acc: 94.71% | Loss: 0.0616\n",
      "Epoch [1068/2000] | Train Acc: 97.97% | Test Acc: 94.72% | Loss: 0.0616\n",
      "Epoch [1069/2000] | Train Acc: 97.93% | Test Acc: 94.77% | Loss: 0.0616\n",
      "Epoch [1070/2000] | Train Acc: 97.98% | Test Acc: 94.71% | Loss: 0.0617\n",
      "Epoch [1071/2000] | Train Acc: 97.95% | Test Acc: 94.74% | Loss: 0.0617\n",
      "Epoch [1072/2000] | Train Acc: 97.98% | Test Acc: 94.66% | Loss: 0.0619\n",
      "Epoch [1073/2000] | Train Acc: 97.95% | Test Acc: 94.74% | Loss: 0.0619\n",
      "Epoch [1074/2000] | Train Acc: 97.97% | Test Acc: 94.59% | Loss: 0.0620\n",
      "Epoch [1075/2000] | Train Acc: 97.92% | Test Acc: 94.75% | Loss: 0.0619\n",
      "Epoch [1076/2000] | Train Acc: 97.97% | Test Acc: 94.69% | Loss: 0.0618\n",
      "Epoch [1077/2000] | Train Acc: 97.97% | Test Acc: 94.72% | Loss: 0.0616\n",
      "Epoch [1078/2000] | Train Acc: 97.98% | Test Acc: 94.66% | Loss: 0.0614\n",
      "Epoch [1079/2000] | Train Acc: 97.95% | Test Acc: 94.69% | Loss: 0.0612\n",
      "Epoch [1080/2000] | Train Acc: 97.95% | Test Acc: 94.74% | Loss: 0.0610\n",
      "Epoch [1081/2000] | Train Acc: 97.98% | Test Acc: 94.72% | Loss: 0.0608\n",
      "Epoch [1082/2000] | Train Acc: 97.98% | Test Acc: 94.72% | Loss: 0.0608\n",
      "Epoch [1083/2000] | Train Acc: 97.97% | Test Acc: 94.74% | Loss: 0.0608\n",
      "Epoch [1084/2000] | Train Acc: 97.97% | Test Acc: 94.77% | Loss: 0.0608\n",
      "Epoch [1085/2000] | Train Acc: 97.97% | Test Acc: 94.68% | Loss: 0.0608\n",
      "Epoch [1086/2000] | Train Acc: 97.95% | Test Acc: 94.75% | Loss: 0.0609\n",
      "Epoch [1087/2000] | Train Acc: 97.99% | Test Acc: 94.69% | Loss: 0.0609\n",
      "Epoch [1088/2000] | Train Acc: 97.95% | Test Acc: 94.74% | Loss: 0.0609\n",
      "Epoch [1089/2000] | Train Acc: 97.98% | Test Acc: 94.71% | Loss: 0.0610\n",
      "Epoch [1090/2000] | Train Acc: 97.96% | Test Acc: 94.75% | Loss: 0.0609\n",
      "Epoch [1091/2000] | Train Acc: 97.99% | Test Acc: 94.71% | Loss: 0.0609\n",
      "Epoch [1092/2000] | Train Acc: 97.95% | Test Acc: 94.75% | Loss: 0.0608\n",
      "Epoch [1093/2000] | Train Acc: 97.99% | Test Acc: 94.69% | Loss: 0.0607\n",
      "Epoch [1094/2000] | Train Acc: 97.95% | Test Acc: 94.75% | Loss: 0.0606\n",
      "Epoch [1095/2000] | Train Acc: 97.98% | Test Acc: 94.75% | Loss: 0.0605\n",
      "Epoch [1096/2000] | Train Acc: 97.98% | Test Acc: 94.72% | Loss: 0.0604\n",
      "Epoch [1097/2000] | Train Acc: 97.98% | Test Acc: 94.74% | Loss: 0.0603\n",
      "Epoch [1098/2000] | Train Acc: 97.99% | Test Acc: 94.74% | Loss: 0.0602\n",
      "Epoch [1099/2000] | Train Acc: 97.98% | Test Acc: 94.72% | Loss: 0.0602\n",
      "Epoch [1100/2000] | Train Acc: 97.99% | Test Acc: 94.72% | Loss: 0.0601\n",
      "Epoch [1101/2000] | Train Acc: 97.99% | Test Acc: 94.72% | Loss: 0.0601\n",
      "Epoch [1102/2000] | Train Acc: 97.98% | Test Acc: 94.75% | Loss: 0.0601\n",
      "Epoch [1103/2000] | Train Acc: 97.98% | Test Acc: 94.72% | Loss: 0.0601\n",
      "Epoch [1104/2000] | Train Acc: 97.98% | Test Acc: 94.75% | Loss: 0.0600\n",
      "Epoch [1105/2000] | Train Acc: 98.00% | Test Acc: 94.78% | Loss: 0.0600\n",
      "Epoch [1106/2000] | Train Acc: 97.98% | Test Acc: 94.71% | Loss: 0.0600\n",
      "Epoch [1107/2000] | Train Acc: 97.98% | Test Acc: 94.74% | Loss: 0.0600\n",
      "Epoch [1108/2000] | Train Acc: 97.99% | Test Acc: 94.71% | Loss: 0.0601\n",
      "Epoch [1109/2000] | Train Acc: 97.97% | Test Acc: 94.72% | Loss: 0.0601\n",
      "Epoch [1110/2000] | Train Acc: 98.00% | Test Acc: 94.72% | Loss: 0.0602\n",
      "Epoch [1111/2000] | Train Acc: 97.98% | Test Acc: 94.75% | Loss: 0.0603\n",
      "Epoch [1112/2000] | Train Acc: 97.99% | Test Acc: 94.62% | Loss: 0.0605\n",
      "Epoch [1113/2000] | Train Acc: 97.97% | Test Acc: 94.79% | Loss: 0.0606\n",
      "Epoch [1114/2000] | Train Acc: 98.01% | Test Acc: 94.48% | Loss: 0.0609\n",
      "Epoch [1115/2000] | Train Acc: 97.89% | Test Acc: 94.79% | Loss: 0.0609\n",
      "Epoch [1116/2000] | Train Acc: 98.00% | Test Acc: 94.53% | Loss: 0.0610\n",
      "Epoch [1117/2000] | Train Acc: 97.91% | Test Acc: 94.75% | Loss: 0.0607\n",
      "Epoch [1118/2000] | Train Acc: 97.99% | Test Acc: 94.72% | Loss: 0.0604\n",
      "Epoch [1119/2000] | Train Acc: 97.97% | Test Acc: 94.72% | Loss: 0.0599\n",
      "Epoch [1120/2000] | Train Acc: 97.98% | Test Acc: 94.75% | Loss: 0.0596\n",
      "Epoch [1121/2000] | Train Acc: 98.00% | Test Acc: 94.82% | Loss: 0.0594\n",
      "Epoch [1122/2000] | Train Acc: 98.01% | Test Acc: 94.77% | Loss: 0.0594\n",
      "Epoch [1123/2000] | Train Acc: 97.98% | Test Acc: 94.69% | Loss: 0.0595\n",
      "Epoch [1124/2000] | Train Acc: 97.98% | Test Acc: 94.72% | Loss: 0.0597\n",
      "Epoch [1125/2000] | Train Acc: 98.00% | Test Acc: 94.68% | Loss: 0.0598\n",
      "Epoch [1126/2000] | Train Acc: 97.97% | Test Acc: 94.72% | Loss: 0.0598\n",
      "Epoch [1127/2000] | Train Acc: 98.00% | Test Acc: 94.68% | Loss: 0.0598\n",
      "Epoch [1128/2000] | Train Acc: 97.97% | Test Acc: 94.72% | Loss: 0.0596\n",
      "Epoch [1129/2000] | Train Acc: 98.00% | Test Acc: 94.75% | Loss: 0.0594\n",
      "Epoch [1130/2000] | Train Acc: 98.01% | Test Acc: 94.72% | Loss: 0.0592\n",
      "Epoch [1131/2000] | Train Acc: 98.00% | Test Acc: 94.69% | Loss: 0.0591\n",
      "Epoch [1132/2000] | Train Acc: 98.00% | Test Acc: 94.77% | Loss: 0.0590\n",
      "Epoch [1133/2000] | Train Acc: 98.01% | Test Acc: 94.72% | Loss: 0.0591\n",
      "Epoch [1134/2000] | Train Acc: 98.00% | Test Acc: 94.71% | Loss: 0.0591\n",
      "Epoch [1135/2000] | Train Acc: 98.01% | Test Acc: 94.74% | Loss: 0.0591\n",
      "Epoch [1136/2000] | Train Acc: 98.00% | Test Acc: 94.72% | Loss: 0.0592\n",
      "Epoch [1137/2000] | Train Acc: 97.98% | Test Acc: 94.71% | Loss: 0.0592\n",
      "Epoch [1138/2000] | Train Acc: 98.01% | Test Acc: 94.74% | Loss: 0.0591\n",
      "Epoch [1139/2000] | Train Acc: 97.98% | Test Acc: 94.71% | Loss: 0.0591\n",
      "Epoch [1140/2000] | Train Acc: 98.00% | Test Acc: 94.75% | Loss: 0.0590\n",
      "Epoch [1141/2000] | Train Acc: 98.01% | Test Acc: 94.74% | Loss: 0.0589\n",
      "Epoch [1142/2000] | Train Acc: 98.01% | Test Acc: 94.78% | Loss: 0.0588\n",
      "Epoch [1143/2000] | Train Acc: 98.02% | Test Acc: 94.72% | Loss: 0.0587\n",
      "Epoch [1144/2000] | Train Acc: 98.00% | Test Acc: 94.71% | Loss: 0.0587\n",
      "Epoch [1145/2000] | Train Acc: 98.02% | Test Acc: 94.72% | Loss: 0.0586\n",
      "Epoch [1146/2000] | Train Acc: 98.02% | Test Acc: 94.72% | Loss: 0.0586\n",
      "Epoch [1147/2000] | Train Acc: 98.01% | Test Acc: 94.77% | Loss: 0.0586\n",
      "Epoch [1148/2000] | Train Acc: 98.03% | Test Acc: 94.69% | Loss: 0.0586\n",
      "Epoch [1149/2000] | Train Acc: 98.01% | Test Acc: 94.75% | Loss: 0.0586\n",
      "Epoch [1150/2000] | Train Acc: 98.03% | Test Acc: 94.68% | Loss: 0.0586\n",
      "Epoch [1151/2000] | Train Acc: 98.02% | Test Acc: 94.69% | Loss: 0.0586\n",
      "Epoch [1152/2000] | Train Acc: 98.01% | Test Acc: 94.68% | Loss: 0.0586\n",
      "Epoch [1153/2000] | Train Acc: 98.01% | Test Acc: 94.68% | Loss: 0.0586\n",
      "Epoch [1154/2000] | Train Acc: 98.01% | Test Acc: 94.65% | Loss: 0.0586\n",
      "Epoch [1155/2000] | Train Acc: 98.01% | Test Acc: 94.68% | Loss: 0.0586\n",
      "Epoch [1156/2000] | Train Acc: 97.98% | Test Acc: 94.65% | Loss: 0.0586\n",
      "Epoch [1157/2000] | Train Acc: 98.01% | Test Acc: 94.65% | Loss: 0.0586\n",
      "Epoch [1158/2000] | Train Acc: 97.99% | Test Acc: 94.65% | Loss: 0.0586\n",
      "Epoch [1159/2000] | Train Acc: 98.01% | Test Acc: 94.62% | Loss: 0.0587\n",
      "Epoch [1160/2000] | Train Acc: 97.99% | Test Acc: 94.65% | Loss: 0.0587\n",
      "Epoch [1161/2000] | Train Acc: 98.02% | Test Acc: 94.62% | Loss: 0.0587\n",
      "Epoch [1162/2000] | Train Acc: 97.99% | Test Acc: 94.65% | Loss: 0.0587\n",
      "Epoch [1163/2000] | Train Acc: 98.02% | Test Acc: 94.60% | Loss: 0.0587\n",
      "Epoch [1164/2000] | Train Acc: 97.99% | Test Acc: 94.65% | Loss: 0.0586\n",
      "Epoch [1165/2000] | Train Acc: 98.02% | Test Acc: 94.66% | Loss: 0.0585\n",
      "Epoch [1166/2000] | Train Acc: 97.99% | Test Acc: 94.68% | Loss: 0.0584\n",
      "Epoch [1167/2000] | Train Acc: 98.02% | Test Acc: 94.66% | Loss: 0.0582\n",
      "Epoch [1168/2000] | Train Acc: 98.01% | Test Acc: 94.68% | Loss: 0.0581\n",
      "Epoch [1169/2000] | Train Acc: 98.02% | Test Acc: 94.75% | Loss: 0.0580\n",
      "Epoch [1170/2000] | Train Acc: 98.04% | Test Acc: 94.71% | Loss: 0.0579\n",
      "Epoch [1171/2000] | Train Acc: 98.01% | Test Acc: 94.71% | Loss: 0.0578\n",
      "Epoch [1172/2000] | Train Acc: 98.02% | Test Acc: 94.71% | Loss: 0.0578\n",
      "Epoch [1173/2000] | Train Acc: 98.03% | Test Acc: 94.69% | Loss: 0.0578\n",
      "Epoch [1174/2000] | Train Acc: 98.02% | Test Acc: 94.75% | Loss: 0.0577\n",
      "Epoch [1175/2000] | Train Acc: 98.03% | Test Acc: 94.68% | Loss: 0.0577\n",
      "Epoch [1176/2000] | Train Acc: 98.04% | Test Acc: 94.75% | Loss: 0.0577\n",
      "Epoch [1177/2000] | Train Acc: 98.03% | Test Acc: 94.66% | Loss: 0.0578\n",
      "Epoch [1178/2000] | Train Acc: 98.03% | Test Acc: 94.69% | Loss: 0.0578\n",
      "Epoch [1179/2000] | Train Acc: 98.03% | Test Acc: 94.66% | Loss: 0.0578\n",
      "Epoch [1180/2000] | Train Acc: 98.03% | Test Acc: 94.66% | Loss: 0.0579\n",
      "Epoch [1181/2000] | Train Acc: 98.01% | Test Acc: 94.65% | Loss: 0.0579\n",
      "Epoch [1182/2000] | Train Acc: 98.03% | Test Acc: 94.59% | Loss: 0.0580\n",
      "Epoch [1183/2000] | Train Acc: 97.99% | Test Acc: 94.66% | Loss: 0.0581\n",
      "Epoch [1184/2000] | Train Acc: 98.03% | Test Acc: 94.51% | Loss: 0.0582\n",
      "Epoch [1185/2000] | Train Acc: 98.01% | Test Acc: 94.72% | Loss: 0.0583\n",
      "Epoch [1186/2000] | Train Acc: 98.03% | Test Acc: 94.51% | Loss: 0.0584\n",
      "Epoch [1187/2000] | Train Acc: 98.00% | Test Acc: 94.69% | Loss: 0.0583\n",
      "Epoch [1188/2000] | Train Acc: 98.03% | Test Acc: 94.56% | Loss: 0.0582\n",
      "Epoch [1189/2000] | Train Acc: 98.01% | Test Acc: 94.65% | Loss: 0.0580\n",
      "Epoch [1190/2000] | Train Acc: 98.04% | Test Acc: 94.63% | Loss: 0.0577\n",
      "Epoch [1191/2000] | Train Acc: 98.02% | Test Acc: 94.66% | Loss: 0.0575\n",
      "Epoch [1192/2000] | Train Acc: 98.04% | Test Acc: 94.72% | Loss: 0.0573\n",
      "Epoch [1193/2000] | Train Acc: 98.03% | Test Acc: 94.72% | Loss: 0.0572\n",
      "Epoch [1194/2000] | Train Acc: 98.04% | Test Acc: 94.68% | Loss: 0.0572\n",
      "Epoch [1195/2000] | Train Acc: 98.04% | Test Acc: 94.66% | Loss: 0.0572\n",
      "Epoch [1196/2000] | Train Acc: 98.03% | Test Acc: 94.66% | Loss: 0.0573\n",
      "Epoch [1197/2000] | Train Acc: 98.05% | Test Acc: 94.65% | Loss: 0.0573\n",
      "Epoch [1198/2000] | Train Acc: 98.02% | Test Acc: 94.66% | Loss: 0.0574\n",
      "Epoch [1199/2000] | Train Acc: 98.05% | Test Acc: 94.60% | Loss: 0.0575\n",
      "Epoch [1200/2000] | Train Acc: 98.01% | Test Acc: 94.66% | Loss: 0.0574\n",
      "Epoch [1201/2000] | Train Acc: 98.04% | Test Acc: 94.63% | Loss: 0.0574\n",
      "Epoch [1202/2000] | Train Acc: 98.02% | Test Acc: 94.65% | Loss: 0.0573\n",
      "Epoch [1203/2000] | Train Acc: 98.05% | Test Acc: 94.66% | Loss: 0.0572\n",
      "Epoch [1204/2000] | Train Acc: 98.03% | Test Acc: 94.69% | Loss: 0.0571\n",
      "Epoch [1205/2000] | Train Acc: 98.04% | Test Acc: 94.77% | Loss: 0.0570\n",
      "Epoch [1206/2000] | Train Acc: 98.05% | Test Acc: 94.75% | Loss: 0.0569\n",
      "Epoch [1207/2000] | Train Acc: 98.05% | Test Acc: 94.74% | Loss: 0.0568\n",
      "Epoch [1208/2000] | Train Acc: 98.04% | Test Acc: 94.74% | Loss: 0.0568\n",
      "Epoch [1209/2000] | Train Acc: 98.04% | Test Acc: 94.75% | Loss: 0.0567\n",
      "Epoch [1210/2000] | Train Acc: 98.04% | Test Acc: 94.75% | Loss: 0.0567\n",
      "Epoch [1211/2000] | Train Acc: 98.04% | Test Acc: 94.72% | Loss: 0.0567\n",
      "Epoch [1212/2000] | Train Acc: 98.05% | Test Acc: 94.72% | Loss: 0.0567\n",
      "Epoch [1213/2000] | Train Acc: 98.05% | Test Acc: 94.65% | Loss: 0.0567\n",
      "Epoch [1214/2000] | Train Acc: 98.05% | Test Acc: 94.69% | Loss: 0.0567\n",
      "Epoch [1215/2000] | Train Acc: 98.05% | Test Acc: 94.68% | Loss: 0.0567\n",
      "Epoch [1216/2000] | Train Acc: 98.05% | Test Acc: 94.68% | Loss: 0.0567\n",
      "Epoch [1217/2000] | Train Acc: 98.04% | Test Acc: 94.66% | Loss: 0.0568\n",
      "Epoch [1218/2000] | Train Acc: 98.06% | Test Acc: 94.66% | Loss: 0.0568\n",
      "Epoch [1219/2000] | Train Acc: 98.03% | Test Acc: 94.69% | Loss: 0.0568\n",
      "Epoch [1220/2000] | Train Acc: 98.07% | Test Acc: 94.62% | Loss: 0.0569\n",
      "Epoch [1221/2000] | Train Acc: 98.03% | Test Acc: 94.68% | Loss: 0.0569\n",
      "Epoch [1222/2000] | Train Acc: 98.07% | Test Acc: 94.60% | Loss: 0.0569\n",
      "Epoch [1223/2000] | Train Acc: 98.02% | Test Acc: 94.68% | Loss: 0.0569\n",
      "Epoch [1224/2000] | Train Acc: 98.06% | Test Acc: 94.57% | Loss: 0.0570\n",
      "Epoch [1225/2000] | Train Acc: 98.01% | Test Acc: 94.69% | Loss: 0.0569\n",
      "Epoch [1226/2000] | Train Acc: 98.07% | Test Acc: 94.59% | Loss: 0.0569\n",
      "Epoch [1227/2000] | Train Acc: 98.02% | Test Acc: 94.69% | Loss: 0.0568\n",
      "Epoch [1228/2000] | Train Acc: 98.07% | Test Acc: 94.66% | Loss: 0.0567\n",
      "Epoch [1229/2000] | Train Acc: 98.03% | Test Acc: 94.69% | Loss: 0.0565\n",
      "Epoch [1230/2000] | Train Acc: 98.06% | Test Acc: 94.72% | Loss: 0.0564\n",
      "Epoch [1231/2000] | Train Acc: 98.06% | Test Acc: 94.74% | Loss: 0.0562\n",
      "Epoch [1232/2000] | Train Acc: 98.05% | Test Acc: 94.71% | Loss: 0.0561\n",
      "Epoch [1233/2000] | Train Acc: 98.05% | Test Acc: 94.69% | Loss: 0.0561\n",
      "Epoch [1234/2000] | Train Acc: 98.04% | Test Acc: 94.72% | Loss: 0.0560\n",
      "Epoch [1235/2000] | Train Acc: 98.05% | Test Acc: 94.74% | Loss: 0.0560\n",
      "Epoch [1236/2000] | Train Acc: 98.06% | Test Acc: 94.72% | Loss: 0.0560\n",
      "Epoch [1237/2000] | Train Acc: 98.06% | Test Acc: 94.74% | Loss: 0.0560\n",
      "Epoch [1238/2000] | Train Acc: 98.06% | Test Acc: 94.72% | Loss: 0.0561\n",
      "Epoch [1239/2000] | Train Acc: 98.07% | Test Acc: 94.71% | Loss: 0.0561\n",
      "Epoch [1240/2000] | Train Acc: 98.05% | Test Acc: 94.71% | Loss: 0.0561\n",
      "Epoch [1241/2000] | Train Acc: 98.06% | Test Acc: 94.68% | Loss: 0.0562\n",
      "Epoch [1242/2000] | Train Acc: 98.03% | Test Acc: 94.71% | Loss: 0.0562\n",
      "Epoch [1243/2000] | Train Acc: 98.07% | Test Acc: 94.59% | Loss: 0.0563\n",
      "Epoch [1244/2000] | Train Acc: 98.03% | Test Acc: 94.68% | Loss: 0.0563\n",
      "Epoch [1245/2000] | Train Acc: 98.07% | Test Acc: 94.56% | Loss: 0.0564\n",
      "Epoch [1246/2000] | Train Acc: 98.02% | Test Acc: 94.72% | Loss: 0.0564\n",
      "Epoch [1247/2000] | Train Acc: 98.07% | Test Acc: 94.54% | Loss: 0.0565\n",
      "Epoch [1248/2000] | Train Acc: 98.02% | Test Acc: 94.68% | Loss: 0.0564\n",
      "Epoch [1249/2000] | Train Acc: 98.08% | Test Acc: 94.59% | Loss: 0.0564\n",
      "Epoch [1250/2000] | Train Acc: 98.03% | Test Acc: 94.71% | Loss: 0.0562\n",
      "Epoch [1251/2000] | Train Acc: 98.07% | Test Acc: 94.69% | Loss: 0.0560\n",
      "Epoch [1252/2000] | Train Acc: 98.04% | Test Acc: 94.71% | Loss: 0.0558\n",
      "Epoch [1253/2000] | Train Acc: 98.07% | Test Acc: 94.75% | Loss: 0.0557\n",
      "Epoch [1254/2000] | Train Acc: 98.07% | Test Acc: 94.74% | Loss: 0.0556\n",
      "Epoch [1255/2000] | Train Acc: 98.05% | Test Acc: 94.72% | Loss: 0.0555\n",
      "Epoch [1256/2000] | Train Acc: 98.06% | Test Acc: 94.74% | Loss: 0.0555\n",
      "Epoch [1257/2000] | Train Acc: 98.06% | Test Acc: 94.69% | Loss: 0.0555\n",
      "Epoch [1258/2000] | Train Acc: 98.07% | Test Acc: 94.72% | Loss: 0.0555\n",
      "Epoch [1259/2000] | Train Acc: 98.07% | Test Acc: 94.72% | Loss: 0.0556\n",
      "Epoch [1260/2000] | Train Acc: 98.08% | Test Acc: 94.74% | Loss: 0.0556\n",
      "Epoch [1261/2000] | Train Acc: 98.05% | Test Acc: 94.71% | Loss: 0.0557\n",
      "Epoch [1262/2000] | Train Acc: 98.07% | Test Acc: 94.69% | Loss: 0.0557\n",
      "Epoch [1263/2000] | Train Acc: 98.04% | Test Acc: 94.71% | Loss: 0.0557\n",
      "Epoch [1264/2000] | Train Acc: 98.09% | Test Acc: 94.63% | Loss: 0.0558\n",
      "Epoch [1265/2000] | Train Acc: 98.05% | Test Acc: 94.71% | Loss: 0.0557\n",
      "Epoch [1266/2000] | Train Acc: 98.08% | Test Acc: 94.65% | Loss: 0.0557\n",
      "Epoch [1267/2000] | Train Acc: 98.05% | Test Acc: 94.71% | Loss: 0.0557\n",
      "Epoch [1268/2000] | Train Acc: 98.08% | Test Acc: 94.72% | Loss: 0.0556\n",
      "Epoch [1269/2000] | Train Acc: 98.05% | Test Acc: 94.74% | Loss: 0.0555\n",
      "Epoch [1270/2000] | Train Acc: 98.08% | Test Acc: 94.75% | Loss: 0.0554\n",
      "Epoch [1271/2000] | Train Acc: 98.07% | Test Acc: 94.68% | Loss: 0.0553\n",
      "Epoch [1272/2000] | Train Acc: 98.08% | Test Acc: 94.75% | Loss: 0.0552\n",
      "Epoch [1273/2000] | Train Acc: 98.07% | Test Acc: 94.74% | Loss: 0.0551\n",
      "Epoch [1274/2000] | Train Acc: 98.07% | Test Acc: 94.71% | Loss: 0.0551\n",
      "Epoch [1275/2000] | Train Acc: 98.05% | Test Acc: 94.77% | Loss: 0.0550\n",
      "Epoch [1276/2000] | Train Acc: 98.06% | Test Acc: 94.75% | Loss: 0.0550\n",
      "Epoch [1277/2000] | Train Acc: 98.07% | Test Acc: 94.72% | Loss: 0.0550\n",
      "Epoch [1278/2000] | Train Acc: 98.06% | Test Acc: 94.74% | Loss: 0.0549\n",
      "Epoch [1279/2000] | Train Acc: 98.07% | Test Acc: 94.75% | Loss: 0.0549\n",
      "Epoch [1280/2000] | Train Acc: 98.07% | Test Acc: 94.69% | Loss: 0.0549\n",
      "Epoch [1281/2000] | Train Acc: 98.08% | Test Acc: 94.78% | Loss: 0.0549\n",
      "Epoch [1282/2000] | Train Acc: 98.08% | Test Acc: 94.72% | Loss: 0.0550\n",
      "Epoch [1283/2000] | Train Acc: 98.09% | Test Acc: 94.72% | Loss: 0.0550\n",
      "Epoch [1284/2000] | Train Acc: 98.07% | Test Acc: 94.74% | Loss: 0.0550\n",
      "Epoch [1285/2000] | Train Acc: 98.08% | Test Acc: 94.68% | Loss: 0.0551\n",
      "Epoch [1286/2000] | Train Acc: 98.05% | Test Acc: 94.69% | Loss: 0.0552\n",
      "Epoch [1287/2000] | Train Acc: 98.07% | Test Acc: 94.56% | Loss: 0.0553\n",
      "Epoch [1288/2000] | Train Acc: 98.04% | Test Acc: 94.71% | Loss: 0.0554\n",
      "Epoch [1289/2000] | Train Acc: 98.09% | Test Acc: 94.50% | Loss: 0.0557\n",
      "Epoch [1290/2000] | Train Acc: 98.03% | Test Acc: 94.68% | Loss: 0.0557\n",
      "Epoch [1291/2000] | Train Acc: 98.10% | Test Acc: 94.48% | Loss: 0.0559\n",
      "Epoch [1292/2000] | Train Acc: 98.02% | Test Acc: 94.71% | Loss: 0.0557\n",
      "Epoch [1293/2000] | Train Acc: 98.09% | Test Acc: 94.56% | Loss: 0.0557\n",
      "Epoch [1294/2000] | Train Acc: 98.05% | Test Acc: 94.71% | Loss: 0.0553\n",
      "Epoch [1295/2000] | Train Acc: 98.09% | Test Acc: 94.75% | Loss: 0.0550\n",
      "Epoch [1296/2000] | Train Acc: 98.08% | Test Acc: 94.71% | Loss: 0.0547\n",
      "Epoch [1297/2000] | Train Acc: 98.06% | Test Acc: 94.69% | Loss: 0.0545\n",
      "Epoch [1298/2000] | Train Acc: 98.07% | Test Acc: 94.75% | Loss: 0.0545\n",
      "Epoch [1299/2000] | Train Acc: 98.07% | Test Acc: 94.69% | Loss: 0.0546\n",
      "Epoch [1300/2000] | Train Acc: 98.08% | Test Acc: 94.68% | Loss: 0.0547\n",
      "Epoch [1301/2000] | Train Acc: 98.08% | Test Acc: 94.66% | Loss: 0.0548\n",
      "Epoch [1302/2000] | Train Acc: 98.09% | Test Acc: 94.63% | Loss: 0.0549\n",
      "Epoch [1303/2000] | Train Acc: 98.06% | Test Acc: 94.69% | Loss: 0.0549\n",
      "Epoch [1304/2000] | Train Acc: 98.08% | Test Acc: 94.68% | Loss: 0.0549\n",
      "Epoch [1305/2000] | Train Acc: 98.06% | Test Acc: 94.69% | Loss: 0.0547\n",
      "Epoch [1306/2000] | Train Acc: 98.08% | Test Acc: 94.77% | Loss: 0.0546\n",
      "Epoch [1307/2000] | Train Acc: 98.08% | Test Acc: 94.66% | Loss: 0.0544\n",
      "Epoch [1308/2000] | Train Acc: 98.09% | Test Acc: 94.71% | Loss: 0.0543\n",
      "Epoch [1309/2000] | Train Acc: 98.07% | Test Acc: 94.72% | Loss: 0.0542\n",
      "Epoch [1310/2000] | Train Acc: 98.07% | Test Acc: 94.71% | Loss: 0.0542\n",
      "Epoch [1311/2000] | Train Acc: 98.09% | Test Acc: 94.77% | Loss: 0.0542\n",
      "Epoch [1312/2000] | Train Acc: 98.08% | Test Acc: 94.68% | Loss: 0.0542\n",
      "Epoch [1313/2000] | Train Acc: 98.09% | Test Acc: 94.75% | Loss: 0.0542\n",
      "Epoch [1314/2000] | Train Acc: 98.08% | Test Acc: 94.68% | Loss: 0.0543\n",
      "Epoch [1315/2000] | Train Acc: 98.09% | Test Acc: 94.75% | Loss: 0.0543\n",
      "Epoch [1316/2000] | Train Acc: 98.08% | Test Acc: 94.71% | Loss: 0.0543\n",
      "Epoch [1317/2000] | Train Acc: 98.09% | Test Acc: 94.75% | Loss: 0.0543\n",
      "Epoch [1318/2000] | Train Acc: 98.08% | Test Acc: 94.71% | Loss: 0.0543\n",
      "Epoch [1319/2000] | Train Acc: 98.10% | Test Acc: 94.77% | Loss: 0.0542\n",
      "Epoch [1320/2000] | Train Acc: 98.09% | Test Acc: 94.68% | Loss: 0.0542\n",
      "Epoch [1321/2000] | Train Acc: 98.09% | Test Acc: 94.78% | Loss: 0.0541\n",
      "Epoch [1322/2000] | Train Acc: 98.08% | Test Acc: 94.69% | Loss: 0.0540\n",
      "Epoch [1323/2000] | Train Acc: 98.09% | Test Acc: 94.77% | Loss: 0.0540\n",
      "Epoch [1324/2000] | Train Acc: 98.08% | Test Acc: 94.72% | Loss: 0.0539\n",
      "Epoch [1325/2000] | Train Acc: 98.10% | Test Acc: 94.75% | Loss: 0.0539\n",
      "Epoch [1326/2000] | Train Acc: 98.07% | Test Acc: 94.68% | Loss: 0.0538\n",
      "Epoch [1327/2000] | Train Acc: 98.11% | Test Acc: 94.74% | Loss: 0.0538\n",
      "Epoch [1328/2000] | Train Acc: 98.07% | Test Acc: 94.69% | Loss: 0.0538\n",
      "Epoch [1329/2000] | Train Acc: 98.08% | Test Acc: 94.75% | Loss: 0.0538\n",
      "Epoch [1330/2000] | Train Acc: 98.09% | Test Acc: 94.74% | Loss: 0.0537\n",
      "Epoch [1331/2000] | Train Acc: 98.09% | Test Acc: 94.71% | Loss: 0.0537\n",
      "Epoch [1332/2000] | Train Acc: 98.09% | Test Acc: 94.77% | Loss: 0.0537\n",
      "Epoch [1333/2000] | Train Acc: 98.09% | Test Acc: 94.69% | Loss: 0.0537\n",
      "Epoch [1334/2000] | Train Acc: 98.09% | Test Acc: 94.78% | Loss: 0.0536\n",
      "Epoch [1335/2000] | Train Acc: 98.08% | Test Acc: 94.68% | Loss: 0.0536\n",
      "Epoch [1336/2000] | Train Acc: 98.10% | Test Acc: 94.75% | Loss: 0.0536\n",
      "Epoch [1337/2000] | Train Acc: 98.08% | Test Acc: 94.72% | Loss: 0.0536\n",
      "Epoch [1338/2000] | Train Acc: 98.09% | Test Acc: 94.77% | Loss: 0.0536\n",
      "Epoch [1339/2000] | Train Acc: 98.08% | Test Acc: 94.69% | Loss: 0.0536\n",
      "Epoch [1340/2000] | Train Acc: 98.09% | Test Acc: 94.78% | Loss: 0.0536\n",
      "Epoch [1341/2000] | Train Acc: 98.10% | Test Acc: 94.68% | Loss: 0.0536\n",
      "Epoch [1342/2000] | Train Acc: 98.10% | Test Acc: 94.72% | Loss: 0.0537\n",
      "Epoch [1343/2000] | Train Acc: 98.08% | Test Acc: 94.69% | Loss: 0.0538\n",
      "Epoch [1344/2000] | Train Acc: 98.10% | Test Acc: 94.59% | Loss: 0.0540\n",
      "Epoch [1345/2000] | Train Acc: 98.07% | Test Acc: 94.75% | Loss: 0.0542\n",
      "Epoch [1346/2000] | Train Acc: 98.13% | Test Acc: 94.36% | Loss: 0.0547\n",
      "Epoch [1347/2000] | Train Acc: 98.02% | Test Acc: 94.81% | Loss: 0.0551\n",
      "Epoch [1348/2000] | Train Acc: 98.10% | Test Acc: 94.32% | Loss: 0.0560\n",
      "Epoch [1349/2000] | Train Acc: 97.96% | Test Acc: 94.81% | Loss: 0.0559\n",
      "Epoch [1350/2000] | Train Acc: 98.11% | Test Acc: 94.44% | Loss: 0.0561\n",
      "Epoch [1351/2000] | Train Acc: 98.02% | Test Acc: 94.68% | Loss: 0.0548\n",
      "Epoch [1352/2000] | Train Acc: 98.10% | Test Acc: 94.72% | Loss: 0.0539\n",
      "Epoch [1353/2000] | Train Acc: 98.09% | Test Acc: 94.74% | Loss: 0.0533\n",
      "Epoch [1354/2000] | Train Acc: 98.10% | Test Acc: 94.69% | Loss: 0.0534\n",
      "Epoch [1355/2000] | Train Acc: 98.12% | Test Acc: 94.51% | Loss: 0.0539\n",
      "Epoch [1356/2000] | Train Acc: 98.04% | Test Acc: 94.69% | Loss: 0.0544\n",
      "Epoch [1357/2000] | Train Acc: 98.12% | Test Acc: 94.56% | Loss: 0.0546\n",
      "Epoch [1358/2000] | Train Acc: 98.06% | Test Acc: 94.68% | Loss: 0.0541\n",
      "Epoch [1359/2000] | Train Acc: 98.11% | Test Acc: 94.75% | Loss: 0.0536\n",
      "Epoch [1360/2000] | Train Acc: 98.09% | Test Acc: 94.74% | Loss: 0.0532\n",
      "Epoch [1361/2000] | Train Acc: 98.08% | Test Acc: 94.69% | Loss: 0.0531\n",
      "Epoch [1362/2000] | Train Acc: 98.11% | Test Acc: 94.69% | Loss: 0.0534\n",
      "Epoch [1363/2000] | Train Acc: 98.08% | Test Acc: 94.74% | Loss: 0.0536\n",
      "Epoch [1364/2000] | Train Acc: 98.11% | Test Acc: 94.69% | Loss: 0.0538\n",
      "Epoch [1365/2000] | Train Acc: 98.09% | Test Acc: 94.71% | Loss: 0.0535\n",
      "Epoch [1366/2000] | Train Acc: 98.11% | Test Acc: 94.79% | Loss: 0.0533\n",
      "Epoch [1367/2000] | Train Acc: 98.10% | Test Acc: 94.77% | Loss: 0.0530\n",
      "Epoch [1368/2000] | Train Acc: 98.09% | Test Acc: 94.68% | Loss: 0.0529\n",
      "Epoch [1369/2000] | Train Acc: 98.10% | Test Acc: 94.77% | Loss: 0.0530\n",
      "Epoch [1370/2000] | Train Acc: 98.10% | Test Acc: 94.69% | Loss: 0.0531\n",
      "Epoch [1371/2000] | Train Acc: 98.12% | Test Acc: 94.74% | Loss: 0.0532\n",
      "Epoch [1372/2000] | Train Acc: 98.09% | Test Acc: 94.72% | Loss: 0.0532\n",
      "Epoch [1373/2000] | Train Acc: 98.11% | Test Acc: 94.79% | Loss: 0.0531\n",
      "Epoch [1374/2000] | Train Acc: 98.10% | Test Acc: 94.72% | Loss: 0.0529\n",
      "Epoch [1375/2000] | Train Acc: 98.11% | Test Acc: 94.74% | Loss: 0.0528\n",
      "Epoch [1376/2000] | Train Acc: 98.11% | Test Acc: 94.77% | Loss: 0.0528\n",
      "Epoch [1377/2000] | Train Acc: 98.09% | Test Acc: 94.69% | Loss: 0.0528\n",
      "Epoch [1378/2000] | Train Acc: 98.10% | Test Acc: 94.81% | Loss: 0.0529\n",
      "Epoch [1379/2000] | Train Acc: 98.10% | Test Acc: 94.69% | Loss: 0.0529\n",
      "Epoch [1380/2000] | Train Acc: 98.11% | Test Acc: 94.82% | Loss: 0.0529\n",
      "Epoch [1381/2000] | Train Acc: 98.10% | Test Acc: 94.74% | Loss: 0.0528\n",
      "Epoch [1382/2000] | Train Acc: 98.11% | Test Acc: 94.82% | Loss: 0.0527\n",
      "Epoch [1383/2000] | Train Acc: 98.11% | Test Acc: 94.75% | Loss: 0.0526\n",
      "Epoch [1384/2000] | Train Acc: 98.10% | Test Acc: 94.75% | Loss: 0.0526\n",
      "Epoch [1385/2000] | Train Acc: 98.11% | Test Acc: 94.81% | Loss: 0.0526\n",
      "Epoch [1386/2000] | Train Acc: 98.11% | Test Acc: 94.72% | Loss: 0.0526\n",
      "Epoch [1387/2000] | Train Acc: 98.11% | Test Acc: 94.81% | Loss: 0.0526\n",
      "Epoch [1388/2000] | Train Acc: 98.11% | Test Acc: 94.74% | Loss: 0.0526\n",
      "Epoch [1389/2000] | Train Acc: 98.11% | Test Acc: 94.79% | Loss: 0.0526\n",
      "Epoch [1390/2000] | Train Acc: 98.11% | Test Acc: 94.74% | Loss: 0.0526\n",
      "Epoch [1391/2000] | Train Acc: 98.10% | Test Acc: 94.78% | Loss: 0.0525\n",
      "Epoch [1392/2000] | Train Acc: 98.11% | Test Acc: 94.77% | Loss: 0.0525\n",
      "Epoch [1393/2000] | Train Acc: 98.11% | Test Acc: 94.77% | Loss: 0.0524\n",
      "Epoch [1394/2000] | Train Acc: 98.10% | Test Acc: 94.75% | Loss: 0.0524\n",
      "Epoch [1395/2000] | Train Acc: 98.11% | Test Acc: 94.75% | Loss: 0.0524\n",
      "Epoch [1396/2000] | Train Acc: 98.11% | Test Acc: 94.78% | Loss: 0.0524\n",
      "Epoch [1397/2000] | Train Acc: 98.09% | Test Acc: 94.75% | Loss: 0.0523\n",
      "Epoch [1398/2000] | Train Acc: 98.11% | Test Acc: 94.82% | Loss: 0.0523\n",
      "Epoch [1399/2000] | Train Acc: 98.11% | Test Acc: 94.77% | Loss: 0.0523\n",
      "Epoch [1400/2000] | Train Acc: 98.11% | Test Acc: 94.82% | Loss: 0.0523\n",
      "Epoch [1401/2000] | Train Acc: 98.12% | Test Acc: 94.75% | Loss: 0.0523\n",
      "Epoch [1402/2000] | Train Acc: 98.11% | Test Acc: 94.82% | Loss: 0.0523\n",
      "Epoch [1403/2000] | Train Acc: 98.11% | Test Acc: 94.75% | Loss: 0.0523\n",
      "Epoch [1404/2000] | Train Acc: 98.11% | Test Acc: 94.85% | Loss: 0.0522\n",
      "Epoch [1405/2000] | Train Acc: 98.11% | Test Acc: 94.77% | Loss: 0.0522\n",
      "Epoch [1406/2000] | Train Acc: 98.11% | Test Acc: 94.87% | Loss: 0.0522\n",
      "Epoch [1407/2000] | Train Acc: 98.12% | Test Acc: 94.77% | Loss: 0.0522\n",
      "Epoch [1408/2000] | Train Acc: 98.11% | Test Acc: 94.87% | Loss: 0.0522\n",
      "Epoch [1409/2000] | Train Acc: 98.12% | Test Acc: 94.77% | Loss: 0.0522\n",
      "Epoch [1410/2000] | Train Acc: 98.11% | Test Acc: 94.88% | Loss: 0.0521\n",
      "Epoch [1411/2000] | Train Acc: 98.12% | Test Acc: 94.78% | Loss: 0.0521\n",
      "Epoch [1412/2000] | Train Acc: 98.11% | Test Acc: 94.84% | Loss: 0.0521\n",
      "Epoch [1413/2000] | Train Acc: 98.11% | Test Acc: 94.77% | Loss: 0.0521\n",
      "Epoch [1414/2000] | Train Acc: 98.11% | Test Acc: 94.85% | Loss: 0.0521\n",
      "Epoch [1415/2000] | Train Acc: 98.12% | Test Acc: 94.75% | Loss: 0.0521\n",
      "Epoch [1416/2000] | Train Acc: 98.13% | Test Acc: 94.84% | Loss: 0.0521\n",
      "Epoch [1417/2000] | Train Acc: 98.13% | Test Acc: 94.77% | Loss: 0.0521\n",
      "Epoch [1418/2000] | Train Acc: 98.11% | Test Acc: 94.75% | Loss: 0.0522\n",
      "Epoch [1419/2000] | Train Acc: 98.10% | Test Acc: 94.79% | Loss: 0.0523\n",
      "Epoch [1420/2000] | Train Acc: 98.14% | Test Acc: 94.75% | Loss: 0.0524\n",
      "Epoch [1421/2000] | Train Acc: 98.10% | Test Acc: 94.78% | Loss: 0.0525\n",
      "Epoch [1422/2000] | Train Acc: 98.16% | Test Acc: 94.63% | Loss: 0.0527\n",
      "Epoch [1423/2000] | Train Acc: 98.08% | Test Acc: 94.81% | Loss: 0.0528\n",
      "Epoch [1424/2000] | Train Acc: 98.16% | Test Acc: 94.60% | Loss: 0.0531\n",
      "Epoch [1425/2000] | Train Acc: 98.07% | Test Acc: 94.81% | Loss: 0.0530\n",
      "Epoch [1426/2000] | Train Acc: 98.16% | Test Acc: 94.63% | Loss: 0.0531\n",
      "Epoch [1427/2000] | Train Acc: 98.09% | Test Acc: 94.79% | Loss: 0.0528\n",
      "Epoch [1428/2000] | Train Acc: 98.16% | Test Acc: 94.75% | Loss: 0.0525\n",
      "Epoch [1429/2000] | Train Acc: 98.10% | Test Acc: 94.75% | Loss: 0.0521\n",
      "Epoch [1430/2000] | Train Acc: 98.13% | Test Acc: 94.77% | Loss: 0.0519\n",
      "Epoch [1431/2000] | Train Acc: 98.09% | Test Acc: 94.79% | Loss: 0.0517\n",
      "Epoch [1432/2000] | Train Acc: 98.10% | Test Acc: 94.75% | Loss: 0.0517\n",
      "Epoch [1433/2000] | Train Acc: 98.13% | Test Acc: 94.77% | Loss: 0.0518\n",
      "Epoch [1434/2000] | Train Acc: 98.11% | Test Acc: 94.79% | Loss: 0.0519\n",
      "Epoch [1435/2000] | Train Acc: 98.15% | Test Acc: 94.72% | Loss: 0.0521\n",
      "Epoch [1436/2000] | Train Acc: 98.11% | Test Acc: 94.82% | Loss: 0.0521\n",
      "Epoch [1437/2000] | Train Acc: 98.15% | Test Acc: 94.74% | Loss: 0.0522\n",
      "Epoch [1438/2000] | Train Acc: 98.10% | Test Acc: 94.79% | Loss: 0.0521\n",
      "Epoch [1439/2000] | Train Acc: 98.14% | Test Acc: 94.78% | Loss: 0.0520\n",
      "Epoch [1440/2000] | Train Acc: 98.11% | Test Acc: 94.78% | Loss: 0.0518\n",
      "Epoch [1441/2000] | Train Acc: 98.13% | Test Acc: 94.88% | Loss: 0.0516\n",
      "Epoch [1442/2000] | Train Acc: 98.13% | Test Acc: 94.78% | Loss: 0.0515\n",
      "Epoch [1443/2000] | Train Acc: 98.13% | Test Acc: 94.78% | Loss: 0.0515\n",
      "Epoch [1444/2000] | Train Acc: 98.13% | Test Acc: 94.85% | Loss: 0.0515\n",
      "Epoch [1445/2000] | Train Acc: 98.12% | Test Acc: 94.78% | Loss: 0.0515\n",
      "Epoch [1446/2000] | Train Acc: 98.12% | Test Acc: 94.84% | Loss: 0.0515\n",
      "Epoch [1447/2000] | Train Acc: 98.12% | Test Acc: 94.78% | Loss: 0.0516\n",
      "Epoch [1448/2000] | Train Acc: 98.14% | Test Acc: 94.78% | Loss: 0.0516\n",
      "Epoch [1449/2000] | Train Acc: 98.11% | Test Acc: 94.78% | Loss: 0.0516\n",
      "Epoch [1450/2000] | Train Acc: 98.14% | Test Acc: 94.79% | Loss: 0.0516\n",
      "Epoch [1451/2000] | Train Acc: 98.11% | Test Acc: 94.79% | Loss: 0.0516\n",
      "Epoch [1452/2000] | Train Acc: 98.13% | Test Acc: 94.85% | Loss: 0.0515\n",
      "Epoch [1453/2000] | Train Acc: 98.12% | Test Acc: 94.79% | Loss: 0.0515\n",
      "Epoch [1454/2000] | Train Acc: 98.13% | Test Acc: 94.87% | Loss: 0.0514\n",
      "Epoch [1455/2000] | Train Acc: 98.14% | Test Acc: 94.79% | Loss: 0.0513\n",
      "Epoch [1456/2000] | Train Acc: 98.12% | Test Acc: 94.85% | Loss: 0.0513\n",
      "Epoch [1457/2000] | Train Acc: 98.13% | Test Acc: 94.78% | Loss: 0.0512\n",
      "Epoch [1458/2000] | Train Acc: 98.14% | Test Acc: 94.79% | Loss: 0.0512\n",
      "Epoch [1459/2000] | Train Acc: 98.12% | Test Acc: 94.81% | Loss: 0.0512\n",
      "Epoch [1460/2000] | Train Acc: 98.12% | Test Acc: 94.78% | Loss: 0.0512\n",
      "Epoch [1461/2000] | Train Acc: 98.14% | Test Acc: 94.84% | Loss: 0.0511\n",
      "Epoch [1462/2000] | Train Acc: 98.13% | Test Acc: 94.78% | Loss: 0.0511\n",
      "Epoch [1463/2000] | Train Acc: 98.13% | Test Acc: 94.85% | Loss: 0.0511\n",
      "Epoch [1464/2000] | Train Acc: 98.13% | Test Acc: 94.79% | Loss: 0.0511\n",
      "Epoch [1465/2000] | Train Acc: 98.12% | Test Acc: 94.88% | Loss: 0.0511\n",
      "Epoch [1466/2000] | Train Acc: 98.15% | Test Acc: 94.79% | Loss: 0.0511\n",
      "Epoch [1467/2000] | Train Acc: 98.13% | Test Acc: 94.85% | Loss: 0.0511\n",
      "Epoch [1468/2000] | Train Acc: 98.13% | Test Acc: 94.78% | Loss: 0.0512\n",
      "Epoch [1469/2000] | Train Acc: 98.15% | Test Acc: 94.82% | Loss: 0.0512\n",
      "Epoch [1470/2000] | Train Acc: 98.12% | Test Acc: 94.78% | Loss: 0.0512\n",
      "Epoch [1471/2000] | Train Acc: 98.16% | Test Acc: 94.75% | Loss: 0.0513\n",
      "Epoch [1472/2000] | Train Acc: 98.13% | Test Acc: 94.81% | Loss: 0.0514\n",
      "Epoch [1473/2000] | Train Acc: 98.18% | Test Acc: 94.68% | Loss: 0.0515\n",
      "Epoch [1474/2000] | Train Acc: 98.11% | Test Acc: 94.79% | Loss: 0.0516\n",
      "Epoch [1475/2000] | Train Acc: 98.18% | Test Acc: 94.65% | Loss: 0.0519\n",
      "Epoch [1476/2000] | Train Acc: 98.09% | Test Acc: 94.81% | Loss: 0.0519\n",
      "Epoch [1477/2000] | Train Acc: 98.17% | Test Acc: 94.63% | Loss: 0.0521\n",
      "Epoch [1478/2000] | Train Acc: 98.09% | Test Acc: 94.77% | Loss: 0.0520\n",
      "Epoch [1479/2000] | Train Acc: 98.18% | Test Acc: 94.66% | Loss: 0.0519\n",
      "Epoch [1480/2000] | Train Acc: 98.11% | Test Acc: 94.82% | Loss: 0.0515\n",
      "Epoch [1481/2000] | Train Acc: 98.18% | Test Acc: 94.82% | Loss: 0.0513\n",
      "Epoch [1482/2000] | Train Acc: 98.13% | Test Acc: 94.81% | Loss: 0.0510\n",
      "Epoch [1483/2000] | Train Acc: 98.13% | Test Acc: 94.79% | Loss: 0.0508\n",
      "Epoch [1484/2000] | Train Acc: 98.14% | Test Acc: 94.85% | Loss: 0.0507\n",
      "Epoch [1485/2000] | Train Acc: 98.13% | Test Acc: 94.78% | Loss: 0.0508\n",
      "Epoch [1486/2000] | Train Acc: 98.15% | Test Acc: 94.79% | Loss: 0.0509\n",
      "Epoch [1487/2000] | Train Acc: 98.13% | Test Acc: 94.82% | Loss: 0.0510\n",
      "Epoch [1488/2000] | Train Acc: 98.17% | Test Acc: 94.77% | Loss: 0.0511\n",
      "Epoch [1489/2000] | Train Acc: 98.13% | Test Acc: 94.82% | Loss: 0.0511\n",
      "Epoch [1490/2000] | Train Acc: 98.18% | Test Acc: 94.77% | Loss: 0.0512\n",
      "Epoch [1491/2000] | Train Acc: 98.13% | Test Acc: 94.84% | Loss: 0.0511\n",
      "Epoch [1492/2000] | Train Acc: 98.18% | Test Acc: 94.79% | Loss: 0.0511\n",
      "Epoch [1493/2000] | Train Acc: 98.13% | Test Acc: 94.79% | Loss: 0.0509\n",
      "Epoch [1494/2000] | Train Acc: 98.15% | Test Acc: 94.90% | Loss: 0.0508\n",
      "Epoch [1495/2000] | Train Acc: 98.15% | Test Acc: 94.81% | Loss: 0.0507\n",
      "Epoch [1496/2000] | Train Acc: 98.13% | Test Acc: 94.79% | Loss: 0.0506\n",
      "Epoch [1497/2000] | Train Acc: 98.13% | Test Acc: 94.79% | Loss: 0.0505\n",
      "Epoch [1498/2000] | Train Acc: 98.13% | Test Acc: 94.81% | Loss: 0.0505\n",
      "Epoch [1499/2000] | Train Acc: 98.15% | Test Acc: 94.90% | Loss: 0.0505\n",
      "Epoch [1500/2000] | Train Acc: 98.14% | Test Acc: 94.78% | Loss: 0.0505\n",
      "Epoch [1501/2000] | Train Acc: 98.15% | Test Acc: 94.82% | Loss: 0.0506\n",
      "Epoch [1502/2000] | Train Acc: 98.13% | Test Acc: 94.78% | Loss: 0.0506\n",
      "Epoch [1503/2000] | Train Acc: 98.16% | Test Acc: 94.82% | Loss: 0.0506\n",
      "Epoch [1504/2000] | Train Acc: 98.12% | Test Acc: 94.78% | Loss: 0.0506\n",
      "Epoch [1505/2000] | Train Acc: 98.16% | Test Acc: 94.84% | Loss: 0.0506\n",
      "Epoch [1506/2000] | Train Acc: 98.13% | Test Acc: 94.79% | Loss: 0.0506\n",
      "Epoch [1507/2000] | Train Acc: 98.16% | Test Acc: 94.85% | Loss: 0.0506\n",
      "Epoch [1508/2000] | Train Acc: 98.13% | Test Acc: 94.78% | Loss: 0.0506\n",
      "Epoch [1509/2000] | Train Acc: 98.17% | Test Acc: 94.82% | Loss: 0.0505\n",
      "Epoch [1510/2000] | Train Acc: 98.13% | Test Acc: 94.78% | Loss: 0.0505\n",
      "Epoch [1511/2000] | Train Acc: 98.16% | Test Acc: 94.81% | Loss: 0.0505\n",
      "Epoch [1512/2000] | Train Acc: 98.14% | Test Acc: 94.79% | Loss: 0.0504\n",
      "Epoch [1513/2000] | Train Acc: 98.16% | Test Acc: 94.85% | Loss: 0.0504\n",
      "Epoch [1514/2000] | Train Acc: 98.14% | Test Acc: 94.81% | Loss: 0.0503\n",
      "Epoch [1515/2000] | Train Acc: 98.15% | Test Acc: 94.88% | Loss: 0.0503\n",
      "Epoch [1516/2000] | Train Acc: 98.15% | Test Acc: 94.79% | Loss: 0.0503\n",
      "Epoch [1517/2000] | Train Acc: 98.15% | Test Acc: 94.88% | Loss: 0.0503\n",
      "Epoch [1518/2000] | Train Acc: 98.14% | Test Acc: 94.79% | Loss: 0.0502\n",
      "Epoch [1519/2000] | Train Acc: 98.15% | Test Acc: 94.88% | Loss: 0.0502\n",
      "Epoch [1520/2000] | Train Acc: 98.14% | Test Acc: 94.79% | Loss: 0.0502\n",
      "Epoch [1521/2000] | Train Acc: 98.15% | Test Acc: 94.88% | Loss: 0.0502\n",
      "Epoch [1522/2000] | Train Acc: 98.15% | Test Acc: 94.81% | Loss: 0.0502\n",
      "Epoch [1523/2000] | Train Acc: 98.16% | Test Acc: 94.87% | Loss: 0.0502\n",
      "Epoch [1524/2000] | Train Acc: 98.15% | Test Acc: 94.78% | Loss: 0.0502\n",
      "Epoch [1525/2000] | Train Acc: 98.17% | Test Acc: 94.82% | Loss: 0.0502\n",
      "Epoch [1526/2000] | Train Acc: 98.14% | Test Acc: 94.81% | Loss: 0.0502\n",
      "Epoch [1527/2000] | Train Acc: 98.17% | Test Acc: 94.78% | Loss: 0.0503\n",
      "Epoch [1528/2000] | Train Acc: 98.14% | Test Acc: 94.84% | Loss: 0.0503\n",
      "Epoch [1529/2000] | Train Acc: 98.19% | Test Acc: 94.75% | Loss: 0.0504\n",
      "Epoch [1530/2000] | Train Acc: 98.13% | Test Acc: 94.81% | Loss: 0.0505\n",
      "Epoch [1531/2000] | Train Acc: 98.18% | Test Acc: 94.66% | Loss: 0.0507\n",
      "Epoch [1532/2000] | Train Acc: 98.12% | Test Acc: 94.84% | Loss: 0.0509\n",
      "Epoch [1533/2000] | Train Acc: 98.19% | Test Acc: 94.59% | Loss: 0.0512\n",
      "Epoch [1534/2000] | Train Acc: 98.10% | Test Acc: 94.84% | Loss: 0.0511\n",
      "Epoch [1535/2000] | Train Acc: 98.20% | Test Acc: 94.62% | Loss: 0.0513\n",
      "Epoch [1536/2000] | Train Acc: 98.11% | Test Acc: 94.79% | Loss: 0.0510\n",
      "Epoch [1537/2000] | Train Acc: 98.18% | Test Acc: 94.75% | Loss: 0.0508\n",
      "Epoch [1538/2000] | Train Acc: 98.14% | Test Acc: 94.82% | Loss: 0.0504\n",
      "Epoch [1539/2000] | Train Acc: 98.18% | Test Acc: 94.85% | Loss: 0.0501\n",
      "Epoch [1540/2000] | Train Acc: 98.13% | Test Acc: 94.84% | Loss: 0.0498\n",
      "Epoch [1541/2000] | Train Acc: 98.15% | Test Acc: 94.81% | Loss: 0.0498\n",
      "Epoch [1542/2000] | Train Acc: 98.17% | Test Acc: 94.84% | Loss: 0.0499\n",
      "Epoch [1543/2000] | Train Acc: 98.14% | Test Acc: 94.84% | Loss: 0.0500\n",
      "Epoch [1544/2000] | Train Acc: 98.20% | Test Acc: 94.75% | Loss: 0.0501\n",
      "Epoch [1545/2000] | Train Acc: 98.14% | Test Acc: 94.81% | Loss: 0.0502\n",
      "Epoch [1546/2000] | Train Acc: 98.19% | Test Acc: 94.77% | Loss: 0.0503\n",
      "Epoch [1547/2000] | Train Acc: 98.14% | Test Acc: 94.81% | Loss: 0.0503\n",
      "Epoch [1548/2000] | Train Acc: 98.20% | Test Acc: 94.82% | Loss: 0.0502\n",
      "Epoch [1549/2000] | Train Acc: 98.14% | Test Acc: 94.82% | Loss: 0.0500\n",
      "Epoch [1550/2000] | Train Acc: 98.19% | Test Acc: 94.93% | Loss: 0.0499\n",
      "Epoch [1551/2000] | Train Acc: 98.15% | Test Acc: 94.82% | Loss: 0.0497\n",
      "Epoch [1552/2000] | Train Acc: 98.16% | Test Acc: 94.81% | Loss: 0.0496\n",
      "Epoch [1553/2000] | Train Acc: 98.16% | Test Acc: 94.88% | Loss: 0.0496\n",
      "Epoch [1554/2000] | Train Acc: 98.15% | Test Acc: 94.79% | Loss: 0.0496\n",
      "Epoch [1555/2000] | Train Acc: 98.18% | Test Acc: 94.85% | Loss: 0.0496\n",
      "Epoch [1556/2000] | Train Acc: 98.16% | Test Acc: 94.81% | Loss: 0.0497\n",
      "Epoch [1557/2000] | Train Acc: 98.19% | Test Acc: 94.87% | Loss: 0.0497\n",
      "Epoch [1558/2000] | Train Acc: 98.15% | Test Acc: 94.81% | Loss: 0.0498\n",
      "Epoch [1559/2000] | Train Acc: 98.20% | Test Acc: 94.85% | Loss: 0.0498\n",
      "Epoch [1560/2000] | Train Acc: 98.15% | Test Acc: 94.82% | Loss: 0.0497\n",
      "Epoch [1561/2000] | Train Acc: 98.21% | Test Acc: 94.87% | Loss: 0.0497\n",
      "Epoch [1562/2000] | Train Acc: 98.15% | Test Acc: 94.84% | Loss: 0.0497\n",
      "Epoch [1563/2000] | Train Acc: 98.18% | Test Acc: 94.90% | Loss: 0.0496\n",
      "Epoch [1564/2000] | Train Acc: 98.17% | Test Acc: 94.81% | Loss: 0.0495\n",
      "Epoch [1565/2000] | Train Acc: 98.18% | Test Acc: 94.93% | Loss: 0.0495\n",
      "Epoch [1566/2000] | Train Acc: 98.15% | Test Acc: 94.85% | Loss: 0.0494\n",
      "Epoch [1567/2000] | Train Acc: 98.18% | Test Acc: 94.85% | Loss: 0.0494\n",
      "Epoch [1568/2000] | Train Acc: 98.16% | Test Acc: 94.81% | Loss: 0.0494\n",
      "Epoch [1569/2000] | Train Acc: 98.17% | Test Acc: 94.81% | Loss: 0.0493\n",
      "Epoch [1570/2000] | Train Acc: 98.17% | Test Acc: 94.85% | Loss: 0.0493\n",
      "Epoch [1571/2000] | Train Acc: 98.16% | Test Acc: 94.84% | Loss: 0.0493\n",
      "Epoch [1572/2000] | Train Acc: 98.17% | Test Acc: 94.88% | Loss: 0.0493\n",
      "Epoch [1573/2000] | Train Acc: 98.17% | Test Acc: 94.85% | Loss: 0.0493\n",
      "Epoch [1574/2000] | Train Acc: 98.18% | Test Acc: 94.93% | Loss: 0.0493\n",
      "Epoch [1575/2000] | Train Acc: 98.16% | Test Acc: 94.82% | Loss: 0.0493\n",
      "Epoch [1576/2000] | Train Acc: 98.18% | Test Acc: 94.90% | Loss: 0.0493\n",
      "Epoch [1577/2000] | Train Acc: 98.17% | Test Acc: 94.88% | Loss: 0.0493\n",
      "Epoch [1578/2000] | Train Acc: 98.18% | Test Acc: 94.90% | Loss: 0.0493\n",
      "Epoch [1579/2000] | Train Acc: 98.19% | Test Acc: 94.84% | Loss: 0.0493\n",
      "Epoch [1580/2000] | Train Acc: 98.20% | Test Acc: 94.87% | Loss: 0.0494\n",
      "Epoch [1581/2000] | Train Acc: 98.16% | Test Acc: 94.85% | Loss: 0.0494\n",
      "Epoch [1582/2000] | Train Acc: 98.21% | Test Acc: 94.79% | Loss: 0.0495\n",
      "Epoch [1583/2000] | Train Acc: 98.16% | Test Acc: 94.82% | Loss: 0.0496\n",
      "Epoch [1584/2000] | Train Acc: 98.21% | Test Acc: 94.74% | Loss: 0.0498\n",
      "Epoch [1585/2000] | Train Acc: 98.17% | Test Acc: 94.85% | Loss: 0.0499\n",
      "Epoch [1586/2000] | Train Acc: 98.20% | Test Acc: 94.59% | Loss: 0.0502\n",
      "Epoch [1587/2000] | Train Acc: 98.13% | Test Acc: 94.88% | Loss: 0.0502\n",
      "Epoch [1588/2000] | Train Acc: 98.20% | Test Acc: 94.59% | Loss: 0.0505\n",
      "Epoch [1589/2000] | Train Acc: 98.13% | Test Acc: 94.84% | Loss: 0.0502\n",
      "Epoch [1590/2000] | Train Acc: 98.20% | Test Acc: 94.72% | Loss: 0.0501\n",
      "Epoch [1591/2000] | Train Acc: 98.17% | Test Acc: 94.84% | Loss: 0.0497\n",
      "Epoch [1592/2000] | Train Acc: 98.21% | Test Acc: 94.85% | Loss: 0.0494\n",
      "Epoch [1593/2000] | Train Acc: 98.16% | Test Acc: 94.84% | Loss: 0.0491\n",
      "Epoch [1594/2000] | Train Acc: 98.18% | Test Acc: 94.87% | Loss: 0.0490\n",
      "Epoch [1595/2000] | Train Acc: 98.19% | Test Acc: 94.87% | Loss: 0.0490\n",
      "Epoch [1596/2000] | Train Acc: 98.18% | Test Acc: 94.82% | Loss: 0.0491\n",
      "Epoch [1597/2000] | Train Acc: 98.20% | Test Acc: 94.78% | Loss: 0.0492\n",
      "Epoch [1598/2000] | Train Acc: 98.17% | Test Acc: 94.81% | Loss: 0.0493\n",
      "Epoch [1599/2000] | Train Acc: 98.20% | Test Acc: 94.75% | Loss: 0.0495\n",
      "Epoch [1600/2000] | Train Acc: 98.18% | Test Acc: 94.81% | Loss: 0.0495\n",
      "Epoch [1601/2000] | Train Acc: 98.20% | Test Acc: 94.79% | Loss: 0.0495\n",
      "Epoch [1602/2000] | Train Acc: 98.17% | Test Acc: 94.85% | Loss: 0.0493\n",
      "Epoch [1603/2000] | Train Acc: 98.21% | Test Acc: 94.91% | Loss: 0.0492\n",
      "Epoch [1604/2000] | Train Acc: 98.19% | Test Acc: 94.87% | Loss: 0.0490\n",
      "Epoch [1605/2000] | Train Acc: 98.19% | Test Acc: 94.93% | Loss: 0.0489\n",
      "Epoch [1606/2000] | Train Acc: 98.18% | Test Acc: 94.81% | Loss: 0.0488\n",
      "Epoch [1607/2000] | Train Acc: 98.18% | Test Acc: 94.84% | Loss: 0.0488\n",
      "Epoch [1608/2000] | Train Acc: 98.20% | Test Acc: 94.90% | Loss: 0.0488\n",
      "Epoch [1609/2000] | Train Acc: 98.18% | Test Acc: 94.88% | Loss: 0.0488\n",
      "Epoch [1610/2000] | Train Acc: 98.20% | Test Acc: 94.88% | Loss: 0.0488\n",
      "Epoch [1611/2000] | Train Acc: 98.20% | Test Acc: 94.82% | Loss: 0.0489\n",
      "Epoch [1612/2000] | Train Acc: 98.21% | Test Acc: 94.88% | Loss: 0.0489\n",
      "Epoch [1613/2000] | Train Acc: 98.18% | Test Acc: 94.81% | Loss: 0.0489\n",
      "Epoch [1614/2000] | Train Acc: 98.21% | Test Acc: 94.88% | Loss: 0.0489\n",
      "Epoch [1615/2000] | Train Acc: 98.18% | Test Acc: 94.81% | Loss: 0.0489\n",
      "Epoch [1616/2000] | Train Acc: 98.21% | Test Acc: 94.88% | Loss: 0.0489\n",
      "Epoch [1617/2000] | Train Acc: 98.20% | Test Acc: 94.84% | Loss: 0.0488\n",
      "Epoch [1618/2000] | Train Acc: 98.20% | Test Acc: 94.88% | Loss: 0.0488\n",
      "Epoch [1619/2000] | Train Acc: 98.20% | Test Acc: 94.87% | Loss: 0.0487\n",
      "Epoch [1620/2000] | Train Acc: 98.20% | Test Acc: 94.91% | Loss: 0.0487\n",
      "Epoch [1621/2000] | Train Acc: 98.19% | Test Acc: 94.85% | Loss: 0.0486\n",
      "Epoch [1622/2000] | Train Acc: 98.20% | Test Acc: 94.90% | Loss: 0.0486\n",
      "Epoch [1623/2000] | Train Acc: 98.19% | Test Acc: 94.81% | Loss: 0.0486\n",
      "Epoch [1624/2000] | Train Acc: 98.21% | Test Acc: 94.82% | Loss: 0.0485\n",
      "Epoch [1625/2000] | Train Acc: 98.20% | Test Acc: 94.81% | Loss: 0.0485\n",
      "Epoch [1626/2000] | Train Acc: 98.20% | Test Acc: 94.81% | Loss: 0.0485\n",
      "Epoch [1627/2000] | Train Acc: 98.21% | Test Acc: 94.82% | Loss: 0.0485\n",
      "Epoch [1628/2000] | Train Acc: 98.21% | Test Acc: 94.82% | Loss: 0.0485\n",
      "Epoch [1629/2000] | Train Acc: 98.21% | Test Acc: 94.85% | Loss: 0.0485\n",
      "Epoch [1630/2000] | Train Acc: 98.21% | Test Acc: 94.81% | Loss: 0.0484\n",
      "Epoch [1631/2000] | Train Acc: 98.21% | Test Acc: 94.90% | Loss: 0.0484\n",
      "Epoch [1632/2000] | Train Acc: 98.20% | Test Acc: 94.85% | Loss: 0.0484\n",
      "Epoch [1633/2000] | Train Acc: 98.21% | Test Acc: 94.90% | Loss: 0.0484\n",
      "Epoch [1634/2000] | Train Acc: 98.21% | Test Acc: 94.87% | Loss: 0.0484\n",
      "Epoch [1635/2000] | Train Acc: 98.21% | Test Acc: 94.85% | Loss: 0.0484\n",
      "Epoch [1636/2000] | Train Acc: 98.21% | Test Acc: 94.85% | Loss: 0.0484\n",
      "Epoch [1637/2000] | Train Acc: 98.20% | Test Acc: 94.87% | Loss: 0.0485\n",
      "Epoch [1638/2000] | Train Acc: 98.22% | Test Acc: 94.84% | Loss: 0.0485\n",
      "Epoch [1639/2000] | Train Acc: 98.22% | Test Acc: 94.85% | Loss: 0.0486\n",
      "Epoch [1640/2000] | Train Acc: 98.21% | Test Acc: 94.82% | Loss: 0.0487\n",
      "Epoch [1641/2000] | Train Acc: 98.22% | Test Acc: 94.69% | Loss: 0.0489\n",
      "Epoch [1642/2000] | Train Acc: 98.19% | Test Acc: 94.85% | Loss: 0.0491\n",
      "Epoch [1643/2000] | Train Acc: 98.20% | Test Acc: 94.62% | Loss: 0.0495\n",
      "Epoch [1644/2000] | Train Acc: 98.16% | Test Acc: 94.93% | Loss: 0.0497\n",
      "Epoch [1645/2000] | Train Acc: 98.20% | Test Acc: 94.56% | Loss: 0.0503\n",
      "Epoch [1646/2000] | Train Acc: 98.13% | Test Acc: 94.93% | Loss: 0.0502\n",
      "Epoch [1647/2000] | Train Acc: 98.20% | Test Acc: 94.59% | Loss: 0.0503\n",
      "Epoch [1648/2000] | Train Acc: 98.16% | Test Acc: 94.84% | Loss: 0.0495\n",
      "Epoch [1649/2000] | Train Acc: 98.22% | Test Acc: 94.84% | Loss: 0.0489\n",
      "Epoch [1650/2000] | Train Acc: 98.18% | Test Acc: 94.84% | Loss: 0.0483\n",
      "Epoch [1651/2000] | Train Acc: 98.22% | Test Acc: 94.84% | Loss: 0.0482\n",
      "Epoch [1652/2000] | Train Acc: 98.21% | Test Acc: 94.78% | Loss: 0.0483\n",
      "Epoch [1653/2000] | Train Acc: 98.20% | Test Acc: 94.85% | Loss: 0.0486\n",
      "Epoch [1654/2000] | Train Acc: 98.21% | Test Acc: 94.65% | Loss: 0.0490\n",
      "Epoch [1655/2000] | Train Acc: 98.17% | Test Acc: 94.87% | Loss: 0.0490\n",
      "Epoch [1656/2000] | Train Acc: 98.21% | Test Acc: 94.78% | Loss: 0.0491\n",
      "Epoch [1657/2000] | Train Acc: 98.20% | Test Acc: 94.82% | Loss: 0.0487\n",
      "Epoch [1658/2000] | Train Acc: 98.22% | Test Acc: 94.85% | Loss: 0.0484\n",
      "Epoch [1659/2000] | Train Acc: 98.20% | Test Acc: 94.85% | Loss: 0.0481\n",
      "Epoch [1660/2000] | Train Acc: 98.23% | Test Acc: 94.87% | Loss: 0.0480\n",
      "Epoch [1661/2000] | Train Acc: 98.21% | Test Acc: 94.91% | Loss: 0.0481\n",
      "Epoch [1662/2000] | Train Acc: 98.23% | Test Acc: 94.84% | Loss: 0.0482\n",
      "Epoch [1663/2000] | Train Acc: 98.22% | Test Acc: 94.82% | Loss: 0.0484\n",
      "Epoch [1664/2000] | Train Acc: 98.21% | Test Acc: 94.79% | Loss: 0.0484\n",
      "Epoch [1665/2000] | Train Acc: 98.22% | Test Acc: 94.94% | Loss: 0.0484\n",
      "Epoch [1666/2000] | Train Acc: 98.22% | Test Acc: 94.84% | Loss: 0.0483\n",
      "Epoch [1667/2000] | Train Acc: 98.22% | Test Acc: 94.88% | Loss: 0.0481\n",
      "Epoch [1668/2000] | Train Acc: 98.22% | Test Acc: 94.85% | Loss: 0.0480\n",
      "Epoch [1669/2000] | Train Acc: 98.23% | Test Acc: 94.85% | Loss: 0.0479\n",
      "Epoch [1670/2000] | Train Acc: 98.24% | Test Acc: 94.84% | Loss: 0.0479\n",
      "Epoch [1671/2000] | Train Acc: 98.22% | Test Acc: 94.85% | Loss: 0.0479\n",
      "Epoch [1672/2000] | Train Acc: 98.22% | Test Acc: 94.88% | Loss: 0.0480\n",
      "Epoch [1673/2000] | Train Acc: 98.23% | Test Acc: 94.82% | Loss: 0.0480\n",
      "Epoch [1674/2000] | Train Acc: 98.22% | Test Acc: 94.93% | Loss: 0.0480\n",
      "Epoch [1675/2000] | Train Acc: 98.22% | Test Acc: 94.84% | Loss: 0.0480\n",
      "Epoch [1676/2000] | Train Acc: 98.22% | Test Acc: 94.87% | Loss: 0.0480\n",
      "Epoch [1677/2000] | Train Acc: 98.22% | Test Acc: 94.87% | Loss: 0.0479\n",
      "Epoch [1678/2000] | Train Acc: 98.23% | Test Acc: 94.88% | Loss: 0.0478\n",
      "Epoch [1679/2000] | Train Acc: 98.23% | Test Acc: 94.84% | Loss: 0.0478\n",
      "Epoch [1680/2000] | Train Acc: 98.25% | Test Acc: 94.85% | Loss: 0.0478\n",
      "Epoch [1681/2000] | Train Acc: 98.25% | Test Acc: 94.88% | Loss: 0.0477\n",
      "Epoch [1682/2000] | Train Acc: 98.23% | Test Acc: 94.85% | Loss: 0.0477\n",
      "Epoch [1683/2000] | Train Acc: 98.23% | Test Acc: 94.88% | Loss: 0.0477\n",
      "Epoch [1684/2000] | Train Acc: 98.23% | Test Acc: 94.88% | Loss: 0.0477\n",
      "Epoch [1685/2000] | Train Acc: 98.23% | Test Acc: 94.84% | Loss: 0.0477\n",
      "Epoch [1686/2000] | Train Acc: 98.23% | Test Acc: 94.85% | Loss: 0.0477\n",
      "Epoch [1687/2000] | Train Acc: 98.23% | Test Acc: 94.85% | Loss: 0.0477\n",
      "Epoch [1688/2000] | Train Acc: 98.23% | Test Acc: 94.84% | Loss: 0.0477\n",
      "Epoch [1689/2000] | Train Acc: 98.23% | Test Acc: 94.85% | Loss: 0.0477\n",
      "Epoch [1690/2000] | Train Acc: 98.23% | Test Acc: 94.85% | Loss: 0.0477\n",
      "Epoch [1691/2000] | Train Acc: 98.23% | Test Acc: 94.88% | Loss: 0.0477\n",
      "Epoch [1692/2000] | Train Acc: 98.23% | Test Acc: 94.85% | Loss: 0.0477\n",
      "Epoch [1693/2000] | Train Acc: 98.24% | Test Acc: 94.87% | Loss: 0.0476\n",
      "Epoch [1694/2000] | Train Acc: 98.23% | Test Acc: 94.88% | Loss: 0.0476\n",
      "Epoch [1695/2000] | Train Acc: 98.24% | Test Acc: 94.85% | Loss: 0.0476\n",
      "Epoch [1696/2000] | Train Acc: 98.24% | Test Acc: 94.85% | Loss: 0.0476\n",
      "Epoch [1697/2000] | Train Acc: 98.25% | Test Acc: 94.85% | Loss: 0.0476\n",
      "Epoch [1698/2000] | Train Acc: 98.24% | Test Acc: 94.87% | Loss: 0.0475\n",
      "Epoch [1699/2000] | Train Acc: 98.25% | Test Acc: 94.85% | Loss: 0.0475\n",
      "Epoch [1700/2000] | Train Acc: 98.25% | Test Acc: 94.87% | Loss: 0.0475\n",
      "Epoch [1701/2000] | Train Acc: 98.24% | Test Acc: 94.85% | Loss: 0.0475\n",
      "Epoch [1702/2000] | Train Acc: 98.25% | Test Acc: 94.88% | Loss: 0.0475\n",
      "Epoch [1703/2000] | Train Acc: 98.26% | Test Acc: 94.84% | Loss: 0.0475\n",
      "Epoch [1704/2000] | Train Acc: 98.24% | Test Acc: 94.85% | Loss: 0.0475\n",
      "Epoch [1705/2000] | Train Acc: 98.25% | Test Acc: 94.85% | Loss: 0.0475\n",
      "Epoch [1706/2000] | Train Acc: 98.23% | Test Acc: 94.84% | Loss: 0.0475\n",
      "Epoch [1707/2000] | Train Acc: 98.24% | Test Acc: 94.88% | Loss: 0.0475\n",
      "Epoch [1708/2000] | Train Acc: 98.25% | Test Acc: 94.78% | Loss: 0.0475\n",
      "Epoch [1709/2000] | Train Acc: 98.24% | Test Acc: 94.91% | Loss: 0.0476\n",
      "Epoch [1710/2000] | Train Acc: 98.25% | Test Acc: 94.79% | Loss: 0.0476\n",
      "Epoch [1711/2000] | Train Acc: 98.24% | Test Acc: 94.82% | Loss: 0.0478\n",
      "Epoch [1712/2000] | Train Acc: 98.23% | Test Acc: 94.84% | Loss: 0.0479\n",
      "Epoch [1713/2000] | Train Acc: 98.23% | Test Acc: 94.66% | Loss: 0.0482\n",
      "Epoch [1714/2000] | Train Acc: 98.19% | Test Acc: 94.90% | Loss: 0.0483\n",
      "Epoch [1715/2000] | Train Acc: 98.24% | Test Acc: 94.60% | Loss: 0.0487\n",
      "Epoch [1716/2000] | Train Acc: 98.18% | Test Acc: 94.90% | Loss: 0.0487\n",
      "Epoch [1717/2000] | Train Acc: 98.23% | Test Acc: 94.59% | Loss: 0.0490\n",
      "Epoch [1718/2000] | Train Acc: 98.18% | Test Acc: 94.82% | Loss: 0.0486\n",
      "Epoch [1719/2000] | Train Acc: 98.23% | Test Acc: 94.87% | Loss: 0.0483\n",
      "Epoch [1720/2000] | Train Acc: 98.22% | Test Acc: 94.79% | Loss: 0.0478\n",
      "Epoch [1721/2000] | Train Acc: 98.25% | Test Acc: 94.87% | Loss: 0.0474\n",
      "Epoch [1722/2000] | Train Acc: 98.26% | Test Acc: 94.85% | Loss: 0.0472\n",
      "Epoch [1723/2000] | Train Acc: 98.26% | Test Acc: 94.79% | Loss: 0.0472\n",
      "Epoch [1724/2000] | Train Acc: 98.25% | Test Acc: 94.88% | Loss: 0.0473\n",
      "Epoch [1725/2000] | Train Acc: 98.23% | Test Acc: 94.78% | Loss: 0.0475\n",
      "Epoch [1726/2000] | Train Acc: 98.24% | Test Acc: 94.77% | Loss: 0.0477\n",
      "Epoch [1727/2000] | Train Acc: 98.23% | Test Acc: 94.82% | Loss: 0.0478\n",
      "Epoch [1728/2000] | Train Acc: 98.24% | Test Acc: 94.82% | Loss: 0.0479\n",
      "Epoch [1729/2000] | Train Acc: 98.23% | Test Acc: 94.79% | Loss: 0.0477\n",
      "Epoch [1730/2000] | Train Acc: 98.24% | Test Acc: 94.90% | Loss: 0.0476\n",
      "Epoch [1731/2000] | Train Acc: 98.26% | Test Acc: 94.84% | Loss: 0.0473\n",
      "Epoch [1732/2000] | Train Acc: 98.27% | Test Acc: 94.90% | Loss: 0.0472\n",
      "Epoch [1733/2000] | Train Acc: 98.26% | Test Acc: 94.87% | Loss: 0.0471\n",
      "Epoch [1734/2000] | Train Acc: 98.26% | Test Acc: 94.85% | Loss: 0.0470\n",
      "Epoch [1735/2000] | Train Acc: 98.26% | Test Acc: 94.87% | Loss: 0.0471\n",
      "Epoch [1736/2000] | Train Acc: 98.26% | Test Acc: 94.75% | Loss: 0.0472\n",
      "Epoch [1737/2000] | Train Acc: 98.25% | Test Acc: 94.91% | Loss: 0.0472\n",
      "Epoch [1738/2000] | Train Acc: 98.26% | Test Acc: 94.77% | Loss: 0.0473\n",
      "Epoch [1739/2000] | Train Acc: 98.25% | Test Acc: 94.90% | Loss: 0.0473\n",
      "Epoch [1740/2000] | Train Acc: 98.26% | Test Acc: 94.77% | Loss: 0.0473\n",
      "Epoch [1741/2000] | Train Acc: 98.25% | Test Acc: 94.91% | Loss: 0.0472\n",
      "Epoch [1742/2000] | Train Acc: 98.26% | Test Acc: 94.81% | Loss: 0.0471\n",
      "Epoch [1743/2000] | Train Acc: 98.26% | Test Acc: 94.84% | Loss: 0.0470\n",
      "Epoch [1744/2000] | Train Acc: 98.27% | Test Acc: 94.90% | Loss: 0.0470\n",
      "Epoch [1745/2000] | Train Acc: 98.28% | Test Acc: 94.91% | Loss: 0.0469\n",
      "Epoch [1746/2000] | Train Acc: 98.29% | Test Acc: 94.90% | Loss: 0.0469\n",
      "Epoch [1747/2000] | Train Acc: 98.28% | Test Acc: 94.85% | Loss: 0.0469\n",
      "Epoch [1748/2000] | Train Acc: 98.27% | Test Acc: 94.85% | Loss: 0.0469\n",
      "Epoch [1749/2000] | Train Acc: 98.28% | Test Acc: 94.81% | Loss: 0.0469\n",
      "Epoch [1750/2000] | Train Acc: 98.27% | Test Acc: 94.85% | Loss: 0.0469\n",
      "Epoch [1751/2000] | Train Acc: 98.26% | Test Acc: 94.81% | Loss: 0.0469\n",
      "Epoch [1752/2000] | Train Acc: 98.26% | Test Acc: 94.88% | Loss: 0.0470\n",
      "Epoch [1753/2000] | Train Acc: 98.27% | Test Acc: 94.79% | Loss: 0.0470\n",
      "Epoch [1754/2000] | Train Acc: 98.27% | Test Acc: 94.90% | Loss: 0.0470\n",
      "Epoch [1755/2000] | Train Acc: 98.27% | Test Acc: 94.78% | Loss: 0.0470\n",
      "Epoch [1756/2000] | Train Acc: 98.26% | Test Acc: 94.90% | Loss: 0.0470\n",
      "Epoch [1757/2000] | Train Acc: 98.27% | Test Acc: 94.74% | Loss: 0.0469\n",
      "Epoch [1758/2000] | Train Acc: 98.26% | Test Acc: 94.88% | Loss: 0.0469\n",
      "Epoch [1759/2000] | Train Acc: 98.27% | Test Acc: 94.74% | Loss: 0.0469\n",
      "Epoch [1760/2000] | Train Acc: 98.26% | Test Acc: 94.90% | Loss: 0.0469\n",
      "Epoch [1761/2000] | Train Acc: 98.27% | Test Acc: 94.74% | Loss: 0.0469\n",
      "Epoch [1762/2000] | Train Acc: 98.26% | Test Acc: 94.90% | Loss: 0.0469\n",
      "Epoch [1763/2000] | Train Acc: 98.26% | Test Acc: 94.77% | Loss: 0.0469\n",
      "Epoch [1764/2000] | Train Acc: 98.26% | Test Acc: 94.90% | Loss: 0.0469\n",
      "Epoch [1765/2000] | Train Acc: 98.27% | Test Acc: 94.77% | Loss: 0.0468\n",
      "Epoch [1766/2000] | Train Acc: 98.27% | Test Acc: 94.88% | Loss: 0.0468\n",
      "Epoch [1767/2000] | Train Acc: 98.27% | Test Acc: 94.75% | Loss: 0.0468\n",
      "Epoch [1768/2000] | Train Acc: 98.26% | Test Acc: 94.87% | Loss: 0.0468\n",
      "Epoch [1769/2000] | Train Acc: 98.27% | Test Acc: 94.72% | Loss: 0.0468\n",
      "Epoch [1770/2000] | Train Acc: 98.27% | Test Acc: 94.84% | Loss: 0.0468\n",
      "Epoch [1771/2000] | Train Acc: 98.28% | Test Acc: 94.75% | Loss: 0.0468\n",
      "Epoch [1772/2000] | Train Acc: 98.27% | Test Acc: 94.88% | Loss: 0.0468\n",
      "Epoch [1773/2000] | Train Acc: 98.29% | Test Acc: 94.75% | Loss: 0.0468\n",
      "Epoch [1774/2000] | Train Acc: 98.27% | Test Acc: 94.85% | Loss: 0.0469\n",
      "Epoch [1775/2000] | Train Acc: 98.28% | Test Acc: 94.78% | Loss: 0.0469\n",
      "Epoch [1776/2000] | Train Acc: 98.26% | Test Acc: 94.84% | Loss: 0.0470\n",
      "Epoch [1777/2000] | Train Acc: 98.25% | Test Acc: 94.81% | Loss: 0.0470\n",
      "Epoch [1778/2000] | Train Acc: 98.26% | Test Acc: 94.84% | Loss: 0.0470\n",
      "Epoch [1779/2000] | Train Acc: 98.25% | Test Acc: 94.79% | Loss: 0.0470\n",
      "Epoch [1780/2000] | Train Acc: 98.25% | Test Acc: 94.81% | Loss: 0.0471\n",
      "Epoch [1781/2000] | Train Acc: 98.25% | Test Acc: 94.79% | Loss: 0.0471\n",
      "Epoch [1782/2000] | Train Acc: 98.26% | Test Acc: 94.85% | Loss: 0.0471\n",
      "Epoch [1783/2000] | Train Acc: 98.25% | Test Acc: 94.77% | Loss: 0.0470\n",
      "Epoch [1784/2000] | Train Acc: 98.26% | Test Acc: 94.82% | Loss: 0.0469\n",
      "Epoch [1785/2000] | Train Acc: 98.26% | Test Acc: 94.75% | Loss: 0.0468\n",
      "Epoch [1786/2000] | Train Acc: 98.26% | Test Acc: 94.90% | Loss: 0.0467\n",
      "Epoch [1787/2000] | Train Acc: 98.28% | Test Acc: 94.81% | Loss: 0.0466\n",
      "Epoch [1788/2000] | Train Acc: 98.28% | Test Acc: 94.85% | Loss: 0.0465\n",
      "Epoch [1789/2000] | Train Acc: 98.29% | Test Acc: 94.82% | Loss: 0.0464\n",
      "Epoch [1790/2000] | Train Acc: 98.28% | Test Acc: 94.82% | Loss: 0.0464\n",
      "Epoch [1791/2000] | Train Acc: 98.29% | Test Acc: 94.85% | Loss: 0.0464\n",
      "Epoch [1792/2000] | Train Acc: 98.30% | Test Acc: 94.84% | Loss: 0.0463\n",
      "Epoch [1793/2000] | Train Acc: 98.30% | Test Acc: 94.84% | Loss: 0.0463\n",
      "Epoch [1794/2000] | Train Acc: 98.30% | Test Acc: 94.85% | Loss: 0.0463\n",
      "Epoch [1795/2000] | Train Acc: 98.29% | Test Acc: 94.81% | Loss: 0.0463\n",
      "Epoch [1796/2000] | Train Acc: 98.29% | Test Acc: 94.84% | Loss: 0.0463\n",
      "Epoch [1797/2000] | Train Acc: 98.28% | Test Acc: 94.87% | Loss: 0.0463\n",
      "Epoch [1798/2000] | Train Acc: 98.30% | Test Acc: 94.82% | Loss: 0.0464\n",
      "Epoch [1799/2000] | Train Acc: 98.29% | Test Acc: 94.88% | Loss: 0.0464\n",
      "Epoch [1800/2000] | Train Acc: 98.29% | Test Acc: 94.79% | Loss: 0.0464\n",
      "Epoch [1801/2000] | Train Acc: 98.27% | Test Acc: 94.87% | Loss: 0.0465\n",
      "Epoch [1802/2000] | Train Acc: 98.30% | Test Acc: 94.78% | Loss: 0.0465\n",
      "Epoch [1803/2000] | Train Acc: 98.27% | Test Acc: 94.82% | Loss: 0.0466\n",
      "Epoch [1804/2000] | Train Acc: 98.27% | Test Acc: 94.82% | Loss: 0.0467\n",
      "Epoch [1805/2000] | Train Acc: 98.27% | Test Acc: 94.75% | Loss: 0.0469\n",
      "Epoch [1806/2000] | Train Acc: 98.24% | Test Acc: 94.87% | Loss: 0.0469\n",
      "Epoch [1807/2000] | Train Acc: 98.26% | Test Acc: 94.69% | Loss: 0.0471\n",
      "Epoch [1808/2000] | Train Acc: 98.23% | Test Acc: 94.88% | Loss: 0.0471\n",
      "Epoch [1809/2000] | Train Acc: 98.25% | Test Acc: 94.69% | Loss: 0.0472\n",
      "Epoch [1810/2000] | Train Acc: 98.23% | Test Acc: 94.82% | Loss: 0.0470\n",
      "Epoch [1811/2000] | Train Acc: 98.27% | Test Acc: 94.82% | Loss: 0.0469\n",
      "Epoch [1812/2000] | Train Acc: 98.27% | Test Acc: 94.77% | Loss: 0.0466\n",
      "Epoch [1813/2000] | Train Acc: 98.27% | Test Acc: 94.81% | Loss: 0.0464\n",
      "Epoch [1814/2000] | Train Acc: 98.29% | Test Acc: 94.84% | Loss: 0.0462\n",
      "Epoch [1815/2000] | Train Acc: 98.29% | Test Acc: 94.87% | Loss: 0.0461\n",
      "Epoch [1816/2000] | Train Acc: 98.32% | Test Acc: 94.82% | Loss: 0.0460\n",
      "Epoch [1817/2000] | Train Acc: 98.30% | Test Acc: 94.81% | Loss: 0.0461\n",
      "Epoch [1818/2000] | Train Acc: 98.28% | Test Acc: 94.84% | Loss: 0.0461\n",
      "Epoch [1819/2000] | Train Acc: 98.32% | Test Acc: 94.77% | Loss: 0.0462\n",
      "Epoch [1820/2000] | Train Acc: 98.28% | Test Acc: 94.90% | Loss: 0.0462\n",
      "Epoch [1821/2000] | Train Acc: 98.31% | Test Acc: 94.78% | Loss: 0.0463\n",
      "Epoch [1822/2000] | Train Acc: 98.27% | Test Acc: 94.82% | Loss: 0.0464\n",
      "Epoch [1823/2000] | Train Acc: 98.28% | Test Acc: 94.82% | Loss: 0.0464\n",
      "Epoch [1824/2000] | Train Acc: 98.27% | Test Acc: 94.82% | Loss: 0.0465\n",
      "Epoch [1825/2000] | Train Acc: 98.28% | Test Acc: 94.82% | Loss: 0.0464\n",
      "Epoch [1826/2000] | Train Acc: 98.27% | Test Acc: 94.84% | Loss: 0.0464\n",
      "Epoch [1827/2000] | Train Acc: 98.29% | Test Acc: 94.78% | Loss: 0.0463\n",
      "Epoch [1828/2000] | Train Acc: 98.28% | Test Acc: 94.91% | Loss: 0.0462\n",
      "Epoch [1829/2000] | Train Acc: 98.30% | Test Acc: 94.81% | Loss: 0.0461\n",
      "Epoch [1830/2000] | Train Acc: 98.29% | Test Acc: 94.84% | Loss: 0.0460\n",
      "Epoch [1831/2000] | Train Acc: 98.30% | Test Acc: 94.85% | Loss: 0.0460\n",
      "Epoch [1832/2000] | Train Acc: 98.30% | Test Acc: 94.82% | Loss: 0.0459\n",
      "Epoch [1833/2000] | Train Acc: 98.31% | Test Acc: 94.84% | Loss: 0.0459\n",
      "Epoch [1834/2000] | Train Acc: 98.32% | Test Acc: 94.84% | Loss: 0.0458\n",
      "Epoch [1835/2000] | Train Acc: 98.32% | Test Acc: 94.82% | Loss: 0.0458\n",
      "Epoch [1836/2000] | Train Acc: 98.31% | Test Acc: 94.87% | Loss: 0.0458\n",
      "Epoch [1837/2000] | Train Acc: 98.30% | Test Acc: 94.84% | Loss: 0.0458\n",
      "Epoch [1838/2000] | Train Acc: 98.30% | Test Acc: 94.82% | Loss: 0.0458\n",
      "Epoch [1839/2000] | Train Acc: 98.30% | Test Acc: 94.82% | Loss: 0.0459\n",
      "Epoch [1840/2000] | Train Acc: 98.31% | Test Acc: 94.81% | Loss: 0.0459\n",
      "Epoch [1841/2000] | Train Acc: 98.29% | Test Acc: 94.85% | Loss: 0.0459\n",
      "Epoch [1842/2000] | Train Acc: 98.32% | Test Acc: 94.78% | Loss: 0.0459\n",
      "Epoch [1843/2000] | Train Acc: 98.29% | Test Acc: 94.85% | Loss: 0.0460\n",
      "Epoch [1844/2000] | Train Acc: 98.33% | Test Acc: 94.79% | Loss: 0.0460\n",
      "Epoch [1845/2000] | Train Acc: 98.28% | Test Acc: 94.78% | Loss: 0.0461\n",
      "Epoch [1846/2000] | Train Acc: 98.28% | Test Acc: 94.79% | Loss: 0.0462\n",
      "Epoch [1847/2000] | Train Acc: 98.28% | Test Acc: 94.78% | Loss: 0.0463\n",
      "Epoch [1848/2000] | Train Acc: 98.27% | Test Acc: 94.85% | Loss: 0.0463\n",
      "Epoch [1849/2000] | Train Acc: 98.27% | Test Acc: 94.74% | Loss: 0.0465\n",
      "Epoch [1850/2000] | Train Acc: 98.26% | Test Acc: 94.87% | Loss: 0.0465\n",
      "Epoch [1851/2000] | Train Acc: 98.27% | Test Acc: 94.75% | Loss: 0.0465\n",
      "Epoch [1852/2000] | Train Acc: 98.26% | Test Acc: 94.84% | Loss: 0.0464\n",
      "Epoch [1853/2000] | Train Acc: 98.28% | Test Acc: 94.79% | Loss: 0.0463\n",
      "Epoch [1854/2000] | Train Acc: 98.27% | Test Acc: 94.78% | Loss: 0.0461\n",
      "Epoch [1855/2000] | Train Acc: 98.28% | Test Acc: 94.85% | Loss: 0.0460\n",
      "Epoch [1856/2000] | Train Acc: 98.32% | Test Acc: 94.77% | Loss: 0.0458\n",
      "Epoch [1857/2000] | Train Acc: 98.29% | Test Acc: 94.82% | Loss: 0.0457\n",
      "Epoch [1858/2000] | Train Acc: 98.30% | Test Acc: 94.82% | Loss: 0.0456\n",
      "Epoch [1859/2000] | Train Acc: 98.33% | Test Acc: 94.84% | Loss: 0.0456\n",
      "Epoch [1860/2000] | Train Acc: 98.31% | Test Acc: 94.85% | Loss: 0.0456\n",
      "Epoch [1861/2000] | Train Acc: 98.31% | Test Acc: 94.79% | Loss: 0.0456\n",
      "Epoch [1862/2000] | Train Acc: 98.30% | Test Acc: 94.84% | Loss: 0.0456\n",
      "Epoch [1863/2000] | Train Acc: 98.33% | Test Acc: 94.78% | Loss: 0.0457\n",
      "Epoch [1864/2000] | Train Acc: 98.29% | Test Acc: 94.84% | Loss: 0.0457\n",
      "Epoch [1865/2000] | Train Acc: 98.33% | Test Acc: 94.79% | Loss: 0.0458\n",
      "Epoch [1866/2000] | Train Acc: 98.29% | Test Acc: 94.81% | Loss: 0.0459\n",
      "Epoch [1867/2000] | Train Acc: 98.30% | Test Acc: 94.79% | Loss: 0.0459\n",
      "Epoch [1868/2000] | Train Acc: 98.29% | Test Acc: 94.79% | Loss: 0.0460\n",
      "Epoch [1869/2000] | Train Acc: 98.28% | Test Acc: 94.78% | Loss: 0.0459\n",
      "Epoch [1870/2000] | Train Acc: 98.28% | Test Acc: 94.77% | Loss: 0.0460\n",
      "Epoch [1871/2000] | Train Acc: 98.28% | Test Acc: 94.78% | Loss: 0.0459\n",
      "Epoch [1872/2000] | Train Acc: 98.29% | Test Acc: 94.82% | Loss: 0.0459\n",
      "Epoch [1873/2000] | Train Acc: 98.30% | Test Acc: 94.77% | Loss: 0.0458\n",
      "Epoch [1874/2000] | Train Acc: 98.29% | Test Acc: 94.82% | Loss: 0.0457\n",
      "Epoch [1875/2000] | Train Acc: 98.32% | Test Acc: 94.79% | Loss: 0.0456\n",
      "Epoch [1876/2000] | Train Acc: 98.30% | Test Acc: 94.82% | Loss: 0.0455\n",
      "Epoch [1877/2000] | Train Acc: 98.32% | Test Acc: 94.79% | Loss: 0.0455\n",
      "Epoch [1878/2000] | Train Acc: 98.30% | Test Acc: 94.81% | Loss: 0.0454\n",
      "Epoch [1879/2000] | Train Acc: 98.32% | Test Acc: 94.81% | Loss: 0.0454\n",
      "Epoch [1880/2000] | Train Acc: 98.33% | Test Acc: 94.79% | Loss: 0.0453\n",
      "Epoch [1881/2000] | Train Acc: 98.33% | Test Acc: 94.79% | Loss: 0.0453\n",
      "Epoch [1882/2000] | Train Acc: 98.33% | Test Acc: 94.79% | Loss: 0.0453\n",
      "Epoch [1883/2000] | Train Acc: 98.32% | Test Acc: 94.78% | Loss: 0.0453\n",
      "Epoch [1884/2000] | Train Acc: 98.33% | Test Acc: 94.82% | Loss: 0.0453\n",
      "Epoch [1885/2000] | Train Acc: 98.31% | Test Acc: 94.81% | Loss: 0.0453\n",
      "Epoch [1886/2000] | Train Acc: 98.31% | Test Acc: 94.81% | Loss: 0.0453\n",
      "Epoch [1887/2000] | Train Acc: 98.30% | Test Acc: 94.82% | Loss: 0.0453\n",
      "Epoch [1888/2000] | Train Acc: 98.33% | Test Acc: 94.79% | Loss: 0.0453\n",
      "Epoch [1889/2000] | Train Acc: 98.31% | Test Acc: 94.87% | Loss: 0.0454\n",
      "Epoch [1890/2000] | Train Acc: 98.32% | Test Acc: 94.78% | Loss: 0.0454\n",
      "Epoch [1891/2000] | Train Acc: 98.30% | Test Acc: 94.82% | Loss: 0.0455\n",
      "Epoch [1892/2000] | Train Acc: 98.32% | Test Acc: 94.81% | Loss: 0.0455\n",
      "Epoch [1893/2000] | Train Acc: 98.29% | Test Acc: 94.72% | Loss: 0.0457\n",
      "Epoch [1894/2000] | Train Acc: 98.28% | Test Acc: 94.87% | Loss: 0.0458\n",
      "Epoch [1895/2000] | Train Acc: 98.29% | Test Acc: 94.66% | Loss: 0.0460\n",
      "Epoch [1896/2000] | Train Acc: 98.26% | Test Acc: 94.90% | Loss: 0.0461\n",
      "Epoch [1897/2000] | Train Acc: 98.28% | Test Acc: 94.56% | Loss: 0.0463\n",
      "Epoch [1898/2000] | Train Acc: 98.24% | Test Acc: 94.88% | Loss: 0.0463\n",
      "Epoch [1899/2000] | Train Acc: 98.27% | Test Acc: 94.63% | Loss: 0.0464\n",
      "Epoch [1900/2000] | Train Acc: 98.26% | Test Acc: 94.85% | Loss: 0.0461\n",
      "Epoch [1901/2000] | Train Acc: 98.29% | Test Acc: 94.78% | Loss: 0.0459\n",
      "Epoch [1902/2000] | Train Acc: 98.29% | Test Acc: 94.79% | Loss: 0.0455\n",
      "Epoch [1903/2000] | Train Acc: 98.30% | Test Acc: 94.85% | Loss: 0.0453\n",
      "Epoch [1904/2000] | Train Acc: 98.32% | Test Acc: 94.79% | Loss: 0.0451\n",
      "Epoch [1905/2000] | Train Acc: 98.35% | Test Acc: 94.85% | Loss: 0.0451\n",
      "Epoch [1906/2000] | Train Acc: 98.31% | Test Acc: 94.84% | Loss: 0.0451\n",
      "Epoch [1907/2000] | Train Acc: 98.33% | Test Acc: 94.79% | Loss: 0.0451\n",
      "Epoch [1908/2000] | Train Acc: 98.30% | Test Acc: 94.82% | Loss: 0.0452\n",
      "Epoch [1909/2000] | Train Acc: 98.34% | Test Acc: 94.82% | Loss: 0.0453\n",
      "Epoch [1910/2000] | Train Acc: 98.30% | Test Acc: 94.79% | Loss: 0.0455\n",
      "Epoch [1911/2000] | Train Acc: 98.30% | Test Acc: 94.84% | Loss: 0.0455\n",
      "Epoch [1912/2000] | Train Acc: 98.30% | Test Acc: 94.79% | Loss: 0.0456\n",
      "Epoch [1913/2000] | Train Acc: 98.30% | Test Acc: 94.81% | Loss: 0.0455\n",
      "Epoch [1914/2000] | Train Acc: 98.30% | Test Acc: 94.81% | Loss: 0.0455\n",
      "Epoch [1915/2000] | Train Acc: 98.32% | Test Acc: 94.82% | Loss: 0.0453\n",
      "Epoch [1916/2000] | Train Acc: 98.31% | Test Acc: 94.90% | Loss: 0.0452\n",
      "Epoch [1917/2000] | Train Acc: 98.34% | Test Acc: 94.81% | Loss: 0.0451\n",
      "Epoch [1918/2000] | Train Acc: 98.31% | Test Acc: 94.82% | Loss: 0.0450\n",
      "Epoch [1919/2000] | Train Acc: 98.32% | Test Acc: 94.81% | Loss: 0.0449\n",
      "Epoch [1920/2000] | Train Acc: 98.33% | Test Acc: 94.78% | Loss: 0.0449\n",
      "Epoch [1921/2000] | Train Acc: 98.34% | Test Acc: 94.81% | Loss: 0.0449\n",
      "Epoch [1922/2000] | Train Acc: 98.33% | Test Acc: 94.87% | Loss: 0.0449\n",
      "Epoch [1923/2000] | Train Acc: 98.31% | Test Acc: 94.82% | Loss: 0.0449\n",
      "Epoch [1924/2000] | Train Acc: 98.33% | Test Acc: 94.82% | Loss: 0.0449\n",
      "Epoch [1925/2000] | Train Acc: 98.31% | Test Acc: 94.91% | Loss: 0.0450\n",
      "Epoch [1926/2000] | Train Acc: 98.35% | Test Acc: 94.85% | Loss: 0.0450\n",
      "Epoch [1927/2000] | Train Acc: 98.32% | Test Acc: 94.90% | Loss: 0.0450\n",
      "Epoch [1928/2000] | Train Acc: 98.34% | Test Acc: 94.82% | Loss: 0.0450\n",
      "Epoch [1929/2000] | Train Acc: 98.32% | Test Acc: 94.88% | Loss: 0.0451\n",
      "Epoch [1930/2000] | Train Acc: 98.33% | Test Acc: 94.84% | Loss: 0.0451\n",
      "Epoch [1931/2000] | Train Acc: 98.31% | Test Acc: 94.85% | Loss: 0.0451\n",
      "Epoch [1932/2000] | Train Acc: 98.33% | Test Acc: 94.84% | Loss: 0.0451\n",
      "Epoch [1933/2000] | Train Acc: 98.31% | Test Acc: 94.81% | Loss: 0.0452\n",
      "Epoch [1934/2000] | Train Acc: 98.32% | Test Acc: 94.84% | Loss: 0.0451\n",
      "Epoch [1935/2000] | Train Acc: 98.30% | Test Acc: 94.82% | Loss: 0.0452\n",
      "Epoch [1936/2000] | Train Acc: 98.32% | Test Acc: 94.84% | Loss: 0.0451\n",
      "Epoch [1937/2000] | Train Acc: 98.31% | Test Acc: 94.84% | Loss: 0.0452\n",
      "Epoch [1938/2000] | Train Acc: 98.32% | Test Acc: 94.84% | Loss: 0.0451\n",
      "Epoch [1939/2000] | Train Acc: 98.32% | Test Acc: 94.84% | Loss: 0.0451\n",
      "Epoch [1940/2000] | Train Acc: 98.33% | Test Acc: 94.82% | Loss: 0.0450\n",
      "Epoch [1941/2000] | Train Acc: 98.32% | Test Acc: 94.90% | Loss: 0.0450\n",
      "Epoch [1942/2000] | Train Acc: 98.34% | Test Acc: 94.84% | Loss: 0.0449\n",
      "Epoch [1943/2000] | Train Acc: 98.31% | Test Acc: 94.90% | Loss: 0.0449\n",
      "Epoch [1944/2000] | Train Acc: 98.34% | Test Acc: 94.82% | Loss: 0.0448\n",
      "Epoch [1945/2000] | Train Acc: 98.31% | Test Acc: 94.84% | Loss: 0.0448\n",
      "Epoch [1946/2000] | Train Acc: 98.35% | Test Acc: 94.85% | Loss: 0.0447\n",
      "Epoch [1947/2000] | Train Acc: 98.31% | Test Acc: 94.85% | Loss: 0.0447\n",
      "Epoch [1948/2000] | Train Acc: 98.33% | Test Acc: 94.87% | Loss: 0.0447\n",
      "Epoch [1949/2000] | Train Acc: 98.32% | Test Acc: 94.87% | Loss: 0.0446\n",
      "Epoch [1950/2000] | Train Acc: 98.34% | Test Acc: 94.85% | Loss: 0.0446\n",
      "Epoch [1951/2000] | Train Acc: 98.33% | Test Acc: 94.87% | Loss: 0.0446\n",
      "Epoch [1952/2000] | Train Acc: 98.35% | Test Acc: 94.85% | Loss: 0.0446\n",
      "Epoch [1953/2000] | Train Acc: 98.33% | Test Acc: 94.87% | Loss: 0.0446\n",
      "Epoch [1954/2000] | Train Acc: 98.35% | Test Acc: 94.87% | Loss: 0.0446\n",
      "Epoch [1955/2000] | Train Acc: 98.32% | Test Acc: 94.85% | Loss: 0.0446\n",
      "Epoch [1956/2000] | Train Acc: 98.35% | Test Acc: 94.87% | Loss: 0.0446\n",
      "Epoch [1957/2000] | Train Acc: 98.33% | Test Acc: 94.84% | Loss: 0.0446\n",
      "Epoch [1958/2000] | Train Acc: 98.35% | Test Acc: 94.84% | Loss: 0.0446\n",
      "Epoch [1959/2000] | Train Acc: 98.31% | Test Acc: 94.88% | Loss: 0.0446\n",
      "Epoch [1960/2000] | Train Acc: 98.34% | Test Acc: 94.85% | Loss: 0.0446\n",
      "Epoch [1961/2000] | Train Acc: 98.33% | Test Acc: 94.85% | Loss: 0.0447\n",
      "Epoch [1962/2000] | Train Acc: 98.34% | Test Acc: 94.84% | Loss: 0.0448\n",
      "Epoch [1963/2000] | Train Acc: 98.32% | Test Acc: 94.75% | Loss: 0.0449\n",
      "Epoch [1964/2000] | Train Acc: 98.33% | Test Acc: 94.88% | Loss: 0.0451\n",
      "Epoch [1965/2000] | Train Acc: 98.31% | Test Acc: 94.60% | Loss: 0.0454\n",
      "Epoch [1966/2000] | Train Acc: 98.27% | Test Acc: 94.87% | Loss: 0.0456\n",
      "Epoch [1967/2000] | Train Acc: 98.30% | Test Acc: 94.56% | Loss: 0.0461\n",
      "Epoch [1968/2000] | Train Acc: 98.24% | Test Acc: 94.93% | Loss: 0.0461\n",
      "Epoch [1969/2000] | Train Acc: 98.29% | Test Acc: 94.54% | Loss: 0.0464\n",
      "Epoch [1970/2000] | Train Acc: 98.24% | Test Acc: 94.90% | Loss: 0.0459\n",
      "Epoch [1971/2000] | Train Acc: 98.30% | Test Acc: 94.75% | Loss: 0.0456\n",
      "Epoch [1972/2000] | Train Acc: 98.31% | Test Acc: 94.82% | Loss: 0.0449\n",
      "Epoch [1973/2000] | Train Acc: 98.32% | Test Acc: 94.82% | Loss: 0.0445\n",
      "Epoch [1974/2000] | Train Acc: 98.37% | Test Acc: 94.82% | Loss: 0.0444\n",
      "Epoch [1975/2000] | Train Acc: 98.34% | Test Acc: 94.84% | Loss: 0.0444\n",
      "Epoch [1976/2000] | Train Acc: 98.32% | Test Acc: 94.77% | Loss: 0.0446\n",
      "Epoch [1977/2000] | Train Acc: 98.33% | Test Acc: 94.87% | Loss: 0.0448\n",
      "Epoch [1978/2000] | Train Acc: 98.32% | Test Acc: 94.74% | Loss: 0.0451\n",
      "Epoch [1979/2000] | Train Acc: 98.29% | Test Acc: 94.84% | Loss: 0.0451\n",
      "Epoch [1980/2000] | Train Acc: 98.31% | Test Acc: 94.75% | Loss: 0.0451\n",
      "Epoch [1981/2000] | Train Acc: 98.32% | Test Acc: 94.88% | Loss: 0.0449\n",
      "Epoch [1982/2000] | Train Acc: 98.33% | Test Acc: 94.93% | Loss: 0.0447\n",
      "Epoch [1983/2000] | Train Acc: 98.34% | Test Acc: 94.87% | Loss: 0.0444\n",
      "Epoch [1984/2000] | Train Acc: 98.32% | Test Acc: 94.84% | Loss: 0.0443\n",
      "Epoch [1985/2000] | Train Acc: 98.36% | Test Acc: 94.87% | Loss: 0.0443\n",
      "Epoch [1986/2000] | Train Acc: 98.35% | Test Acc: 94.85% | Loss: 0.0443\n",
      "Epoch [1987/2000] | Train Acc: 98.33% | Test Acc: 94.90% | Loss: 0.0444\n",
      "Epoch [1988/2000] | Train Acc: 98.35% | Test Acc: 94.87% | Loss: 0.0445\n",
      "Epoch [1989/2000] | Train Acc: 98.33% | Test Acc: 94.88% | Loss: 0.0446\n",
      "Epoch [1990/2000] | Train Acc: 98.34% | Test Acc: 94.87% | Loss: 0.0446\n",
      "Epoch [1991/2000] | Train Acc: 98.33% | Test Acc: 94.91% | Loss: 0.0446\n",
      "Epoch [1992/2000] | Train Acc: 98.34% | Test Acc: 94.90% | Loss: 0.0445\n",
      "Epoch [1993/2000] | Train Acc: 98.33% | Test Acc: 94.87% | Loss: 0.0444\n",
      "Epoch [1994/2000] | Train Acc: 98.35% | Test Acc: 94.85% | Loss: 0.0443\n",
      "Epoch [1995/2000] | Train Acc: 98.33% | Test Acc: 94.84% | Loss: 0.0442\n",
      "Epoch [1996/2000] | Train Acc: 98.36% | Test Acc: 94.85% | Loss: 0.0441\n",
      "Epoch [1997/2000] | Train Acc: 98.36% | Test Acc: 94.85% | Loss: 0.0441\n",
      "Epoch [1998/2000] | Train Acc: 98.33% | Test Acc: 94.85% | Loss: 0.0441\n",
      "Epoch [1999/2000] | Train Acc: 98.36% | Test Acc: 94.88% | Loss: 0.0442\n",
      "Epoch [2000/2000] | Train Acc: 98.34% | Test Acc: 94.88% | Loss: 0.0442\n",
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1200\n",
      "           1       0.80      0.87      0.83       724\n",
      "           2       0.97      0.95      0.96       800\n",
      "           3       0.99      0.98      0.98       800\n",
      "           4       0.99      0.99      0.99       800\n",
      "           5       0.99      0.98      0.99       800\n",
      "           6       0.85      0.81      0.83       800\n",
      "           7       0.99      0.99      0.99       800\n",
      "\n",
      "    accuracy                           0.95      6724\n",
      "   macro avg       0.95      0.95      0.95      6724\n",
      "weighted avg       0.95      0.95      0.95      6724\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Thiết bị: GPU nếu có ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --- Mô hình BiRNN ---\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)         # out shape: (batch, seq_len, hidden*2)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# === Chọn 1 diffusion model cụ thể ===\n",
    "model_type = 'DiffAutoencoder'\n",
    "X_train_aug, y_train_aug, _, _ = generate_samples_with_diffusion(X_train_scaled, y_train, model_type)\n",
    "\n",
    "# === Feature selection ===\n",
    "X_train_selected, X_test_selected, selected_columns = feature_selection(X_train_aug, X_test_scaled, y_train_aug)\n",
    "\n",
    "# === Chuyển sang tensor và đưa lên device ===\n",
    "X_train_tensor = torch.tensor(X_train_selected, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_selected, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_aug, dtype=torch.long).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# === Khởi tạo model ===\n",
    "input_size = X_train_selected.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = len(np.unique(y_train_aug))\n",
    "model = BiLSTM(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# === Huấn luyện ===\n",
    "num_epochs = 2000\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    _, predicted_train = torch.max(outputs, 1)\n",
    "    train_acc = (predicted_train == y_train_tensor).float().mean().item()\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # Đánh giá test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model(X_test_tensor)\n",
    "        _, predicted_test = torch.max(outputs_test, 1)\n",
    "        test_acc = (predicted_test == y_test_tensor).float().mean().item()\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Acc: {train_acc*100:.2f}% | Test Acc: {test_acc*100:.2f}% | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# === Evaluation trên test set ===\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(y_test_tensor.cpu().numpy(), predicted_test.cpu().numpy(), zero_division=0))\n",
    "\n",
    "# === Plot Loss & Accuracy ===\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# --- Loss plot ---\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Train Loss', color='red')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# --- Accuracy plot ---\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, label='Train Accuracy', color='blue')\n",
    "plt.plot(epochs, test_accuracies, label='Test Accuracy', color='green')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8aafae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of samples before augmentation: 26894\n",
      "\n",
      "   ➤ Class 0: 4800 samples before augmentation\n",
      "   ➤ Class 6: 3200 samples before augmentation\n",
      "   ➤ Class 4: 3200 samples before augmentation\n",
      "   ➤ Class 1: 2894 samples before augmentation\n",
      "   ➤ Class 5: 3200 samples before augmentation\n",
      "   ➤ Class 3: 3200 samples before augmentation\n",
      "   ➤ Class 2: 3200 samples before augmentation\n",
      "   ➤ Class 7: 3200 samples before augmentation\n",
      "Number of samples after augmentation: 38400\n",
      "  Class 0: 4800 samples after augmentation\n",
      "  Class 6: 4800 samples after augmentation\n",
      "  Class 4: 4800 samples after augmentation\n",
      "  Class 1: 4800 samples after augmentation\n",
      "  Class 5: 4800 samples after augmentation\n",
      "  Class 3: 4800 samples after augmentation\n",
      "  Class 2: 4800 samples after augmentation\n",
      "  Class 7: 4800 samples after augmentation\n",
      "Selected features (columns): [0, 1, 3, 4, 5, 11, 12, 13, 21, 23, 24, 26, 28, 29, 43, 45, 48, 50, 57, 58, 70, 73, 78, 80, 82, 83, 84, 85, 86, 87]\n",
      "Epoch [1/2000] | Train Acc: 10.16% | Test Acc: 48.38% | Loss: 2.0782\n",
      "Epoch [2/2000] | Train Acc: 37.44% | Test Acc: 51.46% | Loss: 2.0217\n",
      "Epoch [3/2000] | Train Acc: 39.91% | Test Acc: 52.83% | Loss: 1.9684\n",
      "Epoch [4/2000] | Train Acc: 41.10% | Test Acc: 54.77% | Loss: 1.9162\n",
      "Epoch [5/2000] | Train Acc: 42.07% | Test Acc: 55.41% | Loss: 1.8640\n",
      "Epoch [6/2000] | Train Acc: 42.55% | Test Acc: 56.01% | Loss: 1.8114\n",
      "Epoch [7/2000] | Train Acc: 43.11% | Test Acc: 56.08% | Loss: 1.7582\n",
      "Epoch [8/2000] | Train Acc: 43.26% | Test Acc: 56.37% | Loss: 1.7047\n",
      "Epoch [9/2000] | Train Acc: 43.49% | Test Acc: 56.63% | Loss: 1.6520\n",
      "Epoch [10/2000] | Train Acc: 43.72% | Test Acc: 57.33% | Loss: 1.6019\n",
      "Epoch [11/2000] | Train Acc: 44.15% | Test Acc: 58.34% | Loss: 1.5560\n",
      "Epoch [12/2000] | Train Acc: 44.77% | Test Acc: 59.68% | Loss: 1.5154\n",
      "Epoch [13/2000] | Train Acc: 45.53% | Test Acc: 61.01% | Loss: 1.4806\n",
      "Epoch [14/2000] | Train Acc: 46.48% | Test Acc: 62.51% | Loss: 1.4515\n",
      "Epoch [15/2000] | Train Acc: 47.65% | Test Acc: 63.68% | Loss: 1.4275\n",
      "Epoch [16/2000] | Train Acc: 48.26% | Test Acc: 65.20% | Loss: 1.4076\n",
      "Epoch [17/2000] | Train Acc: 49.39% | Test Acc: 65.88% | Loss: 1.3910\n",
      "Epoch [18/2000] | Train Acc: 49.75% | Test Acc: 66.15% | Loss: 1.3766\n",
      "Epoch [19/2000] | Train Acc: 49.96% | Test Acc: 66.72% | Loss: 1.3633\n",
      "Epoch [20/2000] | Train Acc: 50.22% | Test Acc: 67.59% | Loss: 1.3503\n",
      "Epoch [21/2000] | Train Acc: 50.92% | Test Acc: 68.83% | Loss: 1.3371\n",
      "Epoch [22/2000] | Train Acc: 51.77% | Test Acc: 70.05% | Loss: 1.3234\n",
      "Epoch [23/2000] | Train Acc: 52.77% | Test Acc: 71.22% | Loss: 1.3095\n",
      "Epoch [24/2000] | Train Acc: 53.66% | Test Acc: 71.94% | Loss: 1.2957\n",
      "Epoch [25/2000] | Train Acc: 54.46% | Test Acc: 72.55% | Loss: 1.2824\n",
      "Epoch [26/2000] | Train Acc: 55.04% | Test Acc: 72.87% | Loss: 1.2696\n",
      "Epoch [27/2000] | Train Acc: 55.43% | Test Acc: 73.84% | Loss: 1.2576\n",
      "Epoch [28/2000] | Train Acc: 55.99% | Test Acc: 74.52% | Loss: 1.2460\n",
      "Epoch [29/2000] | Train Acc: 56.50% | Test Acc: 75.28% | Loss: 1.2347\n",
      "Epoch [30/2000] | Train Acc: 57.10% | Test Acc: 76.09% | Loss: 1.2236\n",
      "Epoch [31/2000] | Train Acc: 57.56% | Test Acc: 76.90% | Loss: 1.2128\n",
      "Epoch [32/2000] | Train Acc: 58.16% | Test Acc: 77.50% | Loss: 1.2022\n",
      "Epoch [33/2000] | Train Acc: 58.88% | Test Acc: 78.00% | Loss: 1.1921\n",
      "Epoch [34/2000] | Train Acc: 59.18% | Test Acc: 78.35% | Loss: 1.1826\n",
      "Epoch [35/2000] | Train Acc: 59.42% | Test Acc: 78.51% | Loss: 1.1735\n",
      "Epoch [36/2000] | Train Acc: 59.56% | Test Acc: 78.91% | Loss: 1.1650\n",
      "Epoch [37/2000] | Train Acc: 59.84% | Test Acc: 79.13% | Loss: 1.1568\n",
      "Epoch [38/2000] | Train Acc: 59.90% | Test Acc: 79.15% | Loss: 1.1489\n",
      "Epoch [39/2000] | Train Acc: 59.98% | Test Acc: 79.42% | Loss: 1.1410\n",
      "Epoch [40/2000] | Train Acc: 60.12% | Test Acc: 79.52% | Loss: 1.1332\n",
      "Epoch [41/2000] | Train Acc: 60.22% | Test Acc: 79.60% | Loss: 1.1255\n",
      "Epoch [42/2000] | Train Acc: 60.37% | Test Acc: 79.94% | Loss: 1.1180\n",
      "Epoch [43/2000] | Train Acc: 60.59% | Test Acc: 80.23% | Loss: 1.1106\n",
      "Epoch [44/2000] | Train Acc: 60.79% | Test Acc: 80.44% | Loss: 1.1035\n",
      "Epoch [45/2000] | Train Acc: 60.95% | Test Acc: 80.81% | Loss: 1.0966\n",
      "Epoch [46/2000] | Train Acc: 61.11% | Test Acc: 80.96% | Loss: 1.0898\n",
      "Epoch [47/2000] | Train Acc: 61.22% | Test Acc: 81.32% | Loss: 1.0832\n",
      "Epoch [48/2000] | Train Acc: 61.45% | Test Acc: 81.44% | Loss: 1.0768\n",
      "Epoch [49/2000] | Train Acc: 61.51% | Test Acc: 81.62% | Loss: 1.0704\n",
      "Epoch [50/2000] | Train Acc: 61.62% | Test Acc: 81.86% | Loss: 1.0641\n",
      "Epoch [51/2000] | Train Acc: 61.71% | Test Acc: 81.99% | Loss: 1.0579\n",
      "Epoch [52/2000] | Train Acc: 61.84% | Test Acc: 82.18% | Loss: 1.0519\n",
      "Epoch [53/2000] | Train Acc: 61.88% | Test Acc: 82.27% | Loss: 1.0459\n",
      "Epoch [54/2000] | Train Acc: 62.00% | Test Acc: 82.70% | Loss: 1.0401\n",
      "Epoch [55/2000] | Train Acc: 62.12% | Test Acc: 82.90% | Loss: 1.0344\n",
      "Epoch [56/2000] | Train Acc: 62.32% | Test Acc: 83.09% | Loss: 1.0287\n",
      "Epoch [57/2000] | Train Acc: 62.49% | Test Acc: 83.39% | Loss: 1.0231\n",
      "Epoch [58/2000] | Train Acc: 62.70% | Test Acc: 83.82% | Loss: 1.0176\n",
      "Epoch [59/2000] | Train Acc: 63.01% | Test Acc: 83.97% | Loss: 1.0121\n",
      "Epoch [60/2000] | Train Acc: 63.20% | Test Acc: 84.03% | Loss: 1.0067\n",
      "Epoch [61/2000] | Train Acc: 63.27% | Test Acc: 84.04% | Loss: 1.0015\n",
      "Epoch [62/2000] | Train Acc: 63.38% | Test Acc: 84.13% | Loss: 0.9963\n",
      "Epoch [63/2000] | Train Acc: 63.48% | Test Acc: 84.18% | Loss: 0.9912\n",
      "Epoch [64/2000] | Train Acc: 63.62% | Test Acc: 84.06% | Loss: 0.9862\n",
      "Epoch [65/2000] | Train Acc: 63.72% | Test Acc: 84.06% | Loss: 0.9813\n",
      "Epoch [66/2000] | Train Acc: 63.78% | Test Acc: 84.16% | Loss: 0.9764\n",
      "Epoch [67/2000] | Train Acc: 63.86% | Test Acc: 84.24% | Loss: 0.9716\n",
      "Epoch [68/2000] | Train Acc: 63.94% | Test Acc: 84.40% | Loss: 0.9668\n",
      "Epoch [69/2000] | Train Acc: 64.09% | Test Acc: 84.44% | Loss: 0.9621\n",
      "Epoch [70/2000] | Train Acc: 64.22% | Test Acc: 84.58% | Loss: 0.9575\n",
      "Epoch [71/2000] | Train Acc: 64.36% | Test Acc: 84.74% | Loss: 0.9529\n",
      "Epoch [72/2000] | Train Acc: 64.39% | Test Acc: 84.82% | Loss: 0.9483\n",
      "Epoch [73/2000] | Train Acc: 64.52% | Test Acc: 85.23% | Loss: 0.9438\n",
      "Epoch [74/2000] | Train Acc: 65.01% | Test Acc: 85.38% | Loss: 0.9393\n",
      "Epoch [75/2000] | Train Acc: 65.16% | Test Acc: 85.48% | Loss: 0.9349\n",
      "Epoch [76/2000] | Train Acc: 65.32% | Test Acc: 85.65% | Loss: 0.9305\n",
      "Epoch [77/2000] | Train Acc: 65.44% | Test Acc: 85.71% | Loss: 0.9262\n",
      "Epoch [78/2000] | Train Acc: 65.64% | Test Acc: 85.75% | Loss: 0.9219\n",
      "Epoch [79/2000] | Train Acc: 65.69% | Test Acc: 85.81% | Loss: 0.9176\n",
      "Epoch [80/2000] | Train Acc: 65.80% | Test Acc: 85.87% | Loss: 0.9134\n",
      "Epoch [81/2000] | Train Acc: 65.87% | Test Acc: 85.92% | Loss: 0.9092\n",
      "Epoch [82/2000] | Train Acc: 65.94% | Test Acc: 85.93% | Loss: 0.9050\n",
      "Epoch [83/2000] | Train Acc: 65.99% | Test Acc: 86.01% | Loss: 0.9009\n",
      "Epoch [84/2000] | Train Acc: 66.13% | Test Acc: 86.09% | Loss: 0.8968\n",
      "Epoch [85/2000] | Train Acc: 66.24% | Test Acc: 86.27% | Loss: 0.8927\n",
      "Epoch [86/2000] | Train Acc: 66.37% | Test Acc: 86.50% | Loss: 0.8887\n",
      "Epoch [87/2000] | Train Acc: 66.51% | Test Acc: 86.62% | Loss: 0.8846\n",
      "Epoch [88/2000] | Train Acc: 66.62% | Test Acc: 86.66% | Loss: 0.8807\n",
      "Epoch [89/2000] | Train Acc: 66.71% | Test Acc: 86.73% | Loss: 0.8767\n",
      "Epoch [90/2000] | Train Acc: 66.78% | Test Acc: 86.82% | Loss: 0.8728\n",
      "Epoch [91/2000] | Train Acc: 66.87% | Test Acc: 86.82% | Loss: 0.8689\n",
      "Epoch [92/2000] | Train Acc: 66.97% | Test Acc: 86.85% | Loss: 0.8650\n",
      "Epoch [93/2000] | Train Acc: 67.08% | Test Acc: 86.88% | Loss: 0.8611\n",
      "Epoch [94/2000] | Train Acc: 67.18% | Test Acc: 86.94% | Loss: 0.8573\n",
      "Epoch [95/2000] | Train Acc: 67.30% | Test Acc: 87.06% | Loss: 0.8535\n",
      "Epoch [96/2000] | Train Acc: 67.46% | Test Acc: 87.12% | Loss: 0.8497\n",
      "Epoch [97/2000] | Train Acc: 67.59% | Test Acc: 87.17% | Loss: 0.8460\n",
      "Epoch [98/2000] | Train Acc: 67.70% | Test Acc: 87.25% | Loss: 0.8422\n",
      "Epoch [99/2000] | Train Acc: 67.82% | Test Acc: 87.33% | Loss: 0.8385\n",
      "Epoch [100/2000] | Train Acc: 67.98% | Test Acc: 87.34% | Loss: 0.8348\n",
      "Epoch [101/2000] | Train Acc: 68.09% | Test Acc: 87.45% | Loss: 0.8312\n",
      "Epoch [102/2000] | Train Acc: 68.19% | Test Acc: 87.51% | Loss: 0.8275\n",
      "Epoch [103/2000] | Train Acc: 68.29% | Test Acc: 87.64% | Loss: 0.8239\n",
      "Epoch [104/2000] | Train Acc: 68.38% | Test Acc: 87.75% | Loss: 0.8203\n",
      "Epoch [105/2000] | Train Acc: 68.47% | Test Acc: 87.89% | Loss: 0.8167\n",
      "Epoch [106/2000] | Train Acc: 68.59% | Test Acc: 87.89% | Loss: 0.8131\n",
      "Epoch [107/2000] | Train Acc: 68.74% | Test Acc: 87.97% | Loss: 0.8096\n",
      "Epoch [108/2000] | Train Acc: 68.86% | Test Acc: 88.09% | Loss: 0.8060\n",
      "Epoch [109/2000] | Train Acc: 69.01% | Test Acc: 88.15% | Loss: 0.8025\n",
      "Epoch [110/2000] | Train Acc: 69.11% | Test Acc: 88.31% | Loss: 0.7991\n",
      "Epoch [111/2000] | Train Acc: 69.22% | Test Acc: 88.41% | Loss: 0.7956\n",
      "Epoch [112/2000] | Train Acc: 69.38% | Test Acc: 88.53% | Loss: 0.7922\n",
      "Epoch [113/2000] | Train Acc: 69.52% | Test Acc: 88.71% | Loss: 0.7888\n",
      "Epoch [114/2000] | Train Acc: 69.66% | Test Acc: 88.82% | Loss: 0.7854\n",
      "Epoch [115/2000] | Train Acc: 69.84% | Test Acc: 88.92% | Loss: 0.7820\n",
      "Epoch [116/2000] | Train Acc: 69.96% | Test Acc: 88.95% | Loss: 0.7787\n",
      "Epoch [117/2000] | Train Acc: 70.07% | Test Acc: 88.96% | Loss: 0.7754\n",
      "Epoch [118/2000] | Train Acc: 70.29% | Test Acc: 89.08% | Loss: 0.7721\n",
      "Epoch [119/2000] | Train Acc: 70.45% | Test Acc: 89.25% | Loss: 0.7689\n",
      "Epoch [120/2000] | Train Acc: 70.60% | Test Acc: 89.46% | Loss: 0.7656\n",
      "Epoch [121/2000] | Train Acc: 70.69% | Test Acc: 89.50% | Loss: 0.7624\n",
      "Epoch [122/2000] | Train Acc: 70.79% | Test Acc: 89.65% | Loss: 0.7592\n",
      "Epoch [123/2000] | Train Acc: 70.90% | Test Acc: 89.68% | Loss: 0.7561\n",
      "Epoch [124/2000] | Train Acc: 70.98% | Test Acc: 89.78% | Loss: 0.7529\n",
      "Epoch [125/2000] | Train Acc: 71.09% | Test Acc: 89.86% | Loss: 0.7498\n",
      "Epoch [126/2000] | Train Acc: 71.22% | Test Acc: 89.90% | Loss: 0.7467\n",
      "Epoch [127/2000] | Train Acc: 71.35% | Test Acc: 89.92% | Loss: 0.7436\n",
      "Epoch [128/2000] | Train Acc: 71.45% | Test Acc: 89.98% | Loss: 0.7406\n",
      "Epoch [129/2000] | Train Acc: 71.56% | Test Acc: 89.96% | Loss: 0.7376\n",
      "Epoch [130/2000] | Train Acc: 71.64% | Test Acc: 90.01% | Loss: 0.7346\n",
      "Epoch [131/2000] | Train Acc: 71.74% | Test Acc: 90.11% | Loss: 0.7316\n",
      "Epoch [132/2000] | Train Acc: 71.84% | Test Acc: 90.12% | Loss: 0.7286\n",
      "Epoch [133/2000] | Train Acc: 71.93% | Test Acc: 90.17% | Loss: 0.7257\n",
      "Epoch [134/2000] | Train Acc: 72.04% | Test Acc: 90.20% | Loss: 0.7228\n",
      "Epoch [135/2000] | Train Acc: 72.14% | Test Acc: 90.26% | Loss: 0.7199\n",
      "Epoch [136/2000] | Train Acc: 72.28% | Test Acc: 90.30% | Loss: 0.7170\n",
      "Epoch [137/2000] | Train Acc: 72.38% | Test Acc: 90.38% | Loss: 0.7141\n",
      "Epoch [138/2000] | Train Acc: 72.46% | Test Acc: 90.44% | Loss: 0.7113\n",
      "Epoch [139/2000] | Train Acc: 72.62% | Test Acc: 90.45% | Loss: 0.7084\n",
      "Epoch [140/2000] | Train Acc: 72.76% | Test Acc: 90.47% | Loss: 0.7056\n",
      "Epoch [141/2000] | Train Acc: 72.85% | Test Acc: 90.56% | Loss: 0.7028\n",
      "Epoch [142/2000] | Train Acc: 72.94% | Test Acc: 90.57% | Loss: 0.7000\n",
      "Epoch [143/2000] | Train Acc: 73.06% | Test Acc: 90.65% | Loss: 0.6972\n",
      "Epoch [144/2000] | Train Acc: 73.21% | Test Acc: 90.69% | Loss: 0.6944\n",
      "Epoch [145/2000] | Train Acc: 73.29% | Test Acc: 90.73% | Loss: 0.6917\n",
      "Epoch [146/2000] | Train Acc: 73.43% | Test Acc: 90.79% | Loss: 0.6889\n",
      "Epoch [147/2000] | Train Acc: 73.54% | Test Acc: 90.78% | Loss: 0.6862\n",
      "Epoch [148/2000] | Train Acc: 73.67% | Test Acc: 90.79% | Loss: 0.6835\n",
      "Epoch [149/2000] | Train Acc: 73.77% | Test Acc: 90.84% | Loss: 0.6808\n",
      "Epoch [150/2000] | Train Acc: 73.87% | Test Acc: 90.91% | Loss: 0.6781\n",
      "Epoch [151/2000] | Train Acc: 74.01% | Test Acc: 90.93% | Loss: 0.6754\n",
      "Epoch [152/2000] | Train Acc: 74.14% | Test Acc: 90.97% | Loss: 0.6727\n",
      "Epoch [153/2000] | Train Acc: 74.24% | Test Acc: 91.00% | Loss: 0.6700\n",
      "Epoch [154/2000] | Train Acc: 74.36% | Test Acc: 91.02% | Loss: 0.6674\n",
      "Epoch [155/2000] | Train Acc: 74.49% | Test Acc: 91.03% | Loss: 0.6647\n",
      "Epoch [156/2000] | Train Acc: 74.62% | Test Acc: 91.03% | Loss: 0.6621\n",
      "Epoch [157/2000] | Train Acc: 74.72% | Test Acc: 91.05% | Loss: 0.6595\n",
      "Epoch [158/2000] | Train Acc: 74.87% | Test Acc: 91.18% | Loss: 0.6569\n",
      "Epoch [159/2000] | Train Acc: 75.01% | Test Acc: 91.23% | Loss: 0.6542\n",
      "Epoch [160/2000] | Train Acc: 75.16% | Test Acc: 91.26% | Loss: 0.6517\n",
      "Epoch [161/2000] | Train Acc: 75.27% | Test Acc: 91.26% | Loss: 0.6491\n",
      "Epoch [162/2000] | Train Acc: 75.39% | Test Acc: 91.26% | Loss: 0.6465\n",
      "Epoch [163/2000] | Train Acc: 75.46% | Test Acc: 91.30% | Loss: 0.6439\n",
      "Epoch [164/2000] | Train Acc: 75.54% | Test Acc: 91.36% | Loss: 0.6414\n",
      "Epoch [165/2000] | Train Acc: 75.64% | Test Acc: 91.39% | Loss: 0.6388\n",
      "Epoch [166/2000] | Train Acc: 75.77% | Test Acc: 91.40% | Loss: 0.6363\n",
      "Epoch [167/2000] | Train Acc: 75.87% | Test Acc: 91.43% | Loss: 0.6338\n",
      "Epoch [168/2000] | Train Acc: 75.97% | Test Acc: 91.48% | Loss: 0.6313\n",
      "Epoch [169/2000] | Train Acc: 76.06% | Test Acc: 91.52% | Loss: 0.6287\n",
      "Epoch [170/2000] | Train Acc: 76.18% | Test Acc: 91.52% | Loss: 0.6262\n",
      "Epoch [171/2000] | Train Acc: 76.26% | Test Acc: 91.61% | Loss: 0.6237\n",
      "Epoch [172/2000] | Train Acc: 76.39% | Test Acc: 91.63% | Loss: 0.6213\n",
      "Epoch [173/2000] | Train Acc: 76.53% | Test Acc: 91.63% | Loss: 0.6188\n",
      "Epoch [174/2000] | Train Acc: 76.65% | Test Acc: 91.64% | Loss: 0.6163\n",
      "Epoch [175/2000] | Train Acc: 76.75% | Test Acc: 91.66% | Loss: 0.6138\n",
      "Epoch [176/2000] | Train Acc: 76.85% | Test Acc: 91.73% | Loss: 0.6114\n",
      "Epoch [177/2000] | Train Acc: 76.99% | Test Acc: 91.73% | Loss: 0.6089\n",
      "Epoch [178/2000] | Train Acc: 77.08% | Test Acc: 91.76% | Loss: 0.6065\n",
      "Epoch [179/2000] | Train Acc: 77.21% | Test Acc: 91.73% | Loss: 0.6041\n",
      "Epoch [180/2000] | Train Acc: 77.33% | Test Acc: 91.78% | Loss: 0.6016\n",
      "Epoch [181/2000] | Train Acc: 77.46% | Test Acc: 91.79% | Loss: 0.5992\n",
      "Epoch [182/2000] | Train Acc: 77.55% | Test Acc: 91.82% | Loss: 0.5968\n",
      "Epoch [183/2000] | Train Acc: 77.66% | Test Acc: 91.82% | Loss: 0.5944\n",
      "Epoch [184/2000] | Train Acc: 77.80% | Test Acc: 91.79% | Loss: 0.5919\n",
      "Epoch [185/2000] | Train Acc: 77.90% | Test Acc: 91.85% | Loss: 0.5895\n",
      "Epoch [186/2000] | Train Acc: 78.02% | Test Acc: 91.86% | Loss: 0.5871\n",
      "Epoch [187/2000] | Train Acc: 78.15% | Test Acc: 91.89% | Loss: 0.5847\n",
      "Epoch [188/2000] | Train Acc: 78.28% | Test Acc: 91.91% | Loss: 0.5824\n",
      "Epoch [189/2000] | Train Acc: 78.45% | Test Acc: 91.91% | Loss: 0.5800\n",
      "Epoch [190/2000] | Train Acc: 78.56% | Test Acc: 91.94% | Loss: 0.5776\n",
      "Epoch [191/2000] | Train Acc: 78.65% | Test Acc: 91.95% | Loss: 0.5752\n",
      "Epoch [192/2000] | Train Acc: 78.76% | Test Acc: 92.03% | Loss: 0.5728\n",
      "Epoch [193/2000] | Train Acc: 78.88% | Test Acc: 92.12% | Loss: 0.5705\n",
      "Epoch [194/2000] | Train Acc: 79.03% | Test Acc: 92.12% | Loss: 0.5681\n",
      "Epoch [195/2000] | Train Acc: 79.13% | Test Acc: 92.15% | Loss: 0.5657\n",
      "Epoch [196/2000] | Train Acc: 79.25% | Test Acc: 92.16% | Loss: 0.5634\n",
      "Epoch [197/2000] | Train Acc: 79.34% | Test Acc: 92.18% | Loss: 0.5610\n",
      "Epoch [198/2000] | Train Acc: 79.44% | Test Acc: 92.21% | Loss: 0.5587\n",
      "Epoch [199/2000] | Train Acc: 79.57% | Test Acc: 92.19% | Loss: 0.5563\n",
      "Epoch [200/2000] | Train Acc: 79.67% | Test Acc: 92.25% | Loss: 0.5540\n",
      "Epoch [201/2000] | Train Acc: 79.75% | Test Acc: 92.31% | Loss: 0.5516\n",
      "Epoch [202/2000] | Train Acc: 79.85% | Test Acc: 92.30% | Loss: 0.5493\n",
      "Epoch [203/2000] | Train Acc: 79.95% | Test Acc: 92.31% | Loss: 0.5470\n",
      "Epoch [204/2000] | Train Acc: 80.04% | Test Acc: 92.28% | Loss: 0.5446\n",
      "Epoch [205/2000] | Train Acc: 80.16% | Test Acc: 92.31% | Loss: 0.5423\n",
      "Epoch [206/2000] | Train Acc: 80.24% | Test Acc: 92.33% | Loss: 0.5400\n",
      "Epoch [207/2000] | Train Acc: 80.34% | Test Acc: 92.36% | Loss: 0.5377\n",
      "Epoch [208/2000] | Train Acc: 80.46% | Test Acc: 92.39% | Loss: 0.5354\n",
      "Epoch [209/2000] | Train Acc: 80.53% | Test Acc: 92.44% | Loss: 0.5331\n",
      "Epoch [210/2000] | Train Acc: 80.63% | Test Acc: 92.44% | Loss: 0.5308\n",
      "Epoch [211/2000] | Train Acc: 80.77% | Test Acc: 92.49% | Loss: 0.5285\n",
      "Epoch [212/2000] | Train Acc: 80.88% | Test Acc: 92.62% | Loss: 0.5262\n",
      "Epoch [213/2000] | Train Acc: 81.02% | Test Acc: 92.62% | Loss: 0.5239\n",
      "Epoch [214/2000] | Train Acc: 81.09% | Test Acc: 92.67% | Loss: 0.5216\n",
      "Epoch [215/2000] | Train Acc: 81.22% | Test Acc: 92.67% | Loss: 0.5193\n",
      "Epoch [216/2000] | Train Acc: 81.33% | Test Acc: 92.73% | Loss: 0.5170\n",
      "Epoch [217/2000] | Train Acc: 81.45% | Test Acc: 92.71% | Loss: 0.5147\n",
      "Epoch [218/2000] | Train Acc: 81.54% | Test Acc: 92.73% | Loss: 0.5125\n",
      "Epoch [219/2000] | Train Acc: 81.63% | Test Acc: 92.74% | Loss: 0.5102\n",
      "Epoch [220/2000] | Train Acc: 81.72% | Test Acc: 92.74% | Loss: 0.5079\n",
      "Epoch [221/2000] | Train Acc: 81.83% | Test Acc: 92.80% | Loss: 0.5057\n",
      "Epoch [222/2000] | Train Acc: 81.96% | Test Acc: 92.82% | Loss: 0.5034\n",
      "Epoch [223/2000] | Train Acc: 82.10% | Test Acc: 92.91% | Loss: 0.5012\n",
      "Epoch [224/2000] | Train Acc: 82.21% | Test Acc: 92.89% | Loss: 0.4989\n",
      "Epoch [225/2000] | Train Acc: 82.29% | Test Acc: 92.97% | Loss: 0.4967\n",
      "Epoch [226/2000] | Train Acc: 82.41% | Test Acc: 92.91% | Loss: 0.4945\n",
      "Epoch [227/2000] | Train Acc: 82.55% | Test Acc: 93.04% | Loss: 0.4923\n",
      "Epoch [228/2000] | Train Acc: 82.74% | Test Acc: 92.76% | Loss: 0.4901\n",
      "Epoch [229/2000] | Train Acc: 82.53% | Test Acc: 93.44% | Loss: 0.4880\n",
      "Epoch [230/2000] | Train Acc: 83.19% | Test Acc: 92.62% | Loss: 0.4862\n",
      "Epoch [231/2000] | Train Acc: 82.67% | Test Acc: 93.47% | Loss: 0.4846\n",
      "Epoch [232/2000] | Train Acc: 83.37% | Test Acc: 92.95% | Loss: 0.4820\n",
      "Epoch [233/2000] | Train Acc: 83.18% | Test Acc: 92.70% | Loss: 0.4791\n",
      "Epoch [234/2000] | Train Acc: 82.91% | Test Acc: 93.46% | Loss: 0.4774\n",
      "Epoch [235/2000] | Train Acc: 83.67% | Test Acc: 93.01% | Loss: 0.4755\n",
      "Epoch [236/2000] | Train Acc: 83.42% | Test Acc: 92.85% | Loss: 0.4728\n",
      "Epoch [237/2000] | Train Acc: 83.36% | Test Acc: 93.44% | Loss: 0.4708\n",
      "Epoch [238/2000] | Train Acc: 83.95% | Test Acc: 93.05% | Loss: 0.4690\n",
      "Epoch [239/2000] | Train Acc: 83.75% | Test Acc: 93.04% | Loss: 0.4664\n",
      "Epoch [240/2000] | Train Acc: 83.82% | Test Acc: 93.43% | Loss: 0.4644\n",
      "Epoch [241/2000] | Train Acc: 84.25% | Test Acc: 93.04% | Loss: 0.4626\n",
      "Epoch [242/2000] | Train Acc: 84.04% | Test Acc: 93.02% | Loss: 0.4602\n",
      "Epoch [243/2000] | Train Acc: 84.09% | Test Acc: 93.44% | Loss: 0.4582\n",
      "Epoch [244/2000] | Train Acc: 84.54% | Test Acc: 93.16% | Loss: 0.4562\n",
      "Epoch [245/2000] | Train Acc: 84.48% | Test Acc: 93.10% | Loss: 0.4540\n",
      "Epoch [246/2000] | Train Acc: 84.45% | Test Acc: 93.43% | Loss: 0.4519\n",
      "Epoch [247/2000] | Train Acc: 84.80% | Test Acc: 93.17% | Loss: 0.4500\n",
      "Epoch [248/2000] | Train Acc: 84.75% | Test Acc: 93.23% | Loss: 0.4478\n",
      "Epoch [249/2000] | Train Acc: 84.84% | Test Acc: 93.41% | Loss: 0.4458\n",
      "Epoch [250/2000] | Train Acc: 85.08% | Test Acc: 93.23% | Loss: 0.4439\n",
      "Epoch [251/2000] | Train Acc: 85.02% | Test Acc: 93.23% | Loss: 0.4417\n",
      "Epoch [252/2000] | Train Acc: 85.14% | Test Acc: 93.41% | Loss: 0.4397\n",
      "Epoch [253/2000] | Train Acc: 85.37% | Test Acc: 93.25% | Loss: 0.4378\n",
      "Epoch [254/2000] | Train Acc: 85.25% | Test Acc: 93.25% | Loss: 0.4357\n",
      "Epoch [255/2000] | Train Acc: 85.40% | Test Acc: 93.40% | Loss: 0.4337\n",
      "Epoch [256/2000] | Train Acc: 85.57% | Test Acc: 93.26% | Loss: 0.4317\n",
      "Epoch [257/2000] | Train Acc: 85.49% | Test Acc: 93.38% | Loss: 0.4297\n",
      "Epoch [258/2000] | Train Acc: 85.69% | Test Acc: 93.38% | Loss: 0.4277\n",
      "Epoch [259/2000] | Train Acc: 85.78% | Test Acc: 93.31% | Loss: 0.4257\n",
      "Epoch [260/2000] | Train Acc: 85.74% | Test Acc: 93.38% | Loss: 0.4238\n",
      "Epoch [261/2000] | Train Acc: 85.95% | Test Acc: 93.43% | Loss: 0.4218\n",
      "Epoch [262/2000] | Train Acc: 86.04% | Test Acc: 93.34% | Loss: 0.4198\n",
      "Epoch [263/2000] | Train Acc: 86.06% | Test Acc: 93.46% | Loss: 0.4179\n",
      "Epoch [264/2000] | Train Acc: 86.25% | Test Acc: 93.34% | Loss: 0.4160\n",
      "Epoch [265/2000] | Train Acc: 86.32% | Test Acc: 93.38% | Loss: 0.4140\n",
      "Epoch [266/2000] | Train Acc: 86.44% | Test Acc: 93.52% | Loss: 0.4120\n",
      "Epoch [267/2000] | Train Acc: 86.56% | Test Acc: 93.31% | Loss: 0.4101\n",
      "Epoch [268/2000] | Train Acc: 86.52% | Test Acc: 93.53% | Loss: 0.4082\n",
      "Epoch [269/2000] | Train Acc: 86.73% | Test Acc: 93.49% | Loss: 0.4063\n",
      "Epoch [270/2000] | Train Acc: 86.78% | Test Acc: 93.41% | Loss: 0.4044\n",
      "Epoch [271/2000] | Train Acc: 86.80% | Test Acc: 93.60% | Loss: 0.4025\n",
      "Epoch [272/2000] | Train Acc: 86.99% | Test Acc: 93.31% | Loss: 0.4006\n",
      "Epoch [273/2000] | Train Acc: 87.02% | Test Acc: 93.60% | Loss: 0.3987\n",
      "Epoch [274/2000] | Train Acc: 87.17% | Test Acc: 93.52% | Loss: 0.3968\n",
      "Epoch [275/2000] | Train Acc: 87.25% | Test Acc: 93.50% | Loss: 0.3949\n",
      "Epoch [276/2000] | Train Acc: 87.30% | Test Acc: 93.62% | Loss: 0.3930\n",
      "Epoch [277/2000] | Train Acc: 87.43% | Test Acc: 93.38% | Loss: 0.3912\n",
      "Epoch [278/2000] | Train Acc: 87.38% | Test Acc: 93.62% | Loss: 0.3893\n",
      "Epoch [279/2000] | Train Acc: 87.56% | Test Acc: 93.47% | Loss: 0.3875\n",
      "Epoch [280/2000] | Train Acc: 87.62% | Test Acc: 93.56% | Loss: 0.3856\n",
      "Epoch [281/2000] | Train Acc: 87.73% | Test Acc: 93.55% | Loss: 0.3838\n",
      "Epoch [282/2000] | Train Acc: 87.79% | Test Acc: 93.52% | Loss: 0.3819\n",
      "Epoch [283/2000] | Train Acc: 87.84% | Test Acc: 93.65% | Loss: 0.3801\n",
      "Epoch [284/2000] | Train Acc: 87.92% | Test Acc: 93.49% | Loss: 0.3783\n",
      "Epoch [285/2000] | Train Acc: 87.99% | Test Acc: 93.66% | Loss: 0.3765\n",
      "Epoch [286/2000] | Train Acc: 88.11% | Test Acc: 93.56% | Loss: 0.3747\n",
      "Epoch [287/2000] | Train Acc: 88.18% | Test Acc: 93.68% | Loss: 0.3729\n",
      "Epoch [288/2000] | Train Acc: 88.30% | Test Acc: 93.55% | Loss: 0.3711\n",
      "Epoch [289/2000] | Train Acc: 88.33% | Test Acc: 93.66% | Loss: 0.3693\n",
      "Epoch [290/2000] | Train Acc: 88.47% | Test Acc: 93.65% | Loss: 0.3676\n",
      "Epoch [291/2000] | Train Acc: 88.53% | Test Acc: 93.63% | Loss: 0.3659\n",
      "Epoch [292/2000] | Train Acc: 88.65% | Test Acc: 93.52% | Loss: 0.3642\n",
      "Epoch [293/2000] | Train Acc: 88.53% | Test Acc: 93.68% | Loss: 0.3626\n",
      "Epoch [294/2000] | Train Acc: 88.86% | Test Acc: 93.17% | Loss: 0.3610\n",
      "Epoch [295/2000] | Train Acc: 88.39% | Test Acc: 93.69% | Loss: 0.3595\n",
      "Epoch [296/2000] | Train Acc: 88.98% | Test Acc: 93.53% | Loss: 0.3577\n",
      "Epoch [297/2000] | Train Acc: 88.86% | Test Acc: 93.62% | Loss: 0.3557\n",
      "Epoch [298/2000] | Train Acc: 89.05% | Test Acc: 93.66% | Loss: 0.3537\n",
      "Epoch [299/2000] | Train Acc: 89.10% | Test Acc: 93.65% | Loss: 0.3519\n",
      "Epoch [300/2000] | Train Acc: 89.21% | Test Acc: 93.63% | Loss: 0.3504\n",
      "Epoch [301/2000] | Train Acc: 89.35% | Test Acc: 93.60% | Loss: 0.3490\n",
      "Epoch [302/2000] | Train Acc: 89.27% | Test Acc: 93.58% | Loss: 0.3473\n",
      "Epoch [303/2000] | Train Acc: 89.44% | Test Acc: 93.63% | Loss: 0.3454\n",
      "Epoch [304/2000] | Train Acc: 89.52% | Test Acc: 93.63% | Loss: 0.3436\n",
      "Epoch [305/2000] | Train Acc: 89.57% | Test Acc: 93.56% | Loss: 0.3420\n",
      "Epoch [306/2000] | Train Acc: 89.63% | Test Acc: 93.62% | Loss: 0.3405\n",
      "Epoch [307/2000] | Train Acc: 89.66% | Test Acc: 93.59% | Loss: 0.3390\n",
      "Epoch [308/2000] | Train Acc: 89.78% | Test Acc: 93.65% | Loss: 0.3372\n",
      "Epoch [309/2000] | Train Acc: 89.84% | Test Acc: 93.59% | Loss: 0.3355\n",
      "Epoch [310/2000] | Train Acc: 89.90% | Test Acc: 93.62% | Loss: 0.3338\n",
      "Epoch [311/2000] | Train Acc: 90.00% | Test Acc: 93.71% | Loss: 0.3323\n",
      "Epoch [312/2000] | Train Acc: 90.04% | Test Acc: 93.63% | Loss: 0.3308\n",
      "Epoch [313/2000] | Train Acc: 90.14% | Test Acc: 93.78% | Loss: 0.3292\n",
      "Epoch [314/2000] | Train Acc: 90.23% | Test Acc: 93.66% | Loss: 0.3276\n",
      "Epoch [315/2000] | Train Acc: 90.30% | Test Acc: 93.68% | Loss: 0.3260\n",
      "Epoch [316/2000] | Train Acc: 90.30% | Test Acc: 93.77% | Loss: 0.3243\n",
      "Epoch [317/2000] | Train Acc: 90.38% | Test Acc: 93.74% | Loss: 0.3228\n",
      "Epoch [318/2000] | Train Acc: 90.48% | Test Acc: 93.80% | Loss: 0.3213\n",
      "Epoch [319/2000] | Train Acc: 90.51% | Test Acc: 93.74% | Loss: 0.3198\n",
      "Epoch [320/2000] | Train Acc: 90.58% | Test Acc: 93.81% | Loss: 0.3183\n",
      "Epoch [321/2000] | Train Acc: 90.66% | Test Acc: 93.65% | Loss: 0.3167\n",
      "Epoch [322/2000] | Train Acc: 90.71% | Test Acc: 93.83% | Loss: 0.3152\n",
      "Epoch [323/2000] | Train Acc: 90.77% | Test Acc: 93.69% | Loss: 0.3136\n",
      "Epoch [324/2000] | Train Acc: 90.80% | Test Acc: 93.71% | Loss: 0.3121\n",
      "Epoch [325/2000] | Train Acc: 90.89% | Test Acc: 93.87% | Loss: 0.3106\n",
      "Epoch [326/2000] | Train Acc: 90.98% | Test Acc: 93.71% | Loss: 0.3091\n",
      "Epoch [327/2000] | Train Acc: 91.05% | Test Acc: 93.92% | Loss: 0.3076\n",
      "Epoch [328/2000] | Train Acc: 91.10% | Test Acc: 93.68% | Loss: 0.3062\n",
      "Epoch [329/2000] | Train Acc: 91.18% | Test Acc: 93.86% | Loss: 0.3047\n",
      "Epoch [330/2000] | Train Acc: 91.20% | Test Acc: 93.68% | Loss: 0.3033\n",
      "Epoch [331/2000] | Train Acc: 91.31% | Test Acc: 93.80% | Loss: 0.3019\n",
      "Epoch [332/2000] | Train Acc: 91.30% | Test Acc: 93.72% | Loss: 0.3005\n",
      "Epoch [333/2000] | Train Acc: 91.50% | Test Acc: 93.83% | Loss: 0.2991\n",
      "Epoch [334/2000] | Train Acc: 91.41% | Test Acc: 93.69% | Loss: 0.2978\n",
      "Epoch [335/2000] | Train Acc: 91.61% | Test Acc: 93.86% | Loss: 0.2964\n",
      "Epoch [336/2000] | Train Acc: 91.53% | Test Acc: 93.77% | Loss: 0.2949\n",
      "Epoch [337/2000] | Train Acc: 91.74% | Test Acc: 93.89% | Loss: 0.2934\n",
      "Epoch [338/2000] | Train Acc: 91.74% | Test Acc: 93.71% | Loss: 0.2918\n",
      "Epoch [339/2000] | Train Acc: 91.86% | Test Acc: 93.84% | Loss: 0.2903\n",
      "Epoch [340/2000] | Train Acc: 91.95% | Test Acc: 93.98% | Loss: 0.2889\n",
      "Epoch [341/2000] | Train Acc: 92.01% | Test Acc: 93.69% | Loss: 0.2875\n",
      "Epoch [342/2000] | Train Acc: 92.03% | Test Acc: 93.87% | Loss: 0.2862\n",
      "Epoch [343/2000] | Train Acc: 92.08% | Test Acc: 93.75% | Loss: 0.2849\n",
      "Epoch [344/2000] | Train Acc: 92.16% | Test Acc: 93.90% | Loss: 0.2836\n",
      "Epoch [345/2000] | Train Acc: 92.13% | Test Acc: 93.77% | Loss: 0.2823\n",
      "Epoch [346/2000] | Train Acc: 92.30% | Test Acc: 93.93% | Loss: 0.2810\n",
      "Epoch [347/2000] | Train Acc: 92.26% | Test Acc: 93.75% | Loss: 0.2796\n",
      "Epoch [348/2000] | Train Acc: 92.40% | Test Acc: 93.96% | Loss: 0.2782\n",
      "Epoch [349/2000] | Train Acc: 92.42% | Test Acc: 93.72% | Loss: 0.2768\n",
      "Epoch [350/2000] | Train Acc: 92.47% | Test Acc: 93.90% | Loss: 0.2754\n",
      "Epoch [351/2000] | Train Acc: 92.59% | Test Acc: 93.93% | Loss: 0.2740\n",
      "Epoch [352/2000] | Train Acc: 92.66% | Test Acc: 93.77% | Loss: 0.2727\n",
      "Epoch [353/2000] | Train Acc: 92.69% | Test Acc: 93.98% | Loss: 0.2714\n",
      "Epoch [354/2000] | Train Acc: 92.74% | Test Acc: 93.78% | Loss: 0.2702\n",
      "Epoch [355/2000] | Train Acc: 92.82% | Test Acc: 93.96% | Loss: 0.2689\n",
      "Epoch [356/2000] | Train Acc: 92.80% | Test Acc: 93.75% | Loss: 0.2677\n",
      "Epoch [357/2000] | Train Acc: 92.94% | Test Acc: 93.96% | Loss: 0.2665\n",
      "Epoch [358/2000] | Train Acc: 92.89% | Test Acc: 93.78% | Loss: 0.2653\n",
      "Epoch [359/2000] | Train Acc: 93.03% | Test Acc: 93.98% | Loss: 0.2641\n",
      "Epoch [360/2000] | Train Acc: 92.97% | Test Acc: 93.77% | Loss: 0.2630\n",
      "Epoch [361/2000] | Train Acc: 93.12% | Test Acc: 94.01% | Loss: 0.2617\n",
      "Epoch [362/2000] | Train Acc: 93.11% | Test Acc: 93.78% | Loss: 0.2604\n",
      "Epoch [363/2000] | Train Acc: 93.23% | Test Acc: 93.93% | Loss: 0.2590\n",
      "Epoch [364/2000] | Train Acc: 93.23% | Test Acc: 93.77% | Loss: 0.2576\n",
      "Epoch [365/2000] | Train Acc: 93.34% | Test Acc: 93.86% | Loss: 0.2563\n",
      "Epoch [366/2000] | Train Acc: 93.39% | Test Acc: 93.96% | Loss: 0.2551\n",
      "Epoch [367/2000] | Train Acc: 93.42% | Test Acc: 93.75% | Loss: 0.2539\n",
      "Epoch [368/2000] | Train Acc: 93.46% | Test Acc: 93.93% | Loss: 0.2528\n",
      "Epoch [369/2000] | Train Acc: 93.43% | Test Acc: 93.75% | Loss: 0.2517\n",
      "Epoch [370/2000] | Train Acc: 93.55% | Test Acc: 93.98% | Loss: 0.2506\n",
      "Epoch [371/2000] | Train Acc: 93.56% | Test Acc: 93.75% | Loss: 0.2494\n",
      "Epoch [372/2000] | Train Acc: 93.63% | Test Acc: 93.95% | Loss: 0.2482\n",
      "Epoch [373/2000] | Train Acc: 93.59% | Test Acc: 93.74% | Loss: 0.2470\n",
      "Epoch [374/2000] | Train Acc: 93.71% | Test Acc: 93.96% | Loss: 0.2457\n",
      "Epoch [375/2000] | Train Acc: 93.79% | Test Acc: 93.87% | Loss: 0.2445\n",
      "Epoch [376/2000] | Train Acc: 93.86% | Test Acc: 93.95% | Loss: 0.2433\n",
      "Epoch [377/2000] | Train Acc: 93.95% | Test Acc: 93.93% | Loss: 0.2422\n",
      "Epoch [378/2000] | Train Acc: 93.97% | Test Acc: 93.74% | Loss: 0.2411\n",
      "Epoch [379/2000] | Train Acc: 93.99% | Test Acc: 93.93% | Loss: 0.2400\n",
      "Epoch [380/2000] | Train Acc: 93.99% | Test Acc: 93.75% | Loss: 0.2389\n",
      "Epoch [381/2000] | Train Acc: 94.09% | Test Acc: 94.01% | Loss: 0.2379\n",
      "Epoch [382/2000] | Train Acc: 94.10% | Test Acc: 93.66% | Loss: 0.2368\n",
      "Epoch [383/2000] | Train Acc: 94.13% | Test Acc: 94.02% | Loss: 0.2358\n",
      "Epoch [384/2000] | Train Acc: 94.16% | Test Acc: 93.71% | Loss: 0.2348\n",
      "Epoch [385/2000] | Train Acc: 94.23% | Test Acc: 94.04% | Loss: 0.2337\n",
      "Epoch [386/2000] | Train Acc: 94.26% | Test Acc: 93.71% | Loss: 0.2327\n",
      "Epoch [387/2000] | Train Acc: 94.32% | Test Acc: 94.04% | Loss: 0.2315\n",
      "Epoch [388/2000] | Train Acc: 94.34% | Test Acc: 93.78% | Loss: 0.2304\n",
      "Epoch [389/2000] | Train Acc: 94.41% | Test Acc: 93.93% | Loss: 0.2292\n",
      "Epoch [390/2000] | Train Acc: 94.44% | Test Acc: 93.95% | Loss: 0.2280\n",
      "Epoch [391/2000] | Train Acc: 94.52% | Test Acc: 93.99% | Loss: 0.2269\n",
      "Epoch [392/2000] | Train Acc: 94.58% | Test Acc: 93.96% | Loss: 0.2259\n",
      "Epoch [393/2000] | Train Acc: 94.56% | Test Acc: 93.81% | Loss: 0.2249\n",
      "Epoch [394/2000] | Train Acc: 94.58% | Test Acc: 94.04% | Loss: 0.2239\n",
      "Epoch [395/2000] | Train Acc: 94.60% | Test Acc: 93.80% | Loss: 0.2229\n",
      "Epoch [396/2000] | Train Acc: 94.64% | Test Acc: 94.14% | Loss: 0.2220\n",
      "Epoch [397/2000] | Train Acc: 94.68% | Test Acc: 93.75% | Loss: 0.2211\n",
      "Epoch [398/2000] | Train Acc: 94.69% | Test Acc: 94.17% | Loss: 0.2201\n",
      "Epoch [399/2000] | Train Acc: 94.77% | Test Acc: 93.75% | Loss: 0.2191\n",
      "Epoch [400/2000] | Train Acc: 94.78% | Test Acc: 94.16% | Loss: 0.2181\n",
      "Epoch [401/2000] | Train Acc: 94.83% | Test Acc: 93.81% | Loss: 0.2171\n",
      "Epoch [402/2000] | Train Acc: 94.86% | Test Acc: 94.10% | Loss: 0.2160\n",
      "Epoch [403/2000] | Train Acc: 94.96% | Test Acc: 93.89% | Loss: 0.2149\n",
      "Epoch [404/2000] | Train Acc: 94.96% | Test Acc: 94.02% | Loss: 0.2139\n",
      "Epoch [405/2000] | Train Acc: 95.01% | Test Acc: 94.01% | Loss: 0.2129\n",
      "Epoch [406/2000] | Train Acc: 95.05% | Test Acc: 93.95% | Loss: 0.2119\n",
      "Epoch [407/2000] | Train Acc: 95.05% | Test Acc: 94.02% | Loss: 0.2109\n",
      "Epoch [408/2000] | Train Acc: 95.08% | Test Acc: 93.81% | Loss: 0.2100\n",
      "Epoch [409/2000] | Train Acc: 95.09% | Test Acc: 94.11% | Loss: 0.2091\n",
      "Epoch [410/2000] | Train Acc: 95.15% | Test Acc: 93.84% | Loss: 0.2083\n",
      "Epoch [411/2000] | Train Acc: 95.10% | Test Acc: 94.19% | Loss: 0.2074\n",
      "Epoch [412/2000] | Train Acc: 95.18% | Test Acc: 93.78% | Loss: 0.2066\n",
      "Epoch [413/2000] | Train Acc: 95.15% | Test Acc: 94.10% | Loss: 0.2058\n",
      "Epoch [414/2000] | Train Acc: 95.17% | Test Acc: 93.75% | Loss: 0.2050\n",
      "Epoch [415/2000] | Train Acc: 95.18% | Test Acc: 94.11% | Loss: 0.2041\n",
      "Epoch [416/2000] | Train Acc: 95.26% | Test Acc: 93.74% | Loss: 0.2033\n",
      "Epoch [417/2000] | Train Acc: 95.26% | Test Acc: 94.20% | Loss: 0.2022\n",
      "Epoch [418/2000] | Train Acc: 95.41% | Test Acc: 93.92% | Loss: 0.2012\n",
      "Epoch [419/2000] | Train Acc: 95.38% | Test Acc: 94.13% | Loss: 0.2001\n",
      "Epoch [420/2000] | Train Acc: 95.45% | Test Acc: 94.11% | Loss: 0.1991\n",
      "Epoch [421/2000] | Train Acc: 95.48% | Test Acc: 93.98% | Loss: 0.1982\n",
      "Epoch [422/2000] | Train Acc: 95.50% | Test Acc: 94.19% | Loss: 0.1974\n",
      "Epoch [423/2000] | Train Acc: 95.57% | Test Acc: 93.86% | Loss: 0.1967\n",
      "Epoch [424/2000] | Train Acc: 95.50% | Test Acc: 94.26% | Loss: 0.1959\n",
      "Epoch [425/2000] | Train Acc: 95.60% | Test Acc: 93.78% | Loss: 0.1951\n",
      "Epoch [426/2000] | Train Acc: 95.49% | Test Acc: 94.29% | Loss: 0.1943\n",
      "Epoch [427/2000] | Train Acc: 95.64% | Test Acc: 93.87% | Loss: 0.1934\n",
      "Epoch [428/2000] | Train Acc: 95.62% | Test Acc: 94.14% | Loss: 0.1925\n",
      "Epoch [429/2000] | Train Acc: 95.72% | Test Acc: 94.02% | Loss: 0.1916\n",
      "Epoch [430/2000] | Train Acc: 95.69% | Test Acc: 94.14% | Loss: 0.1907\n",
      "Epoch [431/2000] | Train Acc: 95.73% | Test Acc: 94.13% | Loss: 0.1898\n",
      "Epoch [432/2000] | Train Acc: 95.75% | Test Acc: 94.10% | Loss: 0.1890\n",
      "Epoch [433/2000] | Train Acc: 95.81% | Test Acc: 94.21% | Loss: 0.1882\n",
      "Epoch [434/2000] | Train Acc: 95.88% | Test Acc: 93.95% | Loss: 0.1875\n",
      "Epoch [435/2000] | Train Acc: 95.85% | Test Acc: 94.30% | Loss: 0.1867\n",
      "Epoch [436/2000] | Train Acc: 95.95% | Test Acc: 93.86% | Loss: 0.1860\n",
      "Epoch [437/2000] | Train Acc: 95.85% | Test Acc: 94.29% | Loss: 0.1853\n",
      "Epoch [438/2000] | Train Acc: 95.98% | Test Acc: 93.74% | Loss: 0.1846\n",
      "Epoch [439/2000] | Train Acc: 95.83% | Test Acc: 94.27% | Loss: 0.1838\n",
      "Epoch [440/2000] | Train Acc: 96.01% | Test Acc: 93.74% | Loss: 0.1831\n",
      "Epoch [441/2000] | Train Acc: 95.86% | Test Acc: 94.27% | Loss: 0.1823\n",
      "Epoch [442/2000] | Train Acc: 96.04% | Test Acc: 93.86% | Loss: 0.1815\n",
      "Epoch [443/2000] | Train Acc: 95.97% | Test Acc: 94.32% | Loss: 0.1807\n",
      "Epoch [444/2000] | Train Acc: 96.12% | Test Acc: 93.98% | Loss: 0.1799\n",
      "Epoch [445/2000] | Train Acc: 96.08% | Test Acc: 94.23% | Loss: 0.1791\n",
      "Epoch [446/2000] | Train Acc: 96.14% | Test Acc: 94.11% | Loss: 0.1783\n",
      "Epoch [447/2000] | Train Acc: 96.15% | Test Acc: 94.17% | Loss: 0.1775\n",
      "Epoch [448/2000] | Train Acc: 96.21% | Test Acc: 94.20% | Loss: 0.1767\n",
      "Epoch [449/2000] | Train Acc: 96.22% | Test Acc: 94.13% | Loss: 0.1760\n",
      "Epoch [450/2000] | Train Acc: 96.20% | Test Acc: 94.17% | Loss: 0.1753\n",
      "Epoch [451/2000] | Train Acc: 96.24% | Test Acc: 94.08% | Loss: 0.1746\n",
      "Epoch [452/2000] | Train Acc: 96.22% | Test Acc: 94.29% | Loss: 0.1739\n",
      "Epoch [453/2000] | Train Acc: 96.29% | Test Acc: 93.98% | Loss: 0.1733\n",
      "Epoch [454/2000] | Train Acc: 96.25% | Test Acc: 94.30% | Loss: 0.1726\n",
      "Epoch [455/2000] | Train Acc: 96.34% | Test Acc: 93.77% | Loss: 0.1720\n",
      "Epoch [456/2000] | Train Acc: 96.20% | Test Acc: 94.24% | Loss: 0.1715\n",
      "Epoch [457/2000] | Train Acc: 96.35% | Test Acc: 93.80% | Loss: 0.1710\n",
      "Epoch [458/2000] | Train Acc: 96.25% | Test Acc: 94.19% | Loss: 0.1705\n",
      "Epoch [459/2000] | Train Acc: 96.34% | Test Acc: 93.78% | Loss: 0.1701\n",
      "Epoch [460/2000] | Train Acc: 96.28% | Test Acc: 94.23% | Loss: 0.1694\n",
      "Epoch [461/2000] | Train Acc: 96.40% | Test Acc: 93.78% | Loss: 0.1686\n",
      "Epoch [462/2000] | Train Acc: 96.34% | Test Acc: 94.29% | Loss: 0.1675\n",
      "Epoch [463/2000] | Train Acc: 96.53% | Test Acc: 94.19% | Loss: 0.1665\n",
      "Epoch [464/2000] | Train Acc: 96.55% | Test Acc: 94.10% | Loss: 0.1657\n",
      "Epoch [465/2000] | Train Acc: 96.51% | Test Acc: 94.24% | Loss: 0.1652\n",
      "Epoch [466/2000] | Train Acc: 96.55% | Test Acc: 93.78% | Loss: 0.1648\n",
      "Epoch [467/2000] | Train Acc: 96.40% | Test Acc: 94.24% | Loss: 0.1643\n",
      "Epoch [468/2000] | Train Acc: 96.54% | Test Acc: 93.83% | Loss: 0.1637\n",
      "Epoch [469/2000] | Train Acc: 96.43% | Test Acc: 94.27% | Loss: 0.1629\n",
      "Epoch [470/2000] | Train Acc: 96.63% | Test Acc: 94.16% | Loss: 0.1620\n",
      "Epoch [471/2000] | Train Acc: 96.61% | Test Acc: 94.13% | Loss: 0.1613\n",
      "Epoch [472/2000] | Train Acc: 96.59% | Test Acc: 94.29% | Loss: 0.1607\n",
      "Epoch [473/2000] | Train Acc: 96.65% | Test Acc: 93.98% | Loss: 0.1602\n",
      "Epoch [474/2000] | Train Acc: 96.58% | Test Acc: 94.30% | Loss: 0.1597\n",
      "Epoch [475/2000] | Train Acc: 96.67% | Test Acc: 93.99% | Loss: 0.1592\n",
      "Epoch [476/2000] | Train Acc: 96.61% | Test Acc: 94.30% | Loss: 0.1585\n",
      "Epoch [477/2000] | Train Acc: 96.70% | Test Acc: 94.11% | Loss: 0.1578\n",
      "Epoch [478/2000] | Train Acc: 96.67% | Test Acc: 94.19% | Loss: 0.1571\n",
      "Epoch [479/2000] | Train Acc: 96.71% | Test Acc: 94.20% | Loss: 0.1565\n",
      "Epoch [480/2000] | Train Acc: 96.73% | Test Acc: 94.14% | Loss: 0.1559\n",
      "Epoch [481/2000] | Train Acc: 96.72% | Test Acc: 94.36% | Loss: 0.1554\n",
      "Epoch [482/2000] | Train Acc: 96.77% | Test Acc: 94.11% | Loss: 0.1549\n",
      "Epoch [483/2000] | Train Acc: 96.74% | Test Acc: 94.33% | Loss: 0.1543\n",
      "Epoch [484/2000] | Train Acc: 96.78% | Test Acc: 94.19% | Loss: 0.1538\n",
      "Epoch [485/2000] | Train Acc: 96.80% | Test Acc: 94.30% | Loss: 0.1532\n",
      "Epoch [486/2000] | Train Acc: 96.84% | Test Acc: 94.11% | Loss: 0.1526\n",
      "Epoch [487/2000] | Train Acc: 96.80% | Test Acc: 94.24% | Loss: 0.1520\n",
      "Epoch [488/2000] | Train Acc: 96.83% | Test Acc: 94.19% | Loss: 0.1514\n",
      "Epoch [489/2000] | Train Acc: 96.84% | Test Acc: 94.19% | Loss: 0.1508\n",
      "Epoch [490/2000] | Train Acc: 96.85% | Test Acc: 94.24% | Loss: 0.1503\n",
      "Epoch [491/2000] | Train Acc: 96.89% | Test Acc: 94.11% | Loss: 0.1497\n",
      "Epoch [492/2000] | Train Acc: 96.86% | Test Acc: 94.26% | Loss: 0.1492\n",
      "Epoch [493/2000] | Train Acc: 96.87% | Test Acc: 94.14% | Loss: 0.1487\n",
      "Epoch [494/2000] | Train Acc: 96.89% | Test Acc: 94.32% | Loss: 0.1482\n",
      "Epoch [495/2000] | Train Acc: 96.92% | Test Acc: 94.17% | Loss: 0.1477\n",
      "Epoch [496/2000] | Train Acc: 96.92% | Test Acc: 94.32% | Loss: 0.1472\n",
      "Epoch [497/2000] | Train Acc: 96.94% | Test Acc: 94.08% | Loss: 0.1468\n",
      "Epoch [498/2000] | Train Acc: 96.88% | Test Acc: 94.27% | Loss: 0.1463\n",
      "Epoch [499/2000] | Train Acc: 96.92% | Test Acc: 93.86% | Loss: 0.1460\n",
      "Epoch [500/2000] | Train Acc: 96.83% | Test Acc: 94.32% | Loss: 0.1456\n",
      "Epoch [501/2000] | Train Acc: 96.95% | Test Acc: 93.90% | Loss: 0.1452\n",
      "Epoch [502/2000] | Train Acc: 96.86% | Test Acc: 94.35% | Loss: 0.1448\n",
      "Epoch [503/2000] | Train Acc: 96.97% | Test Acc: 93.89% | Loss: 0.1444\n",
      "Epoch [504/2000] | Train Acc: 96.87% | Test Acc: 94.27% | Loss: 0.1437\n",
      "Epoch [505/2000] | Train Acc: 96.99% | Test Acc: 94.21% | Loss: 0.1430\n",
      "Epoch [506/2000] | Train Acc: 97.00% | Test Acc: 94.29% | Loss: 0.1423\n",
      "Epoch [507/2000] | Train Acc: 97.01% | Test Acc: 94.27% | Loss: 0.1416\n",
      "Epoch [508/2000] | Train Acc: 97.03% | Test Acc: 94.21% | Loss: 0.1411\n",
      "Epoch [509/2000] | Train Acc: 97.01% | Test Acc: 94.41% | Loss: 0.1406\n",
      "Epoch [510/2000] | Train Acc: 97.04% | Test Acc: 94.23% | Loss: 0.1403\n",
      "Epoch [511/2000] | Train Acc: 97.01% | Test Acc: 94.35% | Loss: 0.1400\n",
      "Epoch [512/2000] | Train Acc: 97.03% | Test Acc: 94.11% | Loss: 0.1396\n",
      "Epoch [513/2000] | Train Acc: 97.02% | Test Acc: 94.38% | Loss: 0.1391\n",
      "Epoch [514/2000] | Train Acc: 97.05% | Test Acc: 94.23% | Loss: 0.1386\n",
      "Epoch [515/2000] | Train Acc: 97.07% | Test Acc: 94.36% | Loss: 0.1380\n",
      "Epoch [516/2000] | Train Acc: 97.08% | Test Acc: 94.24% | Loss: 0.1374\n",
      "Epoch [517/2000] | Train Acc: 97.06% | Test Acc: 94.26% | Loss: 0.1369\n",
      "Epoch [518/2000] | Train Acc: 97.09% | Test Acc: 94.29% | Loss: 0.1364\n",
      "Epoch [519/2000] | Train Acc: 97.10% | Test Acc: 94.24% | Loss: 0.1360\n",
      "Epoch [520/2000] | Train Acc: 97.11% | Test Acc: 94.39% | Loss: 0.1356\n",
      "Epoch [521/2000] | Train Acc: 97.11% | Test Acc: 94.21% | Loss: 0.1353\n",
      "Epoch [522/2000] | Train Acc: 97.10% | Test Acc: 94.38% | Loss: 0.1349\n",
      "Epoch [523/2000] | Train Acc: 97.10% | Test Acc: 94.19% | Loss: 0.1345\n",
      "Epoch [524/2000] | Train Acc: 97.11% | Test Acc: 94.41% | Loss: 0.1341\n",
      "Epoch [525/2000] | Train Acc: 97.12% | Test Acc: 94.20% | Loss: 0.1336\n",
      "Epoch [526/2000] | Train Acc: 97.13% | Test Acc: 94.38% | Loss: 0.1332\n",
      "Epoch [527/2000] | Train Acc: 97.14% | Test Acc: 94.21% | Loss: 0.1327\n",
      "Epoch [528/2000] | Train Acc: 97.14% | Test Acc: 94.38% | Loss: 0.1322\n",
      "Epoch [529/2000] | Train Acc: 97.15% | Test Acc: 94.20% | Loss: 0.1318\n",
      "Epoch [530/2000] | Train Acc: 97.14% | Test Acc: 94.29% | Loss: 0.1313\n",
      "Epoch [531/2000] | Train Acc: 97.17% | Test Acc: 94.27% | Loss: 0.1309\n",
      "Epoch [532/2000] | Train Acc: 97.17% | Test Acc: 94.29% | Loss: 0.1305\n",
      "Epoch [533/2000] | Train Acc: 97.17% | Test Acc: 94.29% | Loss: 0.1301\n",
      "Epoch [534/2000] | Train Acc: 97.18% | Test Acc: 94.23% | Loss: 0.1297\n",
      "Epoch [535/2000] | Train Acc: 97.19% | Test Acc: 94.32% | Loss: 0.1293\n",
      "Epoch [536/2000] | Train Acc: 97.19% | Test Acc: 94.20% | Loss: 0.1289\n",
      "Epoch [537/2000] | Train Acc: 97.17% | Test Acc: 94.33% | Loss: 0.1285\n",
      "Epoch [538/2000] | Train Acc: 97.21% | Test Acc: 94.21% | Loss: 0.1281\n",
      "Epoch [539/2000] | Train Acc: 97.19% | Test Acc: 94.42% | Loss: 0.1278\n",
      "Epoch [540/2000] | Train Acc: 97.21% | Test Acc: 94.23% | Loss: 0.1274\n",
      "Epoch [541/2000] | Train Acc: 97.22% | Test Acc: 94.35% | Loss: 0.1272\n",
      "Epoch [542/2000] | Train Acc: 97.19% | Test Acc: 94.16% | Loss: 0.1269\n",
      "Epoch [543/2000] | Train Acc: 97.19% | Test Acc: 94.33% | Loss: 0.1268\n",
      "Epoch [544/2000] | Train Acc: 97.18% | Test Acc: 93.89% | Loss: 0.1270\n",
      "Epoch [545/2000] | Train Acc: 97.07% | Test Acc: 94.36% | Loss: 0.1271\n",
      "Epoch [546/2000] | Train Acc: 97.12% | Test Acc: 93.83% | Loss: 0.1275\n",
      "Epoch [547/2000] | Train Acc: 97.04% | Test Acc: 94.38% | Loss: 0.1269\n",
      "Epoch [548/2000] | Train Acc: 97.19% | Test Acc: 94.19% | Loss: 0.1258\n",
      "Epoch [549/2000] | Train Acc: 97.20% | Test Acc: 94.23% | Loss: 0.1244\n",
      "Epoch [550/2000] | Train Acc: 97.26% | Test Acc: 94.36% | Loss: 0.1236\n",
      "Epoch [551/2000] | Train Acc: 97.24% | Test Acc: 93.93% | Loss: 0.1237\n",
      "Epoch [552/2000] | Train Acc: 97.15% | Test Acc: 94.35% | Loss: 0.1240\n",
      "Epoch [553/2000] | Train Acc: 97.22% | Test Acc: 94.11% | Loss: 0.1239\n",
      "Epoch [554/2000] | Train Acc: 97.23% | Test Acc: 94.39% | Loss: 0.1229\n",
      "Epoch [555/2000] | Train Acc: 97.27% | Test Acc: 94.38% | Loss: 0.1220\n",
      "Epoch [556/2000] | Train Acc: 97.27% | Test Acc: 94.20% | Loss: 0.1216\n",
      "Epoch [557/2000] | Train Acc: 97.26% | Test Acc: 94.32% | Loss: 0.1217\n",
      "Epoch [558/2000] | Train Acc: 97.25% | Test Acc: 94.17% | Loss: 0.1217\n",
      "Epoch [559/2000] | Train Acc: 97.22% | Test Acc: 94.39% | Loss: 0.1211\n",
      "Epoch [560/2000] | Train Acc: 97.29% | Test Acc: 94.29% | Loss: 0.1204\n",
      "Epoch [561/2000] | Train Acc: 97.27% | Test Acc: 94.20% | Loss: 0.1199\n",
      "Epoch [562/2000] | Train Acc: 97.27% | Test Acc: 94.33% | Loss: 0.1198\n",
      "Epoch [563/2000] | Train Acc: 97.25% | Test Acc: 94.19% | Loss: 0.1197\n",
      "Epoch [564/2000] | Train Acc: 97.26% | Test Acc: 94.36% | Loss: 0.1194\n",
      "Epoch [565/2000] | Train Acc: 97.28% | Test Acc: 94.27% | Loss: 0.1188\n",
      "Epoch [566/2000] | Train Acc: 97.28% | Test Acc: 94.20% | Loss: 0.1183\n",
      "Epoch [567/2000] | Train Acc: 97.26% | Test Acc: 94.36% | Loss: 0.1181\n",
      "Epoch [568/2000] | Train Acc: 97.28% | Test Acc: 94.17% | Loss: 0.1179\n",
      "Epoch [569/2000] | Train Acc: 97.27% | Test Acc: 94.35% | Loss: 0.1177\n",
      "Epoch [570/2000] | Train Acc: 97.29% | Test Acc: 94.20% | Loss: 0.1173\n",
      "Epoch [571/2000] | Train Acc: 97.28% | Test Acc: 94.30% | Loss: 0.1168\n",
      "Epoch [572/2000] | Train Acc: 97.29% | Test Acc: 94.36% | Loss: 0.1165\n",
      "Epoch [573/2000] | Train Acc: 97.32% | Test Acc: 94.21% | Loss: 0.1163\n",
      "Epoch [574/2000] | Train Acc: 97.28% | Test Acc: 94.38% | Loss: 0.1160\n",
      "Epoch [575/2000] | Train Acc: 97.30% | Test Acc: 94.26% | Loss: 0.1157\n",
      "Epoch [576/2000] | Train Acc: 97.28% | Test Acc: 94.29% | Loss: 0.1154\n",
      "Epoch [577/2000] | Train Acc: 97.31% | Test Acc: 94.29% | Loss: 0.1150\n",
      "Epoch [578/2000] | Train Acc: 97.31% | Test Acc: 94.24% | Loss: 0.1147\n",
      "Epoch [579/2000] | Train Acc: 97.29% | Test Acc: 94.44% | Loss: 0.1144\n",
      "Epoch [580/2000] | Train Acc: 97.33% | Test Acc: 94.24% | Loss: 0.1142\n",
      "Epoch [581/2000] | Train Acc: 97.29% | Test Acc: 94.42% | Loss: 0.1139\n",
      "Epoch [582/2000] | Train Acc: 97.34% | Test Acc: 94.24% | Loss: 0.1136\n",
      "Epoch [583/2000] | Train Acc: 97.29% | Test Acc: 94.30% | Loss: 0.1133\n",
      "Epoch [584/2000] | Train Acc: 97.32% | Test Acc: 94.30% | Loss: 0.1130\n",
      "Epoch [585/2000] | Train Acc: 97.32% | Test Acc: 94.29% | Loss: 0.1127\n",
      "Epoch [586/2000] | Train Acc: 97.33% | Test Acc: 94.33% | Loss: 0.1124\n",
      "Epoch [587/2000] | Train Acc: 97.33% | Test Acc: 94.23% | Loss: 0.1122\n",
      "Epoch [588/2000] | Train Acc: 97.29% | Test Acc: 94.38% | Loss: 0.1119\n",
      "Epoch [589/2000] | Train Acc: 97.33% | Test Acc: 94.24% | Loss: 0.1117\n",
      "Epoch [590/2000] | Train Acc: 97.30% | Test Acc: 94.32% | Loss: 0.1114\n",
      "Epoch [591/2000] | Train Acc: 97.34% | Test Acc: 94.27% | Loss: 0.1111\n",
      "Epoch [592/2000] | Train Acc: 97.33% | Test Acc: 94.30% | Loss: 0.1108\n",
      "Epoch [593/2000] | Train Acc: 97.32% | Test Acc: 94.29% | Loss: 0.1105\n",
      "Epoch [594/2000] | Train Acc: 97.33% | Test Acc: 94.27% | Loss: 0.1103\n",
      "Epoch [595/2000] | Train Acc: 97.33% | Test Acc: 94.30% | Loss: 0.1100\n",
      "Epoch [596/2000] | Train Acc: 97.32% | Test Acc: 94.27% | Loss: 0.1098\n",
      "Epoch [597/2000] | Train Acc: 97.33% | Test Acc: 94.33% | Loss: 0.1095\n",
      "Epoch [598/2000] | Train Acc: 97.33% | Test Acc: 94.26% | Loss: 0.1093\n",
      "Epoch [599/2000] | Train Acc: 97.32% | Test Acc: 94.35% | Loss: 0.1090\n",
      "Epoch [600/2000] | Train Acc: 97.34% | Test Acc: 94.24% | Loss: 0.1088\n",
      "Epoch [601/2000] | Train Acc: 97.31% | Test Acc: 94.39% | Loss: 0.1085\n",
      "Epoch [602/2000] | Train Acc: 97.33% | Test Acc: 94.23% | Loss: 0.1083\n",
      "Epoch [603/2000] | Train Acc: 97.31% | Test Acc: 94.47% | Loss: 0.1081\n",
      "Epoch [604/2000] | Train Acc: 97.35% | Test Acc: 94.20% | Loss: 0.1078\n",
      "Epoch [605/2000] | Train Acc: 97.31% | Test Acc: 94.38% | Loss: 0.1076\n",
      "Epoch [606/2000] | Train Acc: 97.34% | Test Acc: 94.20% | Loss: 0.1074\n",
      "Epoch [607/2000] | Train Acc: 97.32% | Test Acc: 94.39% | Loss: 0.1072\n",
      "Epoch [608/2000] | Train Acc: 97.34% | Test Acc: 94.26% | Loss: 0.1071\n",
      "Epoch [609/2000] | Train Acc: 97.35% | Test Acc: 94.41% | Loss: 0.1070\n",
      "Epoch [610/2000] | Train Acc: 97.33% | Test Acc: 94.32% | Loss: 0.1070\n",
      "Epoch [611/2000] | Train Acc: 97.38% | Test Acc: 94.48% | Loss: 0.1070\n",
      "Epoch [612/2000] | Train Acc: 97.36% | Test Acc: 94.26% | Loss: 0.1071\n",
      "Epoch [613/2000] | Train Acc: 97.37% | Test Acc: 94.48% | Loss: 0.1069\n",
      "Epoch [614/2000] | Train Acc: 97.36% | Test Acc: 94.32% | Loss: 0.1067\n",
      "Epoch [615/2000] | Train Acc: 97.39% | Test Acc: 94.42% | Loss: 0.1060\n",
      "Epoch [616/2000] | Train Acc: 97.35% | Test Acc: 94.23% | Loss: 0.1054\n",
      "Epoch [617/2000] | Train Acc: 97.34% | Test Acc: 94.29% | Loss: 0.1048\n",
      "Epoch [618/2000] | Train Acc: 97.36% | Test Acc: 94.41% | Loss: 0.1044\n",
      "Epoch [619/2000] | Train Acc: 97.37% | Test Acc: 94.26% | Loss: 0.1044\n",
      "Epoch [620/2000] | Train Acc: 97.34% | Test Acc: 94.41% | Loss: 0.1044\n",
      "Epoch [621/2000] | Train Acc: 97.35% | Test Acc: 94.27% | Loss: 0.1044\n",
      "Epoch [622/2000] | Train Acc: 97.38% | Test Acc: 94.42% | Loss: 0.1042\n",
      "Epoch [623/2000] | Train Acc: 97.37% | Test Acc: 94.27% | Loss: 0.1038\n",
      "Epoch [624/2000] | Train Acc: 97.34% | Test Acc: 94.44% | Loss: 0.1034\n",
      "Epoch [625/2000] | Train Acc: 97.40% | Test Acc: 94.32% | Loss: 0.1030\n",
      "Epoch [626/2000] | Train Acc: 97.38% | Test Acc: 94.32% | Loss: 0.1027\n",
      "Epoch [627/2000] | Train Acc: 97.36% | Test Acc: 94.42% | Loss: 0.1026\n",
      "Epoch [628/2000] | Train Acc: 97.39% | Test Acc: 94.26% | Loss: 0.1025\n",
      "Epoch [629/2000] | Train Acc: 97.34% | Test Acc: 94.45% | Loss: 0.1024\n",
      "Epoch [630/2000] | Train Acc: 97.39% | Test Acc: 94.26% | Loss: 0.1022\n",
      "Epoch [631/2000] | Train Acc: 97.34% | Test Acc: 94.45% | Loss: 0.1019\n",
      "Epoch [632/2000] | Train Acc: 97.39% | Test Acc: 94.35% | Loss: 0.1016\n",
      "Epoch [633/2000] | Train Acc: 97.38% | Test Acc: 94.36% | Loss: 0.1013\n",
      "Epoch [634/2000] | Train Acc: 97.41% | Test Acc: 94.36% | Loss: 0.1011\n",
      "Epoch [635/2000] | Train Acc: 97.41% | Test Acc: 94.36% | Loss: 0.1008\n",
      "Epoch [636/2000] | Train Acc: 97.40% | Test Acc: 94.50% | Loss: 0.1007\n",
      "Epoch [637/2000] | Train Acc: 97.41% | Test Acc: 94.33% | Loss: 0.1005\n",
      "Epoch [638/2000] | Train Acc: 97.38% | Test Acc: 94.53% | Loss: 0.1004\n",
      "Epoch [639/2000] | Train Acc: 97.39% | Test Acc: 94.35% | Loss: 0.1002\n",
      "Epoch [640/2000] | Train Acc: 97.38% | Test Acc: 94.53% | Loss: 0.1000\n",
      "Epoch [641/2000] | Train Acc: 97.40% | Test Acc: 94.35% | Loss: 0.0998\n",
      "Epoch [642/2000] | Train Acc: 97.39% | Test Acc: 94.54% | Loss: 0.0996\n",
      "Epoch [643/2000] | Train Acc: 97.43% | Test Acc: 94.39% | Loss: 0.0994\n",
      "Epoch [644/2000] | Train Acc: 97.40% | Test Acc: 94.45% | Loss: 0.0991\n",
      "Epoch [645/2000] | Train Acc: 97.40% | Test Acc: 94.41% | Loss: 0.0989\n",
      "Epoch [646/2000] | Train Acc: 97.41% | Test Acc: 94.42% | Loss: 0.0987\n",
      "Epoch [647/2000] | Train Acc: 97.42% | Test Acc: 94.44% | Loss: 0.0985\n",
      "Epoch [648/2000] | Train Acc: 97.41% | Test Acc: 94.41% | Loss: 0.0983\n",
      "Epoch [649/2000] | Train Acc: 97.42% | Test Acc: 94.48% | Loss: 0.0981\n",
      "Epoch [650/2000] | Train Acc: 97.42% | Test Acc: 94.38% | Loss: 0.0980\n",
      "Epoch [651/2000] | Train Acc: 97.40% | Test Acc: 94.54% | Loss: 0.0978\n",
      "Epoch [652/2000] | Train Acc: 97.43% | Test Acc: 94.36% | Loss: 0.0977\n",
      "Epoch [653/2000] | Train Acc: 97.41% | Test Acc: 94.54% | Loss: 0.0975\n",
      "Epoch [654/2000] | Train Acc: 97.42% | Test Acc: 94.32% | Loss: 0.0974\n",
      "Epoch [655/2000] | Train Acc: 97.39% | Test Acc: 94.50% | Loss: 0.0973\n",
      "Epoch [656/2000] | Train Acc: 97.43% | Test Acc: 94.35% | Loss: 0.0972\n",
      "Epoch [657/2000] | Train Acc: 97.40% | Test Acc: 94.51% | Loss: 0.0972\n",
      "Epoch [658/2000] | Train Acc: 97.44% | Test Acc: 94.35% | Loss: 0.0972\n",
      "Epoch [659/2000] | Train Acc: 97.43% | Test Acc: 94.59% | Loss: 0.0973\n",
      "Epoch [660/2000] | Train Acc: 97.44% | Test Acc: 94.35% | Loss: 0.0974\n",
      "Epoch [661/2000] | Train Acc: 97.45% | Test Acc: 94.57% | Loss: 0.0973\n",
      "Epoch [662/2000] | Train Acc: 97.44% | Test Acc: 94.33% | Loss: 0.0971\n",
      "Epoch [663/2000] | Train Acc: 97.44% | Test Acc: 94.48% | Loss: 0.0966\n",
      "Epoch [664/2000] | Train Acc: 97.45% | Test Acc: 94.35% | Loss: 0.0960\n",
      "Epoch [665/2000] | Train Acc: 97.40% | Test Acc: 94.42% | Loss: 0.0954\n",
      "Epoch [666/2000] | Train Acc: 97.43% | Test Acc: 94.56% | Loss: 0.0951\n",
      "Epoch [667/2000] | Train Acc: 97.46% | Test Acc: 94.27% | Loss: 0.0950\n",
      "Epoch [668/2000] | Train Acc: 97.40% | Test Acc: 94.48% | Loss: 0.0951\n",
      "Epoch [669/2000] | Train Acc: 97.46% | Test Acc: 94.33% | Loss: 0.0952\n",
      "Epoch [670/2000] | Train Acc: 97.43% | Test Acc: 94.54% | Loss: 0.0951\n",
      "Epoch [671/2000] | Train Acc: 97.47% | Test Acc: 94.29% | Loss: 0.0949\n",
      "Epoch [672/2000] | Train Acc: 97.39% | Test Acc: 94.56% | Loss: 0.0945\n",
      "Epoch [673/2000] | Train Acc: 97.47% | Test Acc: 94.38% | Loss: 0.0941\n",
      "Epoch [674/2000] | Train Acc: 97.44% | Test Acc: 94.38% | Loss: 0.0938\n",
      "Epoch [675/2000] | Train Acc: 97.45% | Test Acc: 94.57% | Loss: 0.0937\n",
      "Epoch [676/2000] | Train Acc: 97.45% | Test Acc: 94.36% | Loss: 0.0936\n",
      "Epoch [677/2000] | Train Acc: 97.42% | Test Acc: 94.48% | Loss: 0.0935\n",
      "Epoch [678/2000] | Train Acc: 97.47% | Test Acc: 94.27% | Loss: 0.0935\n",
      "Epoch [679/2000] | Train Acc: 97.39% | Test Acc: 94.54% | Loss: 0.0934\n",
      "Epoch [680/2000] | Train Acc: 97.47% | Test Acc: 94.32% | Loss: 0.0932\n",
      "Epoch [681/2000] | Train Acc: 97.42% | Test Acc: 94.62% | Loss: 0.0929\n",
      "Epoch [682/2000] | Train Acc: 97.47% | Test Acc: 94.39% | Loss: 0.0927\n",
      "Epoch [683/2000] | Train Acc: 97.46% | Test Acc: 94.45% | Loss: 0.0925\n",
      "Epoch [684/2000] | Train Acc: 97.46% | Test Acc: 94.47% | Loss: 0.0923\n",
      "Epoch [685/2000] | Train Acc: 97.46% | Test Acc: 94.36% | Loss: 0.0921\n",
      "Epoch [686/2000] | Train Acc: 97.44% | Test Acc: 94.62% | Loss: 0.0920\n",
      "Epoch [687/2000] | Train Acc: 97.48% | Test Acc: 94.38% | Loss: 0.0919\n",
      "Epoch [688/2000] | Train Acc: 97.43% | Test Acc: 94.60% | Loss: 0.0918\n",
      "Epoch [689/2000] | Train Acc: 97.48% | Test Acc: 94.38% | Loss: 0.0917\n",
      "Epoch [690/2000] | Train Acc: 97.42% | Test Acc: 94.63% | Loss: 0.0916\n",
      "Epoch [691/2000] | Train Acc: 97.47% | Test Acc: 94.39% | Loss: 0.0914\n",
      "Epoch [692/2000] | Train Acc: 97.43% | Test Acc: 94.59% | Loss: 0.0913\n",
      "Epoch [693/2000] | Train Acc: 97.49% | Test Acc: 94.38% | Loss: 0.0911\n",
      "Epoch [694/2000] | Train Acc: 97.43% | Test Acc: 94.62% | Loss: 0.0909\n",
      "Epoch [695/2000] | Train Acc: 97.49% | Test Acc: 94.39% | Loss: 0.0907\n",
      "Epoch [696/2000] | Train Acc: 97.44% | Test Acc: 94.63% | Loss: 0.0906\n",
      "Epoch [697/2000] | Train Acc: 97.48% | Test Acc: 94.44% | Loss: 0.0904\n",
      "Epoch [698/2000] | Train Acc: 97.48% | Test Acc: 94.59% | Loss: 0.0902\n",
      "Epoch [699/2000] | Train Acc: 97.48% | Test Acc: 94.41% | Loss: 0.0901\n",
      "Epoch [700/2000] | Train Acc: 97.48% | Test Acc: 94.53% | Loss: 0.0899\n",
      "Epoch [701/2000] | Train Acc: 97.48% | Test Acc: 94.44% | Loss: 0.0898\n",
      "Epoch [702/2000] | Train Acc: 97.48% | Test Acc: 94.51% | Loss: 0.0897\n",
      "Epoch [703/2000] | Train Acc: 97.47% | Test Acc: 94.44% | Loss: 0.0895\n",
      "Epoch [704/2000] | Train Acc: 97.48% | Test Acc: 94.50% | Loss: 0.0894\n",
      "Epoch [705/2000] | Train Acc: 97.47% | Test Acc: 94.44% | Loss: 0.0892\n",
      "Epoch [706/2000] | Train Acc: 97.48% | Test Acc: 94.54% | Loss: 0.0891\n",
      "Epoch [707/2000] | Train Acc: 97.48% | Test Acc: 94.44% | Loss: 0.0890\n",
      "Epoch [708/2000] | Train Acc: 97.49% | Test Acc: 94.59% | Loss: 0.0888\n",
      "Epoch [709/2000] | Train Acc: 97.49% | Test Acc: 94.42% | Loss: 0.0887\n",
      "Epoch [710/2000] | Train Acc: 97.50% | Test Acc: 94.63% | Loss: 0.0886\n",
      "Epoch [711/2000] | Train Acc: 97.50% | Test Acc: 94.41% | Loss: 0.0885\n",
      "Epoch [712/2000] | Train Acc: 97.47% | Test Acc: 94.62% | Loss: 0.0884\n",
      "Epoch [713/2000] | Train Acc: 97.51% | Test Acc: 94.35% | Loss: 0.0883\n",
      "Epoch [714/2000] | Train Acc: 97.47% | Test Acc: 94.60% | Loss: 0.0883\n",
      "Epoch [715/2000] | Train Acc: 97.52% | Test Acc: 94.35% | Loss: 0.0885\n",
      "Epoch [716/2000] | Train Acc: 97.49% | Test Acc: 94.63% | Loss: 0.0890\n",
      "Epoch [717/2000] | Train Acc: 97.55% | Test Acc: 94.04% | Loss: 0.0900\n",
      "Epoch [718/2000] | Train Acc: 97.27% | Test Acc: 94.59% | Loss: 0.0909\n",
      "Epoch [719/2000] | Train Acc: 97.41% | Test Acc: 94.14% | Loss: 0.0921\n",
      "Epoch [720/2000] | Train Acc: 97.34% | Test Acc: 94.63% | Loss: 0.0905\n",
      "Epoch [721/2000] | Train Acc: 97.53% | Test Acc: 94.48% | Loss: 0.0884\n",
      "Epoch [722/2000] | Train Acc: 97.49% | Test Acc: 94.35% | Loss: 0.0871\n",
      "Epoch [723/2000] | Train Acc: 97.48% | Test Acc: 94.72% | Loss: 0.0877\n",
      "Epoch [724/2000] | Train Acc: 97.55% | Test Acc: 94.29% | Loss: 0.0891\n",
      "Epoch [725/2000] | Train Acc: 97.47% | Test Acc: 94.63% | Loss: 0.0888\n",
      "Epoch [726/2000] | Train Acc: 97.51% | Test Acc: 94.48% | Loss: 0.0875\n",
      "Epoch [727/2000] | Train Acc: 97.51% | Test Acc: 94.36% | Loss: 0.0865\n",
      "Epoch [728/2000] | Train Acc: 97.48% | Test Acc: 94.71% | Loss: 0.0869\n",
      "Epoch [729/2000] | Train Acc: 97.55% | Test Acc: 94.41% | Loss: 0.0878\n",
      "Epoch [730/2000] | Train Acc: 97.51% | Test Acc: 94.57% | Loss: 0.0872\n",
      "Epoch [731/2000] | Train Acc: 97.53% | Test Acc: 94.62% | Loss: 0.0862\n",
      "Epoch [732/2000] | Train Acc: 97.52% | Test Acc: 94.33% | Loss: 0.0860\n",
      "Epoch [733/2000] | Train Acc: 97.51% | Test Acc: 94.66% | Loss: 0.0865\n",
      "Epoch [734/2000] | Train Acc: 97.54% | Test Acc: 94.39% | Loss: 0.0866\n",
      "Epoch [735/2000] | Train Acc: 97.51% | Test Acc: 94.57% | Loss: 0.0859\n",
      "Epoch [736/2000] | Train Acc: 97.54% | Test Acc: 94.53% | Loss: 0.0854\n",
      "Epoch [737/2000] | Train Acc: 97.54% | Test Acc: 94.29% | Loss: 0.0856\n",
      "Epoch [738/2000] | Train Acc: 97.51% | Test Acc: 94.53% | Loss: 0.0858\n",
      "Epoch [739/2000] | Train Acc: 97.54% | Test Acc: 94.50% | Loss: 0.0855\n",
      "Epoch [740/2000] | Train Acc: 97.52% | Test Acc: 94.50% | Loss: 0.0850\n",
      "Epoch [741/2000] | Train Acc: 97.54% | Test Acc: 94.57% | Loss: 0.0849\n",
      "Epoch [742/2000] | Train Acc: 97.55% | Test Acc: 94.38% | Loss: 0.0851\n",
      "Epoch [743/2000] | Train Acc: 97.51% | Test Acc: 94.56% | Loss: 0.0850\n",
      "Epoch [744/2000] | Train Acc: 97.53% | Test Acc: 94.51% | Loss: 0.0847\n",
      "Epoch [745/2000] | Train Acc: 97.53% | Test Acc: 94.44% | Loss: 0.0844\n",
      "Epoch [746/2000] | Train Acc: 97.51% | Test Acc: 94.59% | Loss: 0.0844\n",
      "Epoch [747/2000] | Train Acc: 97.54% | Test Acc: 94.48% | Loss: 0.0845\n",
      "Epoch [748/2000] | Train Acc: 97.52% | Test Acc: 94.66% | Loss: 0.0843\n",
      "Epoch [749/2000] | Train Acc: 97.54% | Test Acc: 94.54% | Loss: 0.0840\n",
      "Epoch [750/2000] | Train Acc: 97.52% | Test Acc: 94.44% | Loss: 0.0839\n",
      "Epoch [751/2000] | Train Acc: 97.50% | Test Acc: 94.56% | Loss: 0.0839\n",
      "Epoch [752/2000] | Train Acc: 97.54% | Test Acc: 94.47% | Loss: 0.0838\n",
      "Epoch [753/2000] | Train Acc: 97.54% | Test Acc: 94.65% | Loss: 0.0836\n",
      "Epoch [754/2000] | Train Acc: 97.54% | Test Acc: 94.59% | Loss: 0.0835\n",
      "Epoch [755/2000] | Train Acc: 97.53% | Test Acc: 94.48% | Loss: 0.0833\n",
      "Epoch [756/2000] | Train Acc: 97.56% | Test Acc: 94.62% | Loss: 0.0833\n",
      "Epoch [757/2000] | Train Acc: 97.55% | Test Acc: 94.50% | Loss: 0.0832\n",
      "Epoch [758/2000] | Train Acc: 97.55% | Test Acc: 94.62% | Loss: 0.0831\n",
      "Epoch [759/2000] | Train Acc: 97.54% | Test Acc: 94.57% | Loss: 0.0829\n",
      "Epoch [760/2000] | Train Acc: 97.54% | Test Acc: 94.47% | Loss: 0.0828\n",
      "Epoch [761/2000] | Train Acc: 97.55% | Test Acc: 94.57% | Loss: 0.0827\n",
      "Epoch [762/2000] | Train Acc: 97.54% | Test Acc: 94.50% | Loss: 0.0827\n",
      "Epoch [763/2000] | Train Acc: 97.54% | Test Acc: 94.62% | Loss: 0.0826\n",
      "Epoch [764/2000] | Train Acc: 97.55% | Test Acc: 94.57% | Loss: 0.0824\n",
      "Epoch [765/2000] | Train Acc: 97.54% | Test Acc: 94.56% | Loss: 0.0823\n",
      "Epoch [766/2000] | Train Acc: 97.56% | Test Acc: 94.63% | Loss: 0.0822\n",
      "Epoch [767/2000] | Train Acc: 97.55% | Test Acc: 94.54% | Loss: 0.0821\n",
      "Epoch [768/2000] | Train Acc: 97.56% | Test Acc: 94.65% | Loss: 0.0820\n",
      "Epoch [769/2000] | Train Acc: 97.56% | Test Acc: 94.51% | Loss: 0.0819\n",
      "Epoch [770/2000] | Train Acc: 97.57% | Test Acc: 94.59% | Loss: 0.0818\n",
      "Epoch [771/2000] | Train Acc: 97.57% | Test Acc: 94.62% | Loss: 0.0817\n",
      "Epoch [772/2000] | Train Acc: 97.57% | Test Acc: 94.54% | Loss: 0.0816\n",
      "Epoch [773/2000] | Train Acc: 97.58% | Test Acc: 94.65% | Loss: 0.0815\n",
      "Epoch [774/2000] | Train Acc: 97.57% | Test Acc: 94.50% | Loss: 0.0814\n",
      "Epoch [775/2000] | Train Acc: 97.56% | Test Acc: 94.63% | Loss: 0.0813\n",
      "Epoch [776/2000] | Train Acc: 97.57% | Test Acc: 94.57% | Loss: 0.0812\n",
      "Epoch [777/2000] | Train Acc: 97.58% | Test Acc: 94.59% | Loss: 0.0811\n",
      "Epoch [778/2000] | Train Acc: 97.58% | Test Acc: 94.60% | Loss: 0.0810\n",
      "Epoch [779/2000] | Train Acc: 97.59% | Test Acc: 94.54% | Loss: 0.0809\n",
      "Epoch [780/2000] | Train Acc: 97.57% | Test Acc: 94.65% | Loss: 0.0808\n",
      "Epoch [781/2000] | Train Acc: 97.58% | Test Acc: 94.59% | Loss: 0.0807\n",
      "Epoch [782/2000] | Train Acc: 97.59% | Test Acc: 94.63% | Loss: 0.0807\n",
      "Epoch [783/2000] | Train Acc: 97.59% | Test Acc: 94.57% | Loss: 0.0806\n",
      "Epoch [784/2000] | Train Acc: 97.58% | Test Acc: 94.62% | Loss: 0.0805\n",
      "Epoch [785/2000] | Train Acc: 97.60% | Test Acc: 94.60% | Loss: 0.0804\n",
      "Epoch [786/2000] | Train Acc: 97.58% | Test Acc: 94.59% | Loss: 0.0803\n",
      "Epoch [787/2000] | Train Acc: 97.59% | Test Acc: 94.59% | Loss: 0.0802\n",
      "Epoch [788/2000] | Train Acc: 97.60% | Test Acc: 94.59% | Loss: 0.0801\n",
      "Epoch [789/2000] | Train Acc: 97.58% | Test Acc: 94.62% | Loss: 0.0800\n",
      "Epoch [790/2000] | Train Acc: 97.60% | Test Acc: 94.54% | Loss: 0.0799\n",
      "Epoch [791/2000] | Train Acc: 97.60% | Test Acc: 94.62% | Loss: 0.0798\n",
      "Epoch [792/2000] | Train Acc: 97.60% | Test Acc: 94.54% | Loss: 0.0797\n",
      "Epoch [793/2000] | Train Acc: 97.60% | Test Acc: 94.60% | Loss: 0.0796\n",
      "Epoch [794/2000] | Train Acc: 97.61% | Test Acc: 94.56% | Loss: 0.0795\n",
      "Epoch [795/2000] | Train Acc: 97.60% | Test Acc: 94.62% | Loss: 0.0795\n",
      "Epoch [796/2000] | Train Acc: 97.61% | Test Acc: 94.57% | Loss: 0.0794\n",
      "Epoch [797/2000] | Train Acc: 97.60% | Test Acc: 94.63% | Loss: 0.0793\n",
      "Epoch [798/2000] | Train Acc: 97.61% | Test Acc: 94.60% | Loss: 0.0792\n",
      "Epoch [799/2000] | Train Acc: 97.61% | Test Acc: 94.65% | Loss: 0.0791\n",
      "Epoch [800/2000] | Train Acc: 97.61% | Test Acc: 94.60% | Loss: 0.0790\n",
      "Epoch [801/2000] | Train Acc: 97.61% | Test Acc: 94.65% | Loss: 0.0789\n",
      "Epoch [802/2000] | Train Acc: 97.62% | Test Acc: 94.60% | Loss: 0.0788\n",
      "Epoch [803/2000] | Train Acc: 97.61% | Test Acc: 94.65% | Loss: 0.0787\n",
      "Epoch [804/2000] | Train Acc: 97.62% | Test Acc: 94.60% | Loss: 0.0787\n",
      "Epoch [805/2000] | Train Acc: 97.61% | Test Acc: 94.66% | Loss: 0.0786\n",
      "Epoch [806/2000] | Train Acc: 97.62% | Test Acc: 94.59% | Loss: 0.0785\n",
      "Epoch [807/2000] | Train Acc: 97.61% | Test Acc: 94.66% | Loss: 0.0784\n",
      "Epoch [808/2000] | Train Acc: 97.62% | Test Acc: 94.57% | Loss: 0.0783\n",
      "Epoch [809/2000] | Train Acc: 97.62% | Test Acc: 94.62% | Loss: 0.0783\n",
      "Epoch [810/2000] | Train Acc: 97.61% | Test Acc: 94.56% | Loss: 0.0782\n",
      "Epoch [811/2000] | Train Acc: 97.62% | Test Acc: 94.60% | Loss: 0.0781\n",
      "Epoch [812/2000] | Train Acc: 97.60% | Test Acc: 94.53% | Loss: 0.0781\n",
      "Epoch [813/2000] | Train Acc: 97.59% | Test Acc: 94.60% | Loss: 0.0781\n",
      "Epoch [814/2000] | Train Acc: 97.63% | Test Acc: 94.45% | Loss: 0.0781\n",
      "Epoch [815/2000] | Train Acc: 97.56% | Test Acc: 94.68% | Loss: 0.0783\n",
      "Epoch [816/2000] | Train Acc: 97.65% | Test Acc: 94.35% | Loss: 0.0788\n",
      "Epoch [817/2000] | Train Acc: 97.59% | Test Acc: 94.71% | Loss: 0.0794\n",
      "Epoch [818/2000] | Train Acc: 97.68% | Test Acc: 94.24% | Loss: 0.0806\n",
      "Epoch [819/2000] | Train Acc: 97.44% | Test Acc: 94.72% | Loss: 0.0810\n",
      "Epoch [820/2000] | Train Acc: 97.65% | Test Acc: 94.39% | Loss: 0.0812\n",
      "Epoch [821/2000] | Train Acc: 97.61% | Test Acc: 94.59% | Loss: 0.0791\n",
      "Epoch [822/2000] | Train Acc: 97.64% | Test Acc: 94.63% | Loss: 0.0775\n",
      "Epoch [823/2000] | Train Acc: 97.62% | Test Acc: 94.38% | Loss: 0.0772\n",
      "Epoch [824/2000] | Train Acc: 97.61% | Test Acc: 94.66% | Loss: 0.0781\n",
      "Epoch [825/2000] | Train Acc: 97.67% | Test Acc: 94.35% | Loss: 0.0791\n",
      "Epoch [826/2000] | Train Acc: 97.59% | Test Acc: 94.62% | Loss: 0.0786\n",
      "Epoch [827/2000] | Train Acc: 97.64% | Test Acc: 94.59% | Loss: 0.0775\n",
      "Epoch [828/2000] | Train Acc: 97.64% | Test Acc: 94.47% | Loss: 0.0768\n",
      "Epoch [829/2000] | Train Acc: 97.57% | Test Acc: 94.66% | Loss: 0.0771\n",
      "Epoch [830/2000] | Train Acc: 97.66% | Test Acc: 94.39% | Loss: 0.0778\n",
      "Epoch [831/2000] | Train Acc: 97.60% | Test Acc: 94.63% | Loss: 0.0776\n",
      "Epoch [832/2000] | Train Acc: 97.65% | Test Acc: 94.56% | Loss: 0.0769\n",
      "Epoch [833/2000] | Train Acc: 97.64% | Test Acc: 94.53% | Loss: 0.0764\n",
      "Epoch [834/2000] | Train Acc: 97.60% | Test Acc: 94.68% | Loss: 0.0765\n",
      "Epoch [835/2000] | Train Acc: 97.65% | Test Acc: 94.48% | Loss: 0.0770\n",
      "Epoch [836/2000] | Train Acc: 97.59% | Test Acc: 94.68% | Loss: 0.0768\n",
      "Epoch [837/2000] | Train Acc: 97.66% | Test Acc: 94.63% | Loss: 0.0764\n",
      "Epoch [838/2000] | Train Acc: 97.65% | Test Acc: 94.60% | Loss: 0.0760\n",
      "Epoch [839/2000] | Train Acc: 97.60% | Test Acc: 94.66% | Loss: 0.0761\n",
      "Epoch [840/2000] | Train Acc: 97.67% | Test Acc: 94.57% | Loss: 0.0763\n",
      "Epoch [841/2000] | Train Acc: 97.61% | Test Acc: 94.66% | Loss: 0.0762\n",
      "Epoch [842/2000] | Train Acc: 97.64% | Test Acc: 94.69% | Loss: 0.0759\n",
      "Epoch [843/2000] | Train Acc: 97.66% | Test Acc: 94.62% | Loss: 0.0756\n",
      "Epoch [844/2000] | Train Acc: 97.65% | Test Acc: 94.63% | Loss: 0.0756\n",
      "Epoch [845/2000] | Train Acc: 97.64% | Test Acc: 94.60% | Loss: 0.0757\n",
      "Epoch [846/2000] | Train Acc: 97.61% | Test Acc: 94.65% | Loss: 0.0757\n",
      "Epoch [847/2000] | Train Acc: 97.64% | Test Acc: 94.68% | Loss: 0.0755\n",
      "Epoch [848/2000] | Train Acc: 97.65% | Test Acc: 94.63% | Loss: 0.0753\n",
      "Epoch [849/2000] | Train Acc: 97.66% | Test Acc: 94.65% | Loss: 0.0752\n",
      "Epoch [850/2000] | Train Acc: 97.63% | Test Acc: 94.60% | Loss: 0.0753\n",
      "Epoch [851/2000] | Train Acc: 97.62% | Test Acc: 94.65% | Loss: 0.0752\n",
      "Epoch [852/2000] | Train Acc: 97.63% | Test Acc: 94.66% | Loss: 0.0751\n",
      "Epoch [853/2000] | Train Acc: 97.66% | Test Acc: 94.77% | Loss: 0.0749\n",
      "Epoch [854/2000] | Train Acc: 97.66% | Test Acc: 94.62% | Loss: 0.0748\n",
      "Epoch [855/2000] | Train Acc: 97.63% | Test Acc: 94.65% | Loss: 0.0748\n",
      "Epoch [856/2000] | Train Acc: 97.66% | Test Acc: 94.63% | Loss: 0.0748\n",
      "Epoch [857/2000] | Train Acc: 97.63% | Test Acc: 94.71% | Loss: 0.0747\n",
      "Epoch [858/2000] | Train Acc: 97.67% | Test Acc: 94.69% | Loss: 0.0746\n",
      "Epoch [859/2000] | Train Acc: 97.66% | Test Acc: 94.72% | Loss: 0.0745\n",
      "Epoch [860/2000] | Train Acc: 97.67% | Test Acc: 94.69% | Loss: 0.0744\n",
      "Epoch [861/2000] | Train Acc: 97.66% | Test Acc: 94.62% | Loss: 0.0744\n",
      "Epoch [862/2000] | Train Acc: 97.63% | Test Acc: 94.72% | Loss: 0.0743\n",
      "Epoch [863/2000] | Train Acc: 97.65% | Test Acc: 94.60% | Loss: 0.0743\n",
      "Epoch [864/2000] | Train Acc: 97.63% | Test Acc: 94.69% | Loss: 0.0742\n",
      "Epoch [865/2000] | Train Acc: 97.67% | Test Acc: 94.72% | Loss: 0.0741\n",
      "Epoch [866/2000] | Train Acc: 97.67% | Test Acc: 94.72% | Loss: 0.0740\n",
      "Epoch [867/2000] | Train Acc: 97.67% | Test Acc: 94.69% | Loss: 0.0740\n",
      "Epoch [868/2000] | Train Acc: 97.67% | Test Acc: 94.65% | Loss: 0.0739\n",
      "Epoch [869/2000] | Train Acc: 97.65% | Test Acc: 94.69% | Loss: 0.0739\n",
      "Epoch [870/2000] | Train Acc: 97.66% | Test Acc: 94.66% | Loss: 0.0738\n",
      "Epoch [871/2000] | Train Acc: 97.64% | Test Acc: 94.74% | Loss: 0.0737\n",
      "Epoch [872/2000] | Train Acc: 97.68% | Test Acc: 94.74% | Loss: 0.0736\n",
      "Epoch [873/2000] | Train Acc: 97.67% | Test Acc: 94.74% | Loss: 0.0736\n",
      "Epoch [874/2000] | Train Acc: 97.68% | Test Acc: 94.72% | Loss: 0.0735\n",
      "Epoch [875/2000] | Train Acc: 97.68% | Test Acc: 94.71% | Loss: 0.0734\n",
      "Epoch [876/2000] | Train Acc: 97.66% | Test Acc: 94.71% | Loss: 0.0734\n",
      "Epoch [877/2000] | Train Acc: 97.68% | Test Acc: 94.69% | Loss: 0.0733\n",
      "Epoch [878/2000] | Train Acc: 97.66% | Test Acc: 94.71% | Loss: 0.0733\n",
      "Epoch [879/2000] | Train Acc: 97.68% | Test Acc: 94.74% | Loss: 0.0732\n",
      "Epoch [880/2000] | Train Acc: 97.66% | Test Acc: 94.72% | Loss: 0.0731\n",
      "Epoch [881/2000] | Train Acc: 97.69% | Test Acc: 94.74% | Loss: 0.0731\n",
      "Epoch [882/2000] | Train Acc: 97.67% | Test Acc: 94.77% | Loss: 0.0730\n",
      "Epoch [883/2000] | Train Acc: 97.70% | Test Acc: 94.72% | Loss: 0.0729\n",
      "Epoch [884/2000] | Train Acc: 97.68% | Test Acc: 94.74% | Loss: 0.0729\n",
      "Epoch [885/2000] | Train Acc: 97.68% | Test Acc: 94.72% | Loss: 0.0728\n",
      "Epoch [886/2000] | Train Acc: 97.69% | Test Acc: 94.74% | Loss: 0.0727\n",
      "Epoch [887/2000] | Train Acc: 97.67% | Test Acc: 94.72% | Loss: 0.0727\n",
      "Epoch [888/2000] | Train Acc: 97.69% | Test Acc: 94.74% | Loss: 0.0726\n",
      "Epoch [889/2000] | Train Acc: 97.68% | Test Acc: 94.72% | Loss: 0.0726\n",
      "Epoch [890/2000] | Train Acc: 97.69% | Test Acc: 94.72% | Loss: 0.0725\n",
      "Epoch [891/2000] | Train Acc: 97.67% | Test Acc: 94.71% | Loss: 0.0724\n",
      "Epoch [892/2000] | Train Acc: 97.68% | Test Acc: 94.72% | Loss: 0.0724\n",
      "Epoch [893/2000] | Train Acc: 97.67% | Test Acc: 94.69% | Loss: 0.0723\n",
      "Epoch [894/2000] | Train Acc: 97.68% | Test Acc: 94.66% | Loss: 0.0723\n",
      "Epoch [895/2000] | Train Acc: 97.66% | Test Acc: 94.69% | Loss: 0.0722\n",
      "Epoch [896/2000] | Train Acc: 97.68% | Test Acc: 94.66% | Loss: 0.0721\n",
      "Epoch [897/2000] | Train Acc: 97.66% | Test Acc: 94.68% | Loss: 0.0721\n",
      "Epoch [898/2000] | Train Acc: 97.68% | Test Acc: 94.68% | Loss: 0.0720\n",
      "Epoch [899/2000] | Train Acc: 97.66% | Test Acc: 94.72% | Loss: 0.0720\n",
      "Epoch [900/2000] | Train Acc: 97.69% | Test Acc: 94.69% | Loss: 0.0720\n",
      "Epoch [901/2000] | Train Acc: 97.68% | Test Acc: 94.68% | Loss: 0.0719\n",
      "Epoch [902/2000] | Train Acc: 97.67% | Test Acc: 94.69% | Loss: 0.0720\n",
      "Epoch [903/2000] | Train Acc: 97.70% | Test Acc: 94.62% | Loss: 0.0720\n",
      "Epoch [904/2000] | Train Acc: 97.65% | Test Acc: 94.72% | Loss: 0.0721\n",
      "Epoch [905/2000] | Train Acc: 97.73% | Test Acc: 94.57% | Loss: 0.0723\n",
      "Epoch [906/2000] | Train Acc: 97.66% | Test Acc: 94.81% | Loss: 0.0726\n",
      "Epoch [907/2000] | Train Acc: 97.77% | Test Acc: 94.53% | Loss: 0.0732\n",
      "Epoch [908/2000] | Train Acc: 97.64% | Test Acc: 94.79% | Loss: 0.0734\n",
      "Epoch [909/2000] | Train Acc: 97.77% | Test Acc: 94.53% | Loss: 0.0738\n",
      "Epoch [910/2000] | Train Acc: 97.65% | Test Acc: 94.81% | Loss: 0.0733\n",
      "Epoch [911/2000] | Train Acc: 97.77% | Test Acc: 94.57% | Loss: 0.0726\n",
      "Epoch [912/2000] | Train Acc: 97.63% | Test Acc: 94.74% | Loss: 0.0716\n",
      "Epoch [913/2000] | Train Acc: 97.68% | Test Acc: 94.68% | Loss: 0.0711\n",
      "Epoch [914/2000] | Train Acc: 97.67% | Test Acc: 94.57% | Loss: 0.0712\n",
      "Epoch [915/2000] | Train Acc: 97.64% | Test Acc: 94.77% | Loss: 0.0716\n",
      "Epoch [916/2000] | Train Acc: 97.78% | Test Acc: 94.54% | Loss: 0.0720\n",
      "Epoch [917/2000] | Train Acc: 97.66% | Test Acc: 94.71% | Loss: 0.0720\n",
      "Epoch [918/2000] | Train Acc: 97.74% | Test Acc: 94.62% | Loss: 0.0717\n",
      "Epoch [919/2000] | Train Acc: 97.67% | Test Acc: 94.74% | Loss: 0.0711\n",
      "Epoch [920/2000] | Train Acc: 97.69% | Test Acc: 94.69% | Loss: 0.0708\n",
      "Epoch [921/2000] | Train Acc: 97.67% | Test Acc: 94.60% | Loss: 0.0708\n",
      "Epoch [922/2000] | Train Acc: 97.66% | Test Acc: 94.71% | Loss: 0.0710\n",
      "Epoch [923/2000] | Train Acc: 97.73% | Test Acc: 94.63% | Loss: 0.0712\n",
      "Epoch [924/2000] | Train Acc: 97.67% | Test Acc: 94.66% | Loss: 0.0711\n",
      "Epoch [925/2000] | Train Acc: 97.71% | Test Acc: 94.72% | Loss: 0.0709\n",
      "Epoch [926/2000] | Train Acc: 97.68% | Test Acc: 94.75% | Loss: 0.0706\n",
      "Epoch [927/2000] | Train Acc: 97.68% | Test Acc: 94.74% | Loss: 0.0704\n",
      "Epoch [928/2000] | Train Acc: 97.67% | Test Acc: 94.71% | Loss: 0.0704\n",
      "Epoch [929/2000] | Train Acc: 97.69% | Test Acc: 94.71% | Loss: 0.0704\n",
      "Epoch [930/2000] | Train Acc: 97.70% | Test Acc: 94.62% | Loss: 0.0705\n",
      "Epoch [931/2000] | Train Acc: 97.66% | Test Acc: 94.69% | Loss: 0.0704\n",
      "Epoch [932/2000] | Train Acc: 97.70% | Test Acc: 94.69% | Loss: 0.0703\n",
      "Epoch [933/2000] | Train Acc: 97.70% | Test Acc: 94.71% | Loss: 0.0701\n",
      "Epoch [934/2000] | Train Acc: 97.68% | Test Acc: 94.75% | Loss: 0.0700\n",
      "Epoch [935/2000] | Train Acc: 97.68% | Test Acc: 94.71% | Loss: 0.0699\n",
      "Epoch [936/2000] | Train Acc: 97.68% | Test Acc: 94.71% | Loss: 0.0699\n",
      "Epoch [937/2000] | Train Acc: 97.68% | Test Acc: 94.72% | Loss: 0.0699\n",
      "Epoch [938/2000] | Train Acc: 97.70% | Test Acc: 94.75% | Loss: 0.0699\n",
      "Epoch [939/2000] | Train Acc: 97.69% | Test Acc: 94.74% | Loss: 0.0699\n",
      "Epoch [940/2000] | Train Acc: 97.70% | Test Acc: 94.74% | Loss: 0.0698\n",
      "Epoch [941/2000] | Train Acc: 97.68% | Test Acc: 94.72% | Loss: 0.0697\n",
      "Epoch [942/2000] | Train Acc: 97.70% | Test Acc: 94.78% | Loss: 0.0696\n",
      "Epoch [943/2000] | Train Acc: 97.70% | Test Acc: 94.77% | Loss: 0.0695\n",
      "Epoch [944/2000] | Train Acc: 97.69% | Test Acc: 94.71% | Loss: 0.0695\n",
      "Epoch [945/2000] | Train Acc: 97.70% | Test Acc: 94.75% | Loss: 0.0694\n",
      "Epoch [946/2000] | Train Acc: 97.68% | Test Acc: 94.69% | Loss: 0.0694\n",
      "Epoch [947/2000] | Train Acc: 97.69% | Test Acc: 94.74% | Loss: 0.0694\n",
      "Epoch [948/2000] | Train Acc: 97.67% | Test Acc: 94.69% | Loss: 0.0693\n",
      "Epoch [949/2000] | Train Acc: 97.70% | Test Acc: 94.77% | Loss: 0.0693\n",
      "Epoch [950/2000] | Train Acc: 97.69% | Test Acc: 94.71% | Loss: 0.0692\n",
      "Epoch [951/2000] | Train Acc: 97.70% | Test Acc: 94.77% | Loss: 0.0692\n",
      "Epoch [952/2000] | Train Acc: 97.69% | Test Acc: 94.69% | Loss: 0.0691\n",
      "Epoch [953/2000] | Train Acc: 97.71% | Test Acc: 94.77% | Loss: 0.0690\n",
      "Epoch [954/2000] | Train Acc: 97.70% | Test Acc: 94.75% | Loss: 0.0690\n",
      "Epoch [955/2000] | Train Acc: 97.70% | Test Acc: 94.77% | Loss: 0.0689\n",
      "Epoch [956/2000] | Train Acc: 97.71% | Test Acc: 94.79% | Loss: 0.0689\n",
      "Epoch [957/2000] | Train Acc: 97.70% | Test Acc: 94.75% | Loss: 0.0688\n",
      "Epoch [958/2000] | Train Acc: 97.71% | Test Acc: 94.77% | Loss: 0.0688\n",
      "Epoch [959/2000] | Train Acc: 97.70% | Test Acc: 94.69% | Loss: 0.0687\n",
      "Epoch [960/2000] | Train Acc: 97.71% | Test Acc: 94.78% | Loss: 0.0687\n",
      "Epoch [961/2000] | Train Acc: 97.70% | Test Acc: 94.74% | Loss: 0.0686\n",
      "Epoch [962/2000] | Train Acc: 97.72% | Test Acc: 94.81% | Loss: 0.0686\n",
      "Epoch [963/2000] | Train Acc: 97.69% | Test Acc: 94.74% | Loss: 0.0686\n",
      "Epoch [964/2000] | Train Acc: 97.71% | Test Acc: 94.78% | Loss: 0.0685\n",
      "Epoch [965/2000] | Train Acc: 97.70% | Test Acc: 94.75% | Loss: 0.0685\n",
      "Epoch [966/2000] | Train Acc: 97.72% | Test Acc: 94.77% | Loss: 0.0685\n",
      "Epoch [967/2000] | Train Acc: 97.71% | Test Acc: 94.71% | Loss: 0.0685\n",
      "Epoch [968/2000] | Train Acc: 97.71% | Test Acc: 94.74% | Loss: 0.0685\n",
      "Epoch [969/2000] | Train Acc: 97.75% | Test Acc: 94.66% | Loss: 0.0686\n",
      "Epoch [970/2000] | Train Acc: 97.68% | Test Acc: 94.88% | Loss: 0.0688\n",
      "Epoch [971/2000] | Train Acc: 97.81% | Test Acc: 94.60% | Loss: 0.0691\n",
      "Epoch [972/2000] | Train Acc: 97.71% | Test Acc: 94.84% | Loss: 0.0693\n",
      "Epoch [973/2000] | Train Acc: 97.83% | Test Acc: 94.57% | Loss: 0.0698\n",
      "Epoch [974/2000] | Train Acc: 97.67% | Test Acc: 94.88% | Loss: 0.0699\n",
      "Epoch [975/2000] | Train Acc: 97.83% | Test Acc: 94.59% | Loss: 0.0701\n",
      "Epoch [976/2000] | Train Acc: 97.68% | Test Acc: 94.84% | Loss: 0.0695\n",
      "Epoch [977/2000] | Train Acc: 97.80% | Test Acc: 94.72% | Loss: 0.0688\n",
      "Epoch [978/2000] | Train Acc: 97.70% | Test Acc: 94.79% | Loss: 0.0681\n",
      "Epoch [979/2000] | Train Acc: 97.71% | Test Acc: 94.77% | Loss: 0.0678\n",
      "Epoch [980/2000] | Train Acc: 97.72% | Test Acc: 94.66% | Loss: 0.0679\n",
      "Epoch [981/2000] | Train Acc: 97.70% | Test Acc: 94.85% | Loss: 0.0682\n",
      "Epoch [982/2000] | Train Acc: 97.81% | Test Acc: 94.65% | Loss: 0.0685\n",
      "Epoch [983/2000] | Train Acc: 97.70% | Test Acc: 94.84% | Loss: 0.0685\n",
      "Epoch [984/2000] | Train Acc: 97.80% | Test Acc: 94.69% | Loss: 0.0683\n",
      "Epoch [985/2000] | Train Acc: 97.69% | Test Acc: 94.75% | Loss: 0.0679\n",
      "Epoch [986/2000] | Train Acc: 97.70% | Test Acc: 94.78% | Loss: 0.0676\n",
      "Epoch [987/2000] | Train Acc: 97.72% | Test Acc: 94.77% | Loss: 0.0674\n",
      "Epoch [988/2000] | Train Acc: 97.73% | Test Acc: 94.71% | Loss: 0.0675\n",
      "Epoch [989/2000] | Train Acc: 97.74% | Test Acc: 94.68% | Loss: 0.0677\n",
      "Epoch [990/2000] | Train Acc: 97.70% | Test Acc: 94.77% | Loss: 0.0677\n",
      "Epoch [991/2000] | Train Acc: 97.80% | Test Acc: 94.72% | Loss: 0.0677\n",
      "Epoch [992/2000] | Train Acc: 97.69% | Test Acc: 94.74% | Loss: 0.0675\n",
      "Epoch [993/2000] | Train Acc: 97.73% | Test Acc: 94.74% | Loss: 0.0673\n",
      "Epoch [994/2000] | Train Acc: 97.73% | Test Acc: 94.75% | Loss: 0.0671\n",
      "Epoch [995/2000] | Train Acc: 97.72% | Test Acc: 94.74% | Loss: 0.0671\n",
      "Epoch [996/2000] | Train Acc: 97.71% | Test Acc: 94.75% | Loss: 0.0671\n",
      "Epoch [997/2000] | Train Acc: 97.73% | Test Acc: 94.71% | Loss: 0.0671\n",
      "Epoch [998/2000] | Train Acc: 97.75% | Test Acc: 94.75% | Loss: 0.0672\n",
      "Epoch [999/2000] | Train Acc: 97.73% | Test Acc: 94.74% | Loss: 0.0671\n",
      "Epoch [1000/2000] | Train Acc: 97.74% | Test Acc: 94.77% | Loss: 0.0671\n",
      "Epoch [1001/2000] | Train Acc: 97.74% | Test Acc: 94.77% | Loss: 0.0669\n",
      "Epoch [1002/2000] | Train Acc: 97.72% | Test Acc: 94.74% | Loss: 0.0668\n",
      "Epoch [1003/2000] | Train Acc: 97.73% | Test Acc: 94.81% | Loss: 0.0667\n",
      "Epoch [1004/2000] | Train Acc: 97.72% | Test Acc: 94.79% | Loss: 0.0667\n",
      "Epoch [1005/2000] | Train Acc: 97.72% | Test Acc: 94.77% | Loss: 0.0667\n",
      "Epoch [1006/2000] | Train Acc: 97.73% | Test Acc: 94.77% | Loss: 0.0666\n",
      "Epoch [1007/2000] | Train Acc: 97.71% | Test Acc: 94.77% | Loss: 0.0666\n",
      "Epoch [1008/2000] | Train Acc: 97.75% | Test Acc: 94.78% | Loss: 0.0666\n",
      "Epoch [1009/2000] | Train Acc: 97.73% | Test Acc: 94.78% | Loss: 0.0666\n",
      "Epoch [1010/2000] | Train Acc: 97.75% | Test Acc: 94.79% | Loss: 0.0665\n",
      "Epoch [1011/2000] | Train Acc: 97.72% | Test Acc: 94.75% | Loss: 0.0665\n",
      "Epoch [1012/2000] | Train Acc: 97.74% | Test Acc: 94.81% | Loss: 0.0664\n",
      "Epoch [1013/2000] | Train Acc: 97.73% | Test Acc: 94.72% | Loss: 0.0663\n",
      "Epoch [1014/2000] | Train Acc: 97.72% | Test Acc: 94.78% | Loss: 0.0663\n",
      "Epoch [1015/2000] | Train Acc: 97.72% | Test Acc: 94.81% | Loss: 0.0662\n",
      "Epoch [1016/2000] | Train Acc: 97.72% | Test Acc: 94.81% | Loss: 0.0662\n",
      "Epoch [1017/2000] | Train Acc: 97.73% | Test Acc: 94.78% | Loss: 0.0661\n",
      "Epoch [1018/2000] | Train Acc: 97.72% | Test Acc: 94.75% | Loss: 0.0661\n",
      "Epoch [1019/2000] | Train Acc: 97.73% | Test Acc: 94.79% | Loss: 0.0661\n",
      "Epoch [1020/2000] | Train Acc: 97.73% | Test Acc: 94.75% | Loss: 0.0660\n",
      "Epoch [1021/2000] | Train Acc: 97.74% | Test Acc: 94.82% | Loss: 0.0660\n",
      "Epoch [1022/2000] | Train Acc: 97.73% | Test Acc: 94.77% | Loss: 0.0660\n",
      "Epoch [1023/2000] | Train Acc: 97.74% | Test Acc: 94.78% | Loss: 0.0659\n",
      "Epoch [1024/2000] | Train Acc: 97.72% | Test Acc: 94.74% | Loss: 0.0659\n",
      "Epoch [1025/2000] | Train Acc: 97.75% | Test Acc: 94.77% | Loss: 0.0659\n",
      "Epoch [1026/2000] | Train Acc: 97.75% | Test Acc: 94.77% | Loss: 0.0659\n",
      "Epoch [1027/2000] | Train Acc: 97.75% | Test Acc: 94.78% | Loss: 0.0659\n",
      "Epoch [1028/2000] | Train Acc: 97.79% | Test Acc: 94.72% | Loss: 0.0660\n",
      "Epoch [1029/2000] | Train Acc: 97.70% | Test Acc: 94.85% | Loss: 0.0661\n",
      "Epoch [1030/2000] | Train Acc: 97.82% | Test Acc: 94.74% | Loss: 0.0662\n",
      "Epoch [1031/2000] | Train Acc: 97.70% | Test Acc: 94.87% | Loss: 0.0663\n",
      "Epoch [1032/2000] | Train Acc: 97.84% | Test Acc: 94.68% | Loss: 0.0666\n",
      "Epoch [1033/2000] | Train Acc: 97.73% | Test Acc: 94.90% | Loss: 0.0667\n",
      "Epoch [1034/2000] | Train Acc: 97.85% | Test Acc: 94.65% | Loss: 0.0670\n",
      "Epoch [1035/2000] | Train Acc: 97.72% | Test Acc: 94.88% | Loss: 0.0668\n",
      "Epoch [1036/2000] | Train Acc: 97.86% | Test Acc: 94.71% | Loss: 0.0667\n",
      "Epoch [1037/2000] | Train Acc: 97.71% | Test Acc: 94.79% | Loss: 0.0662\n",
      "Epoch [1038/2000] | Train Acc: 97.81% | Test Acc: 94.75% | Loss: 0.0657\n",
      "Epoch [1039/2000] | Train Acc: 97.74% | Test Acc: 94.78% | Loss: 0.0654\n",
      "Epoch [1040/2000] | Train Acc: 97.72% | Test Acc: 94.81% | Loss: 0.0652\n",
      "Epoch [1041/2000] | Train Acc: 97.73% | Test Acc: 94.78% | Loss: 0.0653\n",
      "Epoch [1042/2000] | Train Acc: 97.74% | Test Acc: 94.85% | Loss: 0.0654\n",
      "Epoch [1043/2000] | Train Acc: 97.83% | Test Acc: 94.66% | Loss: 0.0656\n",
      "Epoch [1044/2000] | Train Acc: 97.70% | Test Acc: 94.85% | Loss: 0.0657\n",
      "Epoch [1045/2000] | Train Acc: 97.83% | Test Acc: 94.71% | Loss: 0.0657\n",
      "Epoch [1046/2000] | Train Acc: 97.71% | Test Acc: 94.79% | Loss: 0.0655\n",
      "Epoch [1047/2000] | Train Acc: 97.80% | Test Acc: 94.77% | Loss: 0.0653\n",
      "Epoch [1048/2000] | Train Acc: 97.75% | Test Acc: 94.81% | Loss: 0.0650\n",
      "Epoch [1049/2000] | Train Acc: 97.74% | Test Acc: 94.78% | Loss: 0.0649\n",
      "Epoch [1050/2000] | Train Acc: 97.72% | Test Acc: 94.74% | Loss: 0.0648\n",
      "Epoch [1051/2000] | Train Acc: 97.74% | Test Acc: 94.78% | Loss: 0.0649\n",
      "Epoch [1052/2000] | Train Acc: 97.77% | Test Acc: 94.77% | Loss: 0.0649\n",
      "Epoch [1053/2000] | Train Acc: 97.75% | Test Acc: 94.81% | Loss: 0.0650\n",
      "Epoch [1054/2000] | Train Acc: 97.81% | Test Acc: 94.77% | Loss: 0.0650\n",
      "Epoch [1055/2000] | Train Acc: 97.74% | Test Acc: 94.79% | Loss: 0.0650\n",
      "Epoch [1056/2000] | Train Acc: 97.81% | Test Acc: 94.77% | Loss: 0.0649\n",
      "Epoch [1057/2000] | Train Acc: 97.75% | Test Acc: 94.81% | Loss: 0.0648\n",
      "Epoch [1058/2000] | Train Acc: 97.78% | Test Acc: 94.74% | Loss: 0.0647\n",
      "Epoch [1059/2000] | Train Acc: 97.74% | Test Acc: 94.81% | Loss: 0.0646\n",
      "Epoch [1060/2000] | Train Acc: 97.74% | Test Acc: 94.79% | Loss: 0.0645\n",
      "Epoch [1061/2000] | Train Acc: 97.73% | Test Acc: 94.79% | Loss: 0.0644\n",
      "Epoch [1062/2000] | Train Acc: 97.74% | Test Acc: 94.81% | Loss: 0.0644\n",
      "Epoch [1063/2000] | Train Acc: 97.74% | Test Acc: 94.77% | Loss: 0.0644\n",
      "Epoch [1064/2000] | Train Acc: 97.75% | Test Acc: 94.81% | Loss: 0.0644\n",
      "Epoch [1065/2000] | Train Acc: 97.76% | Test Acc: 94.77% | Loss: 0.0644\n",
      "Epoch [1066/2000] | Train Acc: 97.76% | Test Acc: 94.81% | Loss: 0.0644\n",
      "Epoch [1067/2000] | Train Acc: 97.80% | Test Acc: 94.75% | Loss: 0.0644\n",
      "Epoch [1068/2000] | Train Acc: 97.76% | Test Acc: 94.81% | Loss: 0.0643\n",
      "Epoch [1069/2000] | Train Acc: 97.81% | Test Acc: 94.77% | Loss: 0.0643\n",
      "Epoch [1070/2000] | Train Acc: 97.76% | Test Acc: 94.79% | Loss: 0.0643\n",
      "Epoch [1071/2000] | Train Acc: 97.82% | Test Acc: 94.74% | Loss: 0.0643\n",
      "Epoch [1072/2000] | Train Acc: 97.74% | Test Acc: 94.81% | Loss: 0.0643\n",
      "Epoch [1073/2000] | Train Acc: 97.82% | Test Acc: 94.72% | Loss: 0.0643\n",
      "Epoch [1074/2000] | Train Acc: 97.74% | Test Acc: 94.84% | Loss: 0.0643\n",
      "Epoch [1075/2000] | Train Acc: 97.83% | Test Acc: 94.72% | Loss: 0.0643\n",
      "Epoch [1076/2000] | Train Acc: 97.74% | Test Acc: 94.87% | Loss: 0.0642\n",
      "Epoch [1077/2000] | Train Acc: 97.83% | Test Acc: 94.69% | Loss: 0.0642\n",
      "Epoch [1078/2000] | Train Acc: 97.73% | Test Acc: 94.90% | Loss: 0.0642\n",
      "Epoch [1079/2000] | Train Acc: 97.84% | Test Acc: 94.71% | Loss: 0.0642\n",
      "Epoch [1080/2000] | Train Acc: 97.71% | Test Acc: 94.91% | Loss: 0.0642\n",
      "Epoch [1081/2000] | Train Acc: 97.84% | Test Acc: 94.66% | Loss: 0.0642\n",
      "Epoch [1082/2000] | Train Acc: 97.70% | Test Acc: 94.91% | Loss: 0.0642\n",
      "Epoch [1083/2000] | Train Acc: 97.85% | Test Acc: 94.68% | Loss: 0.0642\n",
      "Epoch [1084/2000] | Train Acc: 97.71% | Test Acc: 94.90% | Loss: 0.0641\n",
      "Epoch [1085/2000] | Train Acc: 97.85% | Test Acc: 94.71% | Loss: 0.0640\n",
      "Epoch [1086/2000] | Train Acc: 97.73% | Test Acc: 94.87% | Loss: 0.0639\n",
      "Epoch [1087/2000] | Train Acc: 97.84% | Test Acc: 94.72% | Loss: 0.0639\n",
      "Epoch [1088/2000] | Train Acc: 97.74% | Test Acc: 94.81% | Loss: 0.0638\n",
      "Epoch [1089/2000] | Train Acc: 97.83% | Test Acc: 94.75% | Loss: 0.0637\n",
      "Epoch [1090/2000] | Train Acc: 97.76% | Test Acc: 94.82% | Loss: 0.0636\n",
      "Epoch [1091/2000] | Train Acc: 97.80% | Test Acc: 94.77% | Loss: 0.0635\n",
      "Epoch [1092/2000] | Train Acc: 97.77% | Test Acc: 94.81% | Loss: 0.0634\n",
      "Epoch [1093/2000] | Train Acc: 97.78% | Test Acc: 94.74% | Loss: 0.0634\n",
      "Epoch [1094/2000] | Train Acc: 97.76% | Test Acc: 94.82% | Loss: 0.0633\n",
      "Epoch [1095/2000] | Train Acc: 97.76% | Test Acc: 94.78% | Loss: 0.0633\n",
      "Epoch [1096/2000] | Train Acc: 97.75% | Test Acc: 94.82% | Loss: 0.0632\n",
      "Epoch [1097/2000] | Train Acc: 97.75% | Test Acc: 94.82% | Loss: 0.0632\n",
      "Epoch [1098/2000] | Train Acc: 97.76% | Test Acc: 94.81% | Loss: 0.0632\n",
      "Epoch [1099/2000] | Train Acc: 97.75% | Test Acc: 94.82% | Loss: 0.0631\n",
      "Epoch [1100/2000] | Train Acc: 97.76% | Test Acc: 94.81% | Loss: 0.0631\n",
      "Epoch [1101/2000] | Train Acc: 97.75% | Test Acc: 94.81% | Loss: 0.0630\n",
      "Epoch [1102/2000] | Train Acc: 97.76% | Test Acc: 94.81% | Loss: 0.0630\n",
      "Epoch [1103/2000] | Train Acc: 97.75% | Test Acc: 94.81% | Loss: 0.0630\n",
      "Epoch [1104/2000] | Train Acc: 97.76% | Test Acc: 94.81% | Loss: 0.0629\n",
      "Epoch [1105/2000] | Train Acc: 97.75% | Test Acc: 94.81% | Loss: 0.0629\n",
      "Epoch [1106/2000] | Train Acc: 97.76% | Test Acc: 94.81% | Loss: 0.0629\n",
      "Epoch [1107/2000] | Train Acc: 97.75% | Test Acc: 94.82% | Loss: 0.0629\n",
      "Epoch [1108/2000] | Train Acc: 97.76% | Test Acc: 94.81% | Loss: 0.0628\n",
      "Epoch [1109/2000] | Train Acc: 97.76% | Test Acc: 94.78% | Loss: 0.0628\n",
      "Epoch [1110/2000] | Train Acc: 97.76% | Test Acc: 94.85% | Loss: 0.0628\n",
      "Epoch [1111/2000] | Train Acc: 97.78% | Test Acc: 94.75% | Loss: 0.0627\n",
      "Epoch [1112/2000] | Train Acc: 97.77% | Test Acc: 94.85% | Loss: 0.0627\n",
      "Epoch [1113/2000] | Train Acc: 97.81% | Test Acc: 94.79% | Loss: 0.0627\n",
      "Epoch [1114/2000] | Train Acc: 97.77% | Test Acc: 94.87% | Loss: 0.0628\n",
      "Epoch [1115/2000] | Train Acc: 97.84% | Test Acc: 94.71% | Loss: 0.0629\n",
      "Epoch [1116/2000] | Train Acc: 97.76% | Test Acc: 94.94% | Loss: 0.0631\n",
      "Epoch [1117/2000] | Train Acc: 97.91% | Test Acc: 94.62% | Loss: 0.0637\n",
      "Epoch [1118/2000] | Train Acc: 97.73% | Test Acc: 94.88% | Loss: 0.0646\n",
      "Epoch [1119/2000] | Train Acc: 97.84% | Test Acc: 94.50% | Loss: 0.0664\n",
      "Epoch [1120/2000] | Train Acc: 97.59% | Test Acc: 94.88% | Loss: 0.0673\n",
      "Epoch [1121/2000] | Train Acc: 97.79% | Test Acc: 94.59% | Loss: 0.0684\n",
      "Epoch [1122/2000] | Train Acc: 97.66% | Test Acc: 94.88% | Loss: 0.0655\n",
      "Epoch [1123/2000] | Train Acc: 97.86% | Test Acc: 94.82% | Loss: 0.0630\n",
      "Epoch [1124/2000] | Train Acc: 97.82% | Test Acc: 94.65% | Loss: 0.0625\n",
      "Epoch [1125/2000] | Train Acc: 97.72% | Test Acc: 94.93% | Loss: 0.0639\n",
      "Epoch [1126/2000] | Train Acc: 97.87% | Test Acc: 94.57% | Loss: 0.0658\n",
      "Epoch [1127/2000] | Train Acc: 97.72% | Test Acc: 94.85% | Loss: 0.0647\n",
      "Epoch [1128/2000] | Train Acc: 97.85% | Test Acc: 94.81% | Loss: 0.0630\n",
      "Epoch [1129/2000] | Train Acc: 97.81% | Test Acc: 94.68% | Loss: 0.0623\n",
      "Epoch [1130/2000] | Train Acc: 97.75% | Test Acc: 94.94% | Loss: 0.0633\n",
      "Epoch [1131/2000] | Train Acc: 97.88% | Test Acc: 94.62% | Loss: 0.0645\n",
      "Epoch [1132/2000] | Train Acc: 97.72% | Test Acc: 94.79% | Loss: 0.0635\n",
      "Epoch [1133/2000] | Train Acc: 97.82% | Test Acc: 94.79% | Loss: 0.0623\n",
      "Epoch [1134/2000] | Train Acc: 97.85% | Test Acc: 94.72% | Loss: 0.0623\n",
      "Epoch [1135/2000] | Train Acc: 97.77% | Test Acc: 94.94% | Loss: 0.0631\n",
      "Epoch [1136/2000] | Train Acc: 97.90% | Test Acc: 94.74% | Loss: 0.0635\n",
      "Epoch [1137/2000] | Train Acc: 97.74% | Test Acc: 94.78% | Loss: 0.0626\n",
      "Epoch [1138/2000] | Train Acc: 97.78% | Test Acc: 94.85% | Loss: 0.0619\n",
      "Epoch [1139/2000] | Train Acc: 97.86% | Test Acc: 94.66% | Loss: 0.0623\n",
      "Epoch [1140/2000] | Train Acc: 97.74% | Test Acc: 94.87% | Loss: 0.0627\n",
      "Epoch [1141/2000] | Train Acc: 97.85% | Test Acc: 94.85% | Loss: 0.0626\n",
      "Epoch [1142/2000] | Train Acc: 97.81% | Test Acc: 94.74% | Loss: 0.0620\n",
      "Epoch [1143/2000] | Train Acc: 97.78% | Test Acc: 94.88% | Loss: 0.0619\n",
      "Epoch [1144/2000] | Train Acc: 97.91% | Test Acc: 94.68% | Loss: 0.0622\n",
      "Epoch [1145/2000] | Train Acc: 97.76% | Test Acc: 94.85% | Loss: 0.0623\n",
      "Epoch [1146/2000] | Train Acc: 97.83% | Test Acc: 94.71% | Loss: 0.0621\n",
      "Epoch [1147/2000] | Train Acc: 97.79% | Test Acc: 94.74% | Loss: 0.0617\n",
      "Epoch [1148/2000] | Train Acc: 97.77% | Test Acc: 94.88% | Loss: 0.0618\n",
      "Epoch [1149/2000] | Train Acc: 97.88% | Test Acc: 94.75% | Loss: 0.0620\n",
      "Epoch [1150/2000] | Train Acc: 97.78% | Test Acc: 94.82% | Loss: 0.0620\n",
      "Epoch [1151/2000] | Train Acc: 97.82% | Test Acc: 94.77% | Loss: 0.0617\n",
      "Epoch [1152/2000] | Train Acc: 97.76% | Test Acc: 94.72% | Loss: 0.0615\n",
      "Epoch [1153/2000] | Train Acc: 97.77% | Test Acc: 94.87% | Loss: 0.0616\n",
      "Epoch [1154/2000] | Train Acc: 97.87% | Test Acc: 94.85% | Loss: 0.0618\n",
      "Epoch [1155/2000] | Train Acc: 97.81% | Test Acc: 94.81% | Loss: 0.0617\n",
      "Epoch [1156/2000] | Train Acc: 97.82% | Test Acc: 94.77% | Loss: 0.0615\n",
      "Epoch [1157/2000] | Train Acc: 97.79% | Test Acc: 94.72% | Loss: 0.0614\n",
      "Epoch [1158/2000] | Train Acc: 97.79% | Test Acc: 94.85% | Loss: 0.0615\n",
      "Epoch [1159/2000] | Train Acc: 97.86% | Test Acc: 94.84% | Loss: 0.0615\n",
      "Epoch [1160/2000] | Train Acc: 97.83% | Test Acc: 94.78% | Loss: 0.0615\n",
      "Epoch [1161/2000] | Train Acc: 97.81% | Test Acc: 94.79% | Loss: 0.0613\n",
      "Epoch [1162/2000] | Train Acc: 97.80% | Test Acc: 94.71% | Loss: 0.0612\n",
      "Epoch [1163/2000] | Train Acc: 97.79% | Test Acc: 94.87% | Loss: 0.0613\n",
      "Epoch [1164/2000] | Train Acc: 97.84% | Test Acc: 94.74% | Loss: 0.0613\n",
      "Epoch [1165/2000] | Train Acc: 97.79% | Test Acc: 94.77% | Loss: 0.0613\n",
      "Epoch [1166/2000] | Train Acc: 97.82% | Test Acc: 94.81% | Loss: 0.0612\n",
      "Epoch [1167/2000] | Train Acc: 97.81% | Test Acc: 94.77% | Loss: 0.0611\n",
      "Epoch [1168/2000] | Train Acc: 97.80% | Test Acc: 94.81% | Loss: 0.0611\n",
      "Epoch [1169/2000] | Train Acc: 97.82% | Test Acc: 94.72% | Loss: 0.0611\n",
      "Epoch [1170/2000] | Train Acc: 97.79% | Test Acc: 94.85% | Loss: 0.0611\n",
      "Epoch [1171/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0610\n",
      "Epoch [1172/2000] | Train Acc: 97.80% | Test Acc: 94.78% | Loss: 0.0610\n",
      "Epoch [1173/2000] | Train Acc: 97.80% | Test Acc: 94.81% | Loss: 0.0609\n",
      "Epoch [1174/2000] | Train Acc: 97.82% | Test Acc: 94.69% | Loss: 0.0609\n",
      "Epoch [1175/2000] | Train Acc: 97.80% | Test Acc: 94.81% | Loss: 0.0609\n",
      "Epoch [1176/2000] | Train Acc: 97.82% | Test Acc: 94.75% | Loss: 0.0609\n",
      "Epoch [1177/2000] | Train Acc: 97.79% | Test Acc: 94.81% | Loss: 0.0609\n",
      "Epoch [1178/2000] | Train Acc: 97.81% | Test Acc: 94.82% | Loss: 0.0608\n",
      "Epoch [1179/2000] | Train Acc: 97.81% | Test Acc: 94.79% | Loss: 0.0608\n",
      "Epoch [1180/2000] | Train Acc: 97.80% | Test Acc: 94.82% | Loss: 0.0608\n",
      "Epoch [1181/2000] | Train Acc: 97.82% | Test Acc: 94.75% | Loss: 0.0607\n",
      "Epoch [1182/2000] | Train Acc: 97.79% | Test Acc: 94.82% | Loss: 0.0607\n",
      "Epoch [1183/2000] | Train Acc: 97.82% | Test Acc: 94.77% | Loss: 0.0607\n",
      "Epoch [1184/2000] | Train Acc: 97.81% | Test Acc: 94.78% | Loss: 0.0607\n",
      "Epoch [1185/2000] | Train Acc: 97.80% | Test Acc: 94.77% | Loss: 0.0606\n",
      "Epoch [1186/2000] | Train Acc: 97.81% | Test Acc: 94.82% | Loss: 0.0606\n",
      "Epoch [1187/2000] | Train Acc: 97.82% | Test Acc: 94.81% | Loss: 0.0606\n",
      "Epoch [1188/2000] | Train Acc: 97.82% | Test Acc: 94.77% | Loss: 0.0605\n",
      "Epoch [1189/2000] | Train Acc: 97.82% | Test Acc: 94.84% | Loss: 0.0605\n",
      "Epoch [1190/2000] | Train Acc: 97.82% | Test Acc: 94.82% | Loss: 0.0605\n",
      "Epoch [1191/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0605\n",
      "Epoch [1192/2000] | Train Acc: 97.82% | Test Acc: 94.81% | Loss: 0.0604\n",
      "Epoch [1193/2000] | Train Acc: 97.83% | Test Acc: 94.79% | Loss: 0.0604\n",
      "Epoch [1194/2000] | Train Acc: 97.82% | Test Acc: 94.81% | Loss: 0.0604\n",
      "Epoch [1195/2000] | Train Acc: 97.82% | Test Acc: 94.82% | Loss: 0.0604\n",
      "Epoch [1196/2000] | Train Acc: 97.83% | Test Acc: 94.84% | Loss: 0.0603\n",
      "Epoch [1197/2000] | Train Acc: 97.82% | Test Acc: 94.82% | Loss: 0.0603\n",
      "Epoch [1198/2000] | Train Acc: 97.83% | Test Acc: 94.82% | Loss: 0.0603\n",
      "Epoch [1199/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0602\n",
      "Epoch [1200/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0602\n",
      "Epoch [1201/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0602\n",
      "Epoch [1202/2000] | Train Acc: 97.83% | Test Acc: 94.79% | Loss: 0.0602\n",
      "Epoch [1203/2000] | Train Acc: 97.83% | Test Acc: 94.79% | Loss: 0.0601\n",
      "Epoch [1204/2000] | Train Acc: 97.83% | Test Acc: 94.79% | Loss: 0.0601\n",
      "Epoch [1205/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0601\n",
      "Epoch [1206/2000] | Train Acc: 97.84% | Test Acc: 94.81% | Loss: 0.0601\n",
      "Epoch [1207/2000] | Train Acc: 97.82% | Test Acc: 94.82% | Loss: 0.0600\n",
      "Epoch [1208/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0600\n",
      "Epoch [1209/2000] | Train Acc: 97.83% | Test Acc: 94.82% | Loss: 0.0600\n",
      "Epoch [1210/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0600\n",
      "Epoch [1211/2000] | Train Acc: 97.83% | Test Acc: 94.82% | Loss: 0.0599\n",
      "Epoch [1212/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0599\n",
      "Epoch [1213/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0599\n",
      "Epoch [1214/2000] | Train Acc: 97.83% | Test Acc: 94.82% | Loss: 0.0599\n",
      "Epoch [1215/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0598\n",
      "Epoch [1216/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0598\n",
      "Epoch [1217/2000] | Train Acc: 97.84% | Test Acc: 94.81% | Loss: 0.0598\n",
      "Epoch [1218/2000] | Train Acc: 97.83% | Test Acc: 94.82% | Loss: 0.0598\n",
      "Epoch [1219/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0597\n",
      "Epoch [1220/2000] | Train Acc: 97.84% | Test Acc: 94.82% | Loss: 0.0597\n",
      "Epoch [1221/2000] | Train Acc: 97.84% | Test Acc: 94.82% | Loss: 0.0597\n",
      "Epoch [1222/2000] | Train Acc: 97.83% | Test Acc: 94.84% | Loss: 0.0597\n",
      "Epoch [1223/2000] | Train Acc: 97.84% | Test Acc: 94.82% | Loss: 0.0596\n",
      "Epoch [1224/2000] | Train Acc: 97.84% | Test Acc: 94.82% | Loss: 0.0596\n",
      "Epoch [1225/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0596\n",
      "Epoch [1226/2000] | Train Acc: 97.85% | Test Acc: 94.81% | Loss: 0.0596\n",
      "Epoch [1227/2000] | Train Acc: 97.84% | Test Acc: 94.82% | Loss: 0.0595\n",
      "Epoch [1228/2000] | Train Acc: 97.86% | Test Acc: 94.77% | Loss: 0.0595\n",
      "Epoch [1229/2000] | Train Acc: 97.83% | Test Acc: 94.82% | Loss: 0.0595\n",
      "Epoch [1230/2000] | Train Acc: 97.87% | Test Acc: 94.74% | Loss: 0.0595\n",
      "Epoch [1231/2000] | Train Acc: 97.82% | Test Acc: 94.85% | Loss: 0.0595\n",
      "Epoch [1232/2000] | Train Acc: 97.86% | Test Acc: 94.74% | Loss: 0.0595\n",
      "Epoch [1233/2000] | Train Acc: 97.80% | Test Acc: 94.93% | Loss: 0.0596\n",
      "Epoch [1234/2000] | Train Acc: 97.89% | Test Acc: 94.79% | Loss: 0.0598\n",
      "Epoch [1235/2000] | Train Acc: 97.83% | Test Acc: 94.96% | Loss: 0.0600\n",
      "Epoch [1236/2000] | Train Acc: 97.93% | Test Acc: 94.68% | Loss: 0.0606\n",
      "Epoch [1237/2000] | Train Acc: 97.79% | Test Acc: 94.93% | Loss: 0.0612\n",
      "Epoch [1238/2000] | Train Acc: 97.91% | Test Acc: 94.62% | Loss: 0.0624\n",
      "Epoch [1239/2000] | Train Acc: 97.71% | Test Acc: 94.94% | Loss: 0.0628\n",
      "Epoch [1240/2000] | Train Acc: 97.89% | Test Acc: 94.62% | Loss: 0.0634\n",
      "Epoch [1241/2000] | Train Acc: 97.77% | Test Acc: 94.91% | Loss: 0.0617\n",
      "Epoch [1242/2000] | Train Acc: 97.91% | Test Acc: 94.82% | Loss: 0.0601\n",
      "Epoch [1243/2000] | Train Acc: 97.82% | Test Acc: 94.82% | Loss: 0.0592\n",
      "Epoch [1244/2000] | Train Acc: 97.82% | Test Acc: 94.97% | Loss: 0.0595\n",
      "Epoch [1245/2000] | Train Acc: 97.94% | Test Acc: 94.63% | Loss: 0.0606\n",
      "Epoch [1246/2000] | Train Acc: 97.80% | Test Acc: 94.96% | Loss: 0.0610\n",
      "Epoch [1247/2000] | Train Acc: 97.92% | Test Acc: 94.78% | Loss: 0.0609\n",
      "Epoch [1248/2000] | Train Acc: 97.82% | Test Acc: 94.87% | Loss: 0.0597\n",
      "Epoch [1249/2000] | Train Acc: 97.86% | Test Acc: 94.93% | Loss: 0.0591\n",
      "Epoch [1250/2000] | Train Acc: 97.91% | Test Acc: 94.78% | Loss: 0.0593\n",
      "Epoch [1251/2000] | Train Acc: 97.86% | Test Acc: 94.94% | Loss: 0.0598\n",
      "Epoch [1252/2000] | Train Acc: 97.93% | Test Acc: 94.72% | Loss: 0.0602\n",
      "Epoch [1253/2000] | Train Acc: 97.82% | Test Acc: 94.85% | Loss: 0.0597\n",
      "Epoch [1254/2000] | Train Acc: 97.88% | Test Acc: 94.82% | Loss: 0.0591\n",
      "Epoch [1255/2000] | Train Acc: 97.86% | Test Acc: 94.79% | Loss: 0.0589\n",
      "Epoch [1256/2000] | Train Acc: 97.83% | Test Acc: 94.90% | Loss: 0.0592\n",
      "Epoch [1257/2000] | Train Acc: 97.92% | Test Acc: 94.79% | Loss: 0.0595\n",
      "Epoch [1258/2000] | Train Acc: 97.82% | Test Acc: 94.87% | Loss: 0.0595\n",
      "Epoch [1259/2000] | Train Acc: 97.89% | Test Acc: 94.72% | Loss: 0.0592\n",
      "Epoch [1260/2000] | Train Acc: 97.83% | Test Acc: 94.81% | Loss: 0.0588\n",
      "Epoch [1261/2000] | Train Acc: 97.84% | Test Acc: 94.93% | Loss: 0.0588\n",
      "Epoch [1262/2000] | Train Acc: 97.90% | Test Acc: 94.84% | Loss: 0.0590\n",
      "Epoch [1263/2000] | Train Acc: 97.84% | Test Acc: 94.93% | Loss: 0.0591\n",
      "Epoch [1264/2000] | Train Acc: 97.91% | Test Acc: 94.77% | Loss: 0.0591\n",
      "Epoch [1265/2000] | Train Acc: 97.81% | Test Acc: 94.84% | Loss: 0.0589\n",
      "Epoch [1266/2000] | Train Acc: 97.87% | Test Acc: 94.85% | Loss: 0.0587\n",
      "Epoch [1267/2000] | Train Acc: 97.86% | Test Acc: 94.79% | Loss: 0.0586\n",
      "Epoch [1268/2000] | Train Acc: 97.86% | Test Acc: 94.91% | Loss: 0.0587\n",
      "Epoch [1269/2000] | Train Acc: 97.90% | Test Acc: 94.78% | Loss: 0.0588\n",
      "Epoch [1270/2000] | Train Acc: 97.81% | Test Acc: 94.85% | Loss: 0.0588\n",
      "Epoch [1271/2000] | Train Acc: 97.89% | Test Acc: 94.77% | Loss: 0.0587\n",
      "Epoch [1272/2000] | Train Acc: 97.84% | Test Acc: 94.85% | Loss: 0.0585\n",
      "Epoch [1273/2000] | Train Acc: 97.87% | Test Acc: 94.87% | Loss: 0.0585\n",
      "Epoch [1274/2000] | Train Acc: 97.90% | Test Acc: 94.77% | Loss: 0.0585\n",
      "Epoch [1275/2000] | Train Acc: 97.83% | Test Acc: 94.84% | Loss: 0.0586\n",
      "Epoch [1276/2000] | Train Acc: 97.91% | Test Acc: 94.75% | Loss: 0.0586\n",
      "Epoch [1277/2000] | Train Acc: 97.82% | Test Acc: 94.85% | Loss: 0.0585\n",
      "Epoch [1278/2000] | Train Acc: 97.89% | Test Acc: 94.87% | Loss: 0.0584\n",
      "Epoch [1279/2000] | Train Acc: 97.87% | Test Acc: 94.88% | Loss: 0.0584\n",
      "Epoch [1280/2000] | Train Acc: 97.87% | Test Acc: 94.84% | Loss: 0.0583\n",
      "Epoch [1281/2000] | Train Acc: 97.88% | Test Acc: 94.77% | Loss: 0.0583\n",
      "Epoch [1282/2000] | Train Acc: 97.84% | Test Acc: 94.85% | Loss: 0.0584\n",
      "Epoch [1283/2000] | Train Acc: 97.89% | Test Acc: 94.78% | Loss: 0.0584\n",
      "Epoch [1284/2000] | Train Acc: 97.85% | Test Acc: 94.87% | Loss: 0.0583\n",
      "Epoch [1285/2000] | Train Acc: 97.89% | Test Acc: 94.88% | Loss: 0.0583\n",
      "Epoch [1286/2000] | Train Acc: 97.86% | Test Acc: 94.87% | Loss: 0.0582\n",
      "Epoch [1287/2000] | Train Acc: 97.88% | Test Acc: 94.85% | Loss: 0.0582\n",
      "Epoch [1288/2000] | Train Acc: 97.88% | Test Acc: 94.81% | Loss: 0.0582\n",
      "Epoch [1289/2000] | Train Acc: 97.86% | Test Acc: 94.84% | Loss: 0.0582\n",
      "Epoch [1290/2000] | Train Acc: 97.89% | Test Acc: 94.79% | Loss: 0.0581\n",
      "Epoch [1291/2000] | Train Acc: 97.85% | Test Acc: 94.87% | Loss: 0.0581\n",
      "Epoch [1292/2000] | Train Acc: 97.89% | Test Acc: 94.81% | Loss: 0.0581\n",
      "Epoch [1293/2000] | Train Acc: 97.86% | Test Acc: 94.87% | Loss: 0.0581\n",
      "Epoch [1294/2000] | Train Acc: 97.88% | Test Acc: 94.87% | Loss: 0.0580\n",
      "Epoch [1295/2000] | Train Acc: 97.88% | Test Acc: 94.90% | Loss: 0.0580\n",
      "Epoch [1296/2000] | Train Acc: 97.87% | Test Acc: 94.90% | Loss: 0.0580\n",
      "Epoch [1297/2000] | Train Acc: 97.88% | Test Acc: 94.88% | Loss: 0.0580\n",
      "Epoch [1298/2000] | Train Acc: 97.89% | Test Acc: 94.87% | Loss: 0.0579\n",
      "Epoch [1299/2000] | Train Acc: 97.89% | Test Acc: 94.85% | Loss: 0.0579\n",
      "Epoch [1300/2000] | Train Acc: 97.87% | Test Acc: 94.85% | Loss: 0.0579\n",
      "Epoch [1301/2000] | Train Acc: 97.88% | Test Acc: 94.85% | Loss: 0.0579\n",
      "Epoch [1302/2000] | Train Acc: 97.87% | Test Acc: 94.87% | Loss: 0.0579\n",
      "Epoch [1303/2000] | Train Acc: 97.89% | Test Acc: 94.85% | Loss: 0.0578\n",
      "Epoch [1304/2000] | Train Acc: 97.87% | Test Acc: 94.87% | Loss: 0.0578\n",
      "Epoch [1305/2000] | Train Acc: 97.89% | Test Acc: 94.87% | Loss: 0.0578\n",
      "Epoch [1306/2000] | Train Acc: 97.88% | Test Acc: 94.88% | Loss: 0.0578\n",
      "Epoch [1307/2000] | Train Acc: 97.89% | Test Acc: 94.93% | Loss: 0.0577\n",
      "Epoch [1308/2000] | Train Acc: 97.89% | Test Acc: 94.88% | Loss: 0.0577\n",
      "Epoch [1309/2000] | Train Acc: 97.88% | Test Acc: 94.90% | Loss: 0.0577\n",
      "Epoch [1310/2000] | Train Acc: 97.89% | Test Acc: 94.88% | Loss: 0.0577\n",
      "Epoch [1311/2000] | Train Acc: 97.89% | Test Acc: 94.90% | Loss: 0.0577\n",
      "Epoch [1312/2000] | Train Acc: 97.89% | Test Acc: 94.91% | Loss: 0.0576\n",
      "Epoch [1313/2000] | Train Acc: 97.89% | Test Acc: 94.93% | Loss: 0.0576\n",
      "Epoch [1314/2000] | Train Acc: 97.88% | Test Acc: 94.90% | Loss: 0.0576\n",
      "Epoch [1315/2000] | Train Acc: 97.89% | Test Acc: 94.88% | Loss: 0.0576\n",
      "Epoch [1316/2000] | Train Acc: 97.89% | Test Acc: 94.91% | Loss: 0.0575\n",
      "Epoch [1317/2000] | Train Acc: 97.89% | Test Acc: 94.88% | Loss: 0.0575\n",
      "Epoch [1318/2000] | Train Acc: 97.89% | Test Acc: 94.91% | Loss: 0.0575\n",
      "Epoch [1319/2000] | Train Acc: 97.89% | Test Acc: 94.88% | Loss: 0.0575\n",
      "Epoch [1320/2000] | Train Acc: 97.89% | Test Acc: 94.93% | Loss: 0.0575\n",
      "Epoch [1321/2000] | Train Acc: 97.89% | Test Acc: 94.87% | Loss: 0.0574\n",
      "Epoch [1322/2000] | Train Acc: 97.89% | Test Acc: 94.91% | Loss: 0.0574\n",
      "Epoch [1323/2000] | Train Acc: 97.89% | Test Acc: 94.87% | Loss: 0.0574\n",
      "Epoch [1324/2000] | Train Acc: 97.89% | Test Acc: 94.85% | Loss: 0.0574\n",
      "Epoch [1325/2000] | Train Acc: 97.88% | Test Acc: 94.88% | Loss: 0.0574\n",
      "Epoch [1326/2000] | Train Acc: 97.91% | Test Acc: 94.77% | Loss: 0.0574\n",
      "Epoch [1327/2000] | Train Acc: 97.86% | Test Acc: 94.91% | Loss: 0.0574\n",
      "Epoch [1328/2000] | Train Acc: 97.92% | Test Acc: 94.82% | Loss: 0.0574\n",
      "Epoch [1329/2000] | Train Acc: 97.86% | Test Acc: 94.96% | Loss: 0.0575\n",
      "Epoch [1330/2000] | Train Acc: 97.94% | Test Acc: 94.82% | Loss: 0.0577\n",
      "Epoch [1331/2000] | Train Acc: 97.90% | Test Acc: 94.99% | Loss: 0.0579\n",
      "Epoch [1332/2000] | Train Acc: 97.95% | Test Acc: 94.68% | Loss: 0.0585\n",
      "Epoch [1333/2000] | Train Acc: 97.83% | Test Acc: 94.97% | Loss: 0.0592\n",
      "Epoch [1334/2000] | Train Acc: 97.94% | Test Acc: 94.63% | Loss: 0.0605\n",
      "Epoch [1335/2000] | Train Acc: 97.72% | Test Acc: 94.97% | Loss: 0.0609\n",
      "Epoch [1336/2000] | Train Acc: 97.90% | Test Acc: 94.66% | Loss: 0.0615\n",
      "Epoch [1337/2000] | Train Acc: 97.79% | Test Acc: 94.93% | Loss: 0.0597\n",
      "Epoch [1338/2000] | Train Acc: 97.93% | Test Acc: 94.85% | Loss: 0.0581\n",
      "Epoch [1339/2000] | Train Acc: 97.86% | Test Acc: 94.82% | Loss: 0.0572\n",
      "Epoch [1340/2000] | Train Acc: 97.86% | Test Acc: 94.97% | Loss: 0.0575\n",
      "Epoch [1341/2000] | Train Acc: 97.96% | Test Acc: 94.71% | Loss: 0.0586\n",
      "Epoch [1342/2000] | Train Acc: 97.82% | Test Acc: 94.99% | Loss: 0.0591\n",
      "Epoch [1343/2000] | Train Acc: 97.95% | Test Acc: 94.74% | Loss: 0.0589\n",
      "Epoch [1344/2000] | Train Acc: 97.86% | Test Acc: 94.96% | Loss: 0.0577\n",
      "Epoch [1345/2000] | Train Acc: 97.89% | Test Acc: 94.97% | Loss: 0.0570\n",
      "Epoch [1346/2000] | Train Acc: 97.96% | Test Acc: 94.78% | Loss: 0.0572\n",
      "Epoch [1347/2000] | Train Acc: 97.90% | Test Acc: 94.97% | Loss: 0.0578\n",
      "Epoch [1348/2000] | Train Acc: 97.95% | Test Acc: 94.77% | Loss: 0.0582\n",
      "Epoch [1349/2000] | Train Acc: 97.85% | Test Acc: 94.93% | Loss: 0.0578\n",
      "Epoch [1350/2000] | Train Acc: 97.92% | Test Acc: 94.88% | Loss: 0.0572\n",
      "Epoch [1351/2000] | Train Acc: 97.90% | Test Acc: 94.85% | Loss: 0.0569\n",
      "Epoch [1352/2000] | Train Acc: 97.91% | Test Acc: 94.91% | Loss: 0.0571\n",
      "Epoch [1353/2000] | Train Acc: 97.94% | Test Acc: 94.82% | Loss: 0.0575\n",
      "Epoch [1354/2000] | Train Acc: 97.90% | Test Acc: 94.96% | Loss: 0.0575\n",
      "Epoch [1355/2000] | Train Acc: 97.95% | Test Acc: 94.81% | Loss: 0.0573\n",
      "Epoch [1356/2000] | Train Acc: 97.88% | Test Acc: 94.96% | Loss: 0.0569\n",
      "Epoch [1357/2000] | Train Acc: 97.90% | Test Acc: 95.03% | Loss: 0.0568\n",
      "Epoch [1358/2000] | Train Acc: 97.95% | Test Acc: 94.85% | Loss: 0.0570\n",
      "Epoch [1359/2000] | Train Acc: 97.93% | Test Acc: 94.96% | Loss: 0.0572\n",
      "Epoch [1360/2000] | Train Acc: 97.94% | Test Acc: 94.84% | Loss: 0.0572\n",
      "Epoch [1361/2000] | Train Acc: 97.89% | Test Acc: 94.90% | Loss: 0.0569\n",
      "Epoch [1362/2000] | Train Acc: 97.91% | Test Acc: 94.93% | Loss: 0.0567\n",
      "Epoch [1363/2000] | Train Acc: 97.91% | Test Acc: 94.82% | Loss: 0.0567\n",
      "Epoch [1364/2000] | Train Acc: 97.92% | Test Acc: 95.02% | Loss: 0.0568\n",
      "Epoch [1365/2000] | Train Acc: 97.95% | Test Acc: 94.81% | Loss: 0.0569\n",
      "Epoch [1366/2000] | Train Acc: 97.90% | Test Acc: 94.96% | Loss: 0.0569\n",
      "Epoch [1367/2000] | Train Acc: 97.93% | Test Acc: 94.91% | Loss: 0.0567\n",
      "Epoch [1368/2000] | Train Acc: 97.91% | Test Acc: 94.96% | Loss: 0.0566\n",
      "Epoch [1369/2000] | Train Acc: 97.91% | Test Acc: 94.99% | Loss: 0.0565\n",
      "Epoch [1370/2000] | Train Acc: 97.95% | Test Acc: 94.82% | Loss: 0.0566\n",
      "Epoch [1371/2000] | Train Acc: 97.89% | Test Acc: 94.97% | Loss: 0.0566\n",
      "Epoch [1372/2000] | Train Acc: 97.94% | Test Acc: 94.85% | Loss: 0.0567\n",
      "Epoch [1373/2000] | Train Acc: 97.90% | Test Acc: 94.97% | Loss: 0.0566\n",
      "Epoch [1374/2000] | Train Acc: 97.94% | Test Acc: 94.94% | Loss: 0.0565\n",
      "Epoch [1375/2000] | Train Acc: 97.91% | Test Acc: 94.96% | Loss: 0.0564\n",
      "Epoch [1376/2000] | Train Acc: 97.91% | Test Acc: 94.91% | Loss: 0.0564\n",
      "Epoch [1377/2000] | Train Acc: 97.93% | Test Acc: 94.84% | Loss: 0.0564\n",
      "Epoch [1378/2000] | Train Acc: 97.90% | Test Acc: 94.96% | Loss: 0.0564\n",
      "Epoch [1379/2000] | Train Acc: 97.95% | Test Acc: 94.84% | Loss: 0.0564\n",
      "Epoch [1380/2000] | Train Acc: 97.92% | Test Acc: 94.97% | Loss: 0.0564\n",
      "Epoch [1381/2000] | Train Acc: 97.93% | Test Acc: 94.97% | Loss: 0.0564\n",
      "Epoch [1382/2000] | Train Acc: 97.92% | Test Acc: 94.94% | Loss: 0.0563\n",
      "Epoch [1383/2000] | Train Acc: 97.92% | Test Acc: 94.94% | Loss: 0.0563\n",
      "Epoch [1384/2000] | Train Acc: 97.92% | Test Acc: 94.96% | Loss: 0.0563\n",
      "Epoch [1385/2000] | Train Acc: 97.92% | Test Acc: 94.96% | Loss: 0.0563\n",
      "Epoch [1386/2000] | Train Acc: 97.93% | Test Acc: 94.91% | Loss: 0.0563\n",
      "Epoch [1387/2000] | Train Acc: 97.93% | Test Acc: 94.97% | Loss: 0.0562\n",
      "Epoch [1388/2000] | Train Acc: 97.93% | Test Acc: 94.96% | Loss: 0.0562\n",
      "Epoch [1389/2000] | Train Acc: 97.92% | Test Acc: 94.88% | Loss: 0.0562\n",
      "Epoch [1390/2000] | Train Acc: 97.91% | Test Acc: 94.96% | Loss: 0.0562\n",
      "Epoch [1391/2000] | Train Acc: 97.92% | Test Acc: 94.96% | Loss: 0.0561\n",
      "Epoch [1392/2000] | Train Acc: 97.92% | Test Acc: 94.96% | Loss: 0.0561\n",
      "Epoch [1393/2000] | Train Acc: 97.92% | Test Acc: 94.97% | Loss: 0.0561\n",
      "Epoch [1394/2000] | Train Acc: 97.92% | Test Acc: 94.91% | Loss: 0.0561\n",
      "Epoch [1395/2000] | Train Acc: 97.91% | Test Acc: 94.97% | Loss: 0.0561\n",
      "Epoch [1396/2000] | Train Acc: 97.93% | Test Acc: 94.90% | Loss: 0.0561\n",
      "Epoch [1397/2000] | Train Acc: 97.93% | Test Acc: 94.96% | Loss: 0.0560\n",
      "Epoch [1398/2000] | Train Acc: 97.93% | Test Acc: 94.90% | Loss: 0.0560\n",
      "Epoch [1399/2000] | Train Acc: 97.93% | Test Acc: 94.97% | Loss: 0.0560\n",
      "Epoch [1400/2000] | Train Acc: 97.93% | Test Acc: 94.93% | Loss: 0.0560\n",
      "Epoch [1401/2000] | Train Acc: 97.92% | Test Acc: 94.97% | Loss: 0.0560\n",
      "Epoch [1402/2000] | Train Acc: 97.93% | Test Acc: 94.93% | Loss: 0.0559\n",
      "Epoch [1403/2000] | Train Acc: 97.92% | Test Acc: 94.96% | Loss: 0.0559\n",
      "Epoch [1404/2000] | Train Acc: 97.92% | Test Acc: 94.94% | Loss: 0.0559\n",
      "Epoch [1405/2000] | Train Acc: 97.93% | Test Acc: 94.96% | Loss: 0.0559\n",
      "Epoch [1406/2000] | Train Acc: 97.92% | Test Acc: 94.94% | Loss: 0.0559\n",
      "Epoch [1407/2000] | Train Acc: 97.93% | Test Acc: 94.97% | Loss: 0.0558\n",
      "Epoch [1408/2000] | Train Acc: 97.93% | Test Acc: 94.96% | Loss: 0.0558\n",
      "Epoch [1409/2000] | Train Acc: 97.92% | Test Acc: 94.94% | Loss: 0.0558\n",
      "Epoch [1410/2000] | Train Acc: 97.93% | Test Acc: 94.94% | Loss: 0.0558\n",
      "Epoch [1411/2000] | Train Acc: 97.92% | Test Acc: 94.93% | Loss: 0.0558\n",
      "Epoch [1412/2000] | Train Acc: 97.94% | Test Acc: 94.96% | Loss: 0.0557\n",
      "Epoch [1413/2000] | Train Acc: 97.93% | Test Acc: 94.91% | Loss: 0.0557\n",
      "Epoch [1414/2000] | Train Acc: 97.93% | Test Acc: 94.99% | Loss: 0.0557\n",
      "Epoch [1415/2000] | Train Acc: 97.94% | Test Acc: 94.93% | Loss: 0.0557\n",
      "Epoch [1416/2000] | Train Acc: 97.94% | Test Acc: 94.97% | Loss: 0.0557\n",
      "Epoch [1417/2000] | Train Acc: 97.95% | Test Acc: 94.96% | Loss: 0.0557\n",
      "Epoch [1418/2000] | Train Acc: 97.94% | Test Acc: 94.91% | Loss: 0.0557\n",
      "Epoch [1419/2000] | Train Acc: 97.95% | Test Acc: 95.02% | Loss: 0.0557\n",
      "Epoch [1420/2000] | Train Acc: 97.95% | Test Acc: 94.84% | Loss: 0.0557\n",
      "Epoch [1421/2000] | Train Acc: 97.93% | Test Acc: 95.02% | Loss: 0.0557\n",
      "Epoch [1422/2000] | Train Acc: 97.96% | Test Acc: 94.84% | Loss: 0.0557\n",
      "Epoch [1423/2000] | Train Acc: 97.92% | Test Acc: 94.99% | Loss: 0.0558\n",
      "Epoch [1424/2000] | Train Acc: 97.96% | Test Acc: 94.91% | Loss: 0.0559\n",
      "Epoch [1425/2000] | Train Acc: 97.94% | Test Acc: 94.96% | Loss: 0.0561\n",
      "Epoch [1426/2000] | Train Acc: 97.99% | Test Acc: 94.85% | Loss: 0.0566\n",
      "Epoch [1427/2000] | Train Acc: 97.89% | Test Acc: 94.99% | Loss: 0.0570\n",
      "Epoch [1428/2000] | Train Acc: 97.96% | Test Acc: 94.79% | Loss: 0.0578\n",
      "Epoch [1429/2000] | Train Acc: 97.82% | Test Acc: 94.97% | Loss: 0.0581\n",
      "Epoch [1430/2000] | Train Acc: 97.96% | Test Acc: 94.78% | Loss: 0.0586\n",
      "Epoch [1431/2000] | Train Acc: 97.83% | Test Acc: 95.00% | Loss: 0.0578\n",
      "Epoch [1432/2000] | Train Acc: 97.97% | Test Acc: 94.85% | Loss: 0.0569\n",
      "Epoch [1433/2000] | Train Acc: 97.93% | Test Acc: 94.96% | Loss: 0.0558\n",
      "Epoch [1434/2000] | Train Acc: 97.92% | Test Acc: 95.06% | Loss: 0.0554\n",
      "Epoch [1435/2000] | Train Acc: 97.97% | Test Acc: 94.88% | Loss: 0.0556\n",
      "Epoch [1436/2000] | Train Acc: 97.92% | Test Acc: 94.96% | Loss: 0.0561\n",
      "Epoch [1437/2000] | Train Acc: 97.98% | Test Acc: 94.85% | Loss: 0.0567\n",
      "Epoch [1438/2000] | Train Acc: 97.91% | Test Acc: 94.96% | Loss: 0.0566\n",
      "Epoch [1439/2000] | Train Acc: 97.96% | Test Acc: 94.87% | Loss: 0.0563\n",
      "Epoch [1440/2000] | Train Acc: 97.93% | Test Acc: 95.00% | Loss: 0.0556\n",
      "Epoch [1441/2000] | Train Acc: 97.95% | Test Acc: 95.08% | Loss: 0.0553\n",
      "Epoch [1442/2000] | Train Acc: 97.96% | Test Acc: 94.90% | Loss: 0.0553\n",
      "Epoch [1443/2000] | Train Acc: 97.90% | Test Acc: 95.00% | Loss: 0.0556\n",
      "Epoch [1444/2000] | Train Acc: 97.97% | Test Acc: 94.90% | Loss: 0.0559\n",
      "Epoch [1445/2000] | Train Acc: 97.94% | Test Acc: 95.02% | Loss: 0.0559\n",
      "Epoch [1446/2000] | Train Acc: 97.95% | Test Acc: 94.87% | Loss: 0.0557\n",
      "Epoch [1447/2000] | Train Acc: 97.93% | Test Acc: 95.03% | Loss: 0.0554\n",
      "Epoch [1448/2000] | Train Acc: 97.97% | Test Acc: 95.03% | Loss: 0.0552\n",
      "Epoch [1449/2000] | Train Acc: 97.96% | Test Acc: 94.85% | Loss: 0.0552\n",
      "Epoch [1450/2000] | Train Acc: 97.94% | Test Acc: 95.08% | Loss: 0.0553\n",
      "Epoch [1451/2000] | Train Acc: 97.97% | Test Acc: 94.93% | Loss: 0.0554\n",
      "Epoch [1452/2000] | Train Acc: 97.93% | Test Acc: 95.06% | Loss: 0.0554\n",
      "Epoch [1453/2000] | Train Acc: 97.96% | Test Acc: 94.87% | Loss: 0.0554\n",
      "Epoch [1454/2000] | Train Acc: 97.93% | Test Acc: 95.08% | Loss: 0.0552\n",
      "Epoch [1455/2000] | Train Acc: 97.97% | Test Acc: 94.96% | Loss: 0.0551\n",
      "Epoch [1456/2000] | Train Acc: 97.96% | Test Acc: 94.97% | Loss: 0.0550\n",
      "Epoch [1457/2000] | Train Acc: 97.96% | Test Acc: 95.08% | Loss: 0.0550\n",
      "Epoch [1458/2000] | Train Acc: 97.96% | Test Acc: 94.93% | Loss: 0.0551\n",
      "Epoch [1459/2000] | Train Acc: 97.95% | Test Acc: 95.09% | Loss: 0.0551\n",
      "Epoch [1460/2000] | Train Acc: 97.97% | Test Acc: 94.91% | Loss: 0.0551\n",
      "Epoch [1461/2000] | Train Acc: 97.94% | Test Acc: 95.09% | Loss: 0.0551\n",
      "Epoch [1462/2000] | Train Acc: 97.96% | Test Acc: 94.91% | Loss: 0.0550\n",
      "Epoch [1463/2000] | Train Acc: 97.95% | Test Acc: 95.03% | Loss: 0.0550\n",
      "Epoch [1464/2000] | Train Acc: 97.97% | Test Acc: 94.97% | Loss: 0.0549\n",
      "Epoch [1465/2000] | Train Acc: 97.97% | Test Acc: 94.97% | Loss: 0.0549\n",
      "Epoch [1466/2000] | Train Acc: 97.96% | Test Acc: 95.05% | Loss: 0.0549\n",
      "Epoch [1467/2000] | Train Acc: 97.98% | Test Acc: 94.94% | Loss: 0.0549\n",
      "Epoch [1468/2000] | Train Acc: 97.95% | Test Acc: 95.08% | Loss: 0.0549\n",
      "Epoch [1469/2000] | Train Acc: 97.97% | Test Acc: 94.91% | Loss: 0.0549\n",
      "Epoch [1470/2000] | Train Acc: 97.95% | Test Acc: 95.08% | Loss: 0.0549\n",
      "Epoch [1471/2000] | Train Acc: 97.97% | Test Acc: 94.90% | Loss: 0.0548\n",
      "Epoch [1472/2000] | Train Acc: 97.95% | Test Acc: 95.03% | Loss: 0.0548\n",
      "Epoch [1473/2000] | Train Acc: 97.98% | Test Acc: 94.96% | Loss: 0.0548\n",
      "Epoch [1474/2000] | Train Acc: 97.96% | Test Acc: 95.02% | Loss: 0.0548\n",
      "Epoch [1475/2000] | Train Acc: 97.98% | Test Acc: 94.97% | Loss: 0.0547\n",
      "Epoch [1476/2000] | Train Acc: 97.96% | Test Acc: 95.00% | Loss: 0.0547\n",
      "Epoch [1477/2000] | Train Acc: 97.96% | Test Acc: 94.99% | Loss: 0.0547\n",
      "Epoch [1478/2000] | Train Acc: 97.96% | Test Acc: 94.97% | Loss: 0.0547\n",
      "Epoch [1479/2000] | Train Acc: 97.97% | Test Acc: 95.00% | Loss: 0.0546\n",
      "Epoch [1480/2000] | Train Acc: 97.98% | Test Acc: 94.97% | Loss: 0.0546\n",
      "Epoch [1481/2000] | Train Acc: 97.96% | Test Acc: 95.02% | Loss: 0.0546\n",
      "Epoch [1482/2000] | Train Acc: 97.98% | Test Acc: 94.97% | Loss: 0.0546\n",
      "Epoch [1483/2000] | Train Acc: 97.96% | Test Acc: 95.05% | Loss: 0.0546\n",
      "Epoch [1484/2000] | Train Acc: 97.98% | Test Acc: 94.96% | Loss: 0.0546\n",
      "Epoch [1485/2000] | Train Acc: 97.96% | Test Acc: 95.05% | Loss: 0.0546\n",
      "Epoch [1486/2000] | Train Acc: 97.99% | Test Acc: 94.90% | Loss: 0.0546\n",
      "Epoch [1487/2000] | Train Acc: 97.96% | Test Acc: 95.06% | Loss: 0.0546\n",
      "Epoch [1488/2000] | Train Acc: 97.99% | Test Acc: 94.91% | Loss: 0.0546\n",
      "Epoch [1489/2000] | Train Acc: 97.96% | Test Acc: 95.09% | Loss: 0.0546\n",
      "Epoch [1490/2000] | Train Acc: 97.98% | Test Acc: 94.91% | Loss: 0.0546\n",
      "Epoch [1491/2000] | Train Acc: 97.95% | Test Acc: 95.11% | Loss: 0.0546\n",
      "Epoch [1492/2000] | Train Acc: 98.00% | Test Acc: 94.87% | Loss: 0.0546\n",
      "Epoch [1493/2000] | Train Acc: 97.95% | Test Acc: 95.08% | Loss: 0.0546\n",
      "Epoch [1494/2000] | Train Acc: 97.99% | Test Acc: 94.88% | Loss: 0.0547\n",
      "Epoch [1495/2000] | Train Acc: 97.96% | Test Acc: 95.03% | Loss: 0.0548\n",
      "Epoch [1496/2000] | Train Acc: 97.98% | Test Acc: 94.96% | Loss: 0.0549\n",
      "Epoch [1497/2000] | Train Acc: 97.97% | Test Acc: 95.00% | Loss: 0.0550\n",
      "Epoch [1498/2000] | Train Acc: 97.99% | Test Acc: 94.88% | Loss: 0.0553\n",
      "Epoch [1499/2000] | Train Acc: 97.98% | Test Acc: 95.02% | Loss: 0.0554\n",
      "Epoch [1500/2000] | Train Acc: 97.99% | Test Acc: 94.85% | Loss: 0.0557\n",
      "Epoch [1501/2000] | Train Acc: 97.92% | Test Acc: 95.02% | Loss: 0.0557\n",
      "Epoch [1502/2000] | Train Acc: 97.99% | Test Acc: 94.87% | Loss: 0.0558\n",
      "Epoch [1503/2000] | Train Acc: 97.94% | Test Acc: 95.02% | Loss: 0.0555\n",
      "Epoch [1504/2000] | Train Acc: 97.99% | Test Acc: 94.88% | Loss: 0.0552\n",
      "Epoch [1505/2000] | Train Acc: 97.97% | Test Acc: 95.11% | Loss: 0.0547\n",
      "Epoch [1506/2000] | Train Acc: 97.99% | Test Acc: 95.02% | Loss: 0.0544\n",
      "Epoch [1507/2000] | Train Acc: 97.96% | Test Acc: 95.00% | Loss: 0.0542\n",
      "Epoch [1508/2000] | Train Acc: 97.96% | Test Acc: 95.11% | Loss: 0.0542\n",
      "Epoch [1509/2000] | Train Acc: 98.00% | Test Acc: 94.88% | Loss: 0.0543\n",
      "Epoch [1510/2000] | Train Acc: 97.95% | Test Acc: 95.08% | Loss: 0.0545\n",
      "Epoch [1511/2000] | Train Acc: 97.98% | Test Acc: 94.96% | Loss: 0.0546\n",
      "Epoch [1512/2000] | Train Acc: 97.98% | Test Acc: 95.06% | Loss: 0.0547\n",
      "Epoch [1513/2000] | Train Acc: 97.99% | Test Acc: 94.90% | Loss: 0.0547\n",
      "Epoch [1514/2000] | Train Acc: 97.97% | Test Acc: 95.08% | Loss: 0.0546\n",
      "Epoch [1515/2000] | Train Acc: 97.99% | Test Acc: 94.85% | Loss: 0.0545\n",
      "Epoch [1516/2000] | Train Acc: 97.95% | Test Acc: 95.11% | Loss: 0.0543\n",
      "Epoch [1517/2000] | Train Acc: 98.01% | Test Acc: 95.00% | Loss: 0.0542\n",
      "Epoch [1518/2000] | Train Acc: 97.97% | Test Acc: 95.03% | Loss: 0.0541\n",
      "Epoch [1519/2000] | Train Acc: 97.98% | Test Acc: 95.09% | Loss: 0.0540\n",
      "Epoch [1520/2000] | Train Acc: 98.01% | Test Acc: 94.91% | Loss: 0.0541\n",
      "Epoch [1521/2000] | Train Acc: 97.97% | Test Acc: 95.12% | Loss: 0.0541\n",
      "Epoch [1522/2000] | Train Acc: 98.01% | Test Acc: 94.88% | Loss: 0.0541\n",
      "Epoch [1523/2000] | Train Acc: 97.96% | Test Acc: 95.11% | Loss: 0.0542\n",
      "Epoch [1524/2000] | Train Acc: 98.01% | Test Acc: 94.87% | Loss: 0.0542\n",
      "Epoch [1525/2000] | Train Acc: 97.96% | Test Acc: 95.11% | Loss: 0.0542\n",
      "Epoch [1526/2000] | Train Acc: 98.01% | Test Acc: 94.87% | Loss: 0.0542\n",
      "Epoch [1527/2000] | Train Acc: 97.95% | Test Acc: 95.14% | Loss: 0.0542\n",
      "Epoch [1528/2000] | Train Acc: 98.01% | Test Acc: 94.91% | Loss: 0.0541\n",
      "Epoch [1529/2000] | Train Acc: 97.96% | Test Acc: 95.15% | Loss: 0.0541\n",
      "Epoch [1530/2000] | Train Acc: 98.01% | Test Acc: 94.94% | Loss: 0.0540\n",
      "Epoch [1531/2000] | Train Acc: 97.98% | Test Acc: 95.12% | Loss: 0.0540\n",
      "Epoch [1532/2000] | Train Acc: 98.01% | Test Acc: 95.03% | Loss: 0.0539\n",
      "Epoch [1533/2000] | Train Acc: 97.98% | Test Acc: 95.11% | Loss: 0.0539\n",
      "Epoch [1534/2000] | Train Acc: 98.01% | Test Acc: 95.05% | Loss: 0.0538\n",
      "Epoch [1535/2000] | Train Acc: 97.98% | Test Acc: 95.06% | Loss: 0.0538\n",
      "Epoch [1536/2000] | Train Acc: 97.99% | Test Acc: 95.05% | Loss: 0.0538\n",
      "Epoch [1537/2000] | Train Acc: 97.99% | Test Acc: 95.05% | Loss: 0.0538\n",
      "Epoch [1538/2000] | Train Acc: 97.98% | Test Acc: 95.08% | Loss: 0.0538\n",
      "Epoch [1539/2000] | Train Acc: 97.99% | Test Acc: 95.02% | Loss: 0.0537\n",
      "Epoch [1540/2000] | Train Acc: 97.98% | Test Acc: 95.09% | Loss: 0.0537\n",
      "Epoch [1541/2000] | Train Acc: 98.00% | Test Acc: 95.02% | Loss: 0.0537\n",
      "Epoch [1542/2000] | Train Acc: 97.99% | Test Acc: 95.15% | Loss: 0.0537\n",
      "Epoch [1543/2000] | Train Acc: 98.02% | Test Acc: 95.00% | Loss: 0.0537\n",
      "Epoch [1544/2000] | Train Acc: 97.98% | Test Acc: 95.14% | Loss: 0.0537\n",
      "Epoch [1545/2000] | Train Acc: 98.02% | Test Acc: 94.97% | Loss: 0.0537\n",
      "Epoch [1546/2000] | Train Acc: 97.97% | Test Acc: 95.15% | Loss: 0.0538\n",
      "Epoch [1547/2000] | Train Acc: 98.03% | Test Acc: 94.94% | Loss: 0.0538\n",
      "Epoch [1548/2000] | Train Acc: 97.96% | Test Acc: 95.12% | Loss: 0.0539\n",
      "Epoch [1549/2000] | Train Acc: 98.01% | Test Acc: 94.94% | Loss: 0.0540\n",
      "Epoch [1550/2000] | Train Acc: 97.99% | Test Acc: 95.05% | Loss: 0.0541\n",
      "Epoch [1551/2000] | Train Acc: 98.00% | Test Acc: 94.96% | Loss: 0.0544\n",
      "Epoch [1552/2000] | Train Acc: 98.02% | Test Acc: 95.06% | Loss: 0.0547\n",
      "Epoch [1553/2000] | Train Acc: 98.00% | Test Acc: 94.84% | Loss: 0.0552\n",
      "Epoch [1554/2000] | Train Acc: 97.90% | Test Acc: 95.08% | Loss: 0.0554\n",
      "Epoch [1555/2000] | Train Acc: 97.99% | Test Acc: 94.84% | Loss: 0.0559\n",
      "Epoch [1556/2000] | Train Acc: 97.89% | Test Acc: 95.06% | Loss: 0.0555\n",
      "Epoch [1557/2000] | Train Acc: 97.99% | Test Acc: 94.97% | Loss: 0.0552\n",
      "Epoch [1558/2000] | Train Acc: 97.99% | Test Acc: 95.15% | Loss: 0.0544\n",
      "Epoch [1559/2000] | Train Acc: 98.02% | Test Acc: 95.06% | Loss: 0.0538\n",
      "Epoch [1560/2000] | Train Acc: 97.99% | Test Acc: 95.03% | Loss: 0.0535\n",
      "Epoch [1561/2000] | Train Acc: 97.99% | Test Acc: 95.09% | Loss: 0.0535\n",
      "Epoch [1562/2000] | Train Acc: 98.01% | Test Acc: 94.97% | Loss: 0.0538\n",
      "Epoch [1563/2000] | Train Acc: 98.01% | Test Acc: 95.02% | Loss: 0.0541\n",
      "Epoch [1564/2000] | Train Acc: 98.00% | Test Acc: 94.99% | Loss: 0.0544\n",
      "Epoch [1565/2000] | Train Acc: 98.01% | Test Acc: 95.12% | Loss: 0.0543\n",
      "Epoch [1566/2000] | Train Acc: 98.01% | Test Acc: 94.94% | Loss: 0.0541\n",
      "Epoch [1567/2000] | Train Acc: 97.98% | Test Acc: 95.17% | Loss: 0.0537\n",
      "Epoch [1568/2000] | Train Acc: 98.02% | Test Acc: 95.08% | Loss: 0.0534\n",
      "Epoch [1569/2000] | Train Acc: 98.01% | Test Acc: 95.02% | Loss: 0.0533\n",
      "Epoch [1570/2000] | Train Acc: 98.00% | Test Acc: 95.14% | Loss: 0.0534\n",
      "Epoch [1571/2000] | Train Acc: 98.03% | Test Acc: 94.90% | Loss: 0.0535\n",
      "Epoch [1572/2000] | Train Acc: 97.99% | Test Acc: 95.14% | Loss: 0.0537\n",
      "Epoch [1573/2000] | Train Acc: 98.01% | Test Acc: 94.96% | Loss: 0.0538\n",
      "Epoch [1574/2000] | Train Acc: 98.00% | Test Acc: 95.12% | Loss: 0.0537\n",
      "Epoch [1575/2000] | Train Acc: 98.04% | Test Acc: 94.93% | Loss: 0.0536\n",
      "Epoch [1576/2000] | Train Acc: 97.98% | Test Acc: 95.14% | Loss: 0.0534\n",
      "Epoch [1577/2000] | Train Acc: 98.02% | Test Acc: 95.08% | Loss: 0.0533\n",
      "Epoch [1578/2000] | Train Acc: 98.00% | Test Acc: 95.06% | Loss: 0.0532\n",
      "Epoch [1579/2000] | Train Acc: 98.00% | Test Acc: 95.15% | Loss: 0.0532\n",
      "Epoch [1580/2000] | Train Acc: 98.02% | Test Acc: 94.96% | Loss: 0.0532\n",
      "Epoch [1581/2000] | Train Acc: 97.99% | Test Acc: 95.17% | Loss: 0.0533\n",
      "Epoch [1582/2000] | Train Acc: 98.03% | Test Acc: 94.93% | Loss: 0.0533\n",
      "Epoch [1583/2000] | Train Acc: 97.98% | Test Acc: 95.17% | Loss: 0.0534\n",
      "Epoch [1584/2000] | Train Acc: 98.04% | Test Acc: 94.94% | Loss: 0.0533\n",
      "Epoch [1585/2000] | Train Acc: 97.98% | Test Acc: 95.15% | Loss: 0.0533\n",
      "Epoch [1586/2000] | Train Acc: 98.04% | Test Acc: 95.02% | Loss: 0.0532\n",
      "Epoch [1587/2000] | Train Acc: 98.00% | Test Acc: 95.15% | Loss: 0.0532\n",
      "Epoch [1588/2000] | Train Acc: 98.02% | Test Acc: 95.08% | Loss: 0.0531\n",
      "Epoch [1589/2000] | Train Acc: 98.01% | Test Acc: 95.09% | Loss: 0.0531\n",
      "Epoch [1590/2000] | Train Acc: 98.01% | Test Acc: 95.09% | Loss: 0.0530\n",
      "Epoch [1591/2000] | Train Acc: 98.01% | Test Acc: 95.05% | Loss: 0.0530\n",
      "Epoch [1592/2000] | Train Acc: 98.01% | Test Acc: 95.15% | Loss: 0.0530\n",
      "Epoch [1593/2000] | Train Acc: 98.01% | Test Acc: 95.03% | Loss: 0.0530\n",
      "Epoch [1594/2000] | Train Acc: 98.00% | Test Acc: 95.17% | Loss: 0.0530\n",
      "Epoch [1595/2000] | Train Acc: 98.03% | Test Acc: 94.99% | Loss: 0.0530\n",
      "Epoch [1596/2000] | Train Acc: 98.00% | Test Acc: 95.15% | Loss: 0.0530\n",
      "Epoch [1597/2000] | Train Acc: 98.03% | Test Acc: 94.99% | Loss: 0.0530\n",
      "Epoch [1598/2000] | Train Acc: 98.01% | Test Acc: 95.15% | Loss: 0.0530\n",
      "Epoch [1599/2000] | Train Acc: 98.04% | Test Acc: 95.00% | Loss: 0.0530\n",
      "Epoch [1600/2000] | Train Acc: 97.99% | Test Acc: 95.17% | Loss: 0.0530\n",
      "Epoch [1601/2000] | Train Acc: 98.04% | Test Acc: 94.99% | Loss: 0.0530\n",
      "Epoch [1602/2000] | Train Acc: 97.99% | Test Acc: 95.15% | Loss: 0.0530\n",
      "Epoch [1603/2000] | Train Acc: 98.04% | Test Acc: 94.97% | Loss: 0.0530\n",
      "Epoch [1604/2000] | Train Acc: 97.98% | Test Acc: 95.12% | Loss: 0.0530\n",
      "Epoch [1605/2000] | Train Acc: 98.04% | Test Acc: 94.94% | Loss: 0.0530\n",
      "Epoch [1606/2000] | Train Acc: 97.98% | Test Acc: 95.14% | Loss: 0.0530\n",
      "Epoch [1607/2000] | Train Acc: 98.05% | Test Acc: 94.94% | Loss: 0.0531\n",
      "Epoch [1608/2000] | Train Acc: 97.99% | Test Acc: 95.14% | Loss: 0.0531\n",
      "Epoch [1609/2000] | Train Acc: 98.05% | Test Acc: 94.96% | Loss: 0.0531\n",
      "Epoch [1610/2000] | Train Acc: 98.01% | Test Acc: 95.11% | Loss: 0.0531\n",
      "Epoch [1611/2000] | Train Acc: 98.05% | Test Acc: 94.96% | Loss: 0.0532\n",
      "Epoch [1612/2000] | Train Acc: 98.02% | Test Acc: 95.09% | Loss: 0.0533\n",
      "Epoch [1613/2000] | Train Acc: 98.03% | Test Acc: 94.97% | Loss: 0.0534\n",
      "Epoch [1614/2000] | Train Acc: 98.04% | Test Acc: 95.08% | Loss: 0.0534\n",
      "Epoch [1615/2000] | Train Acc: 98.02% | Test Acc: 94.97% | Loss: 0.0535\n",
      "Epoch [1616/2000] | Train Acc: 98.04% | Test Acc: 95.11% | Loss: 0.0535\n",
      "Epoch [1617/2000] | Train Acc: 98.03% | Test Acc: 95.00% | Loss: 0.0536\n",
      "Epoch [1618/2000] | Train Acc: 98.04% | Test Acc: 95.08% | Loss: 0.0535\n",
      "Epoch [1619/2000] | Train Acc: 98.04% | Test Acc: 94.94% | Loss: 0.0534\n",
      "Epoch [1620/2000] | Train Acc: 98.01% | Test Acc: 95.11% | Loss: 0.0532\n",
      "Epoch [1621/2000] | Train Acc: 98.05% | Test Acc: 94.93% | Loss: 0.0531\n",
      "Epoch [1622/2000] | Train Acc: 98.00% | Test Acc: 95.15% | Loss: 0.0529\n",
      "Epoch [1623/2000] | Train Acc: 98.04% | Test Acc: 95.03% | Loss: 0.0528\n",
      "Epoch [1624/2000] | Train Acc: 98.01% | Test Acc: 95.14% | Loss: 0.0527\n",
      "Epoch [1625/2000] | Train Acc: 98.02% | Test Acc: 95.08% | Loss: 0.0526\n",
      "Epoch [1626/2000] | Train Acc: 98.02% | Test Acc: 95.02% | Loss: 0.0526\n",
      "Epoch [1627/2000] | Train Acc: 98.02% | Test Acc: 95.15% | Loss: 0.0526\n",
      "Epoch [1628/2000] | Train Acc: 98.03% | Test Acc: 94.97% | Loss: 0.0526\n",
      "Epoch [1629/2000] | Train Acc: 98.01% | Test Acc: 95.15% | Loss: 0.0526\n",
      "Epoch [1630/2000] | Train Acc: 98.05% | Test Acc: 94.96% | Loss: 0.0527\n",
      "Epoch [1631/2000] | Train Acc: 98.00% | Test Acc: 95.12% | Loss: 0.0527\n",
      "Epoch [1632/2000] | Train Acc: 98.06% | Test Acc: 94.96% | Loss: 0.0528\n",
      "Epoch [1633/2000] | Train Acc: 98.01% | Test Acc: 95.11% | Loss: 0.0528\n",
      "Epoch [1634/2000] | Train Acc: 98.06% | Test Acc: 95.00% | Loss: 0.0529\n",
      "Epoch [1635/2000] | Train Acc: 98.03% | Test Acc: 95.11% | Loss: 0.0529\n",
      "Epoch [1636/2000] | Train Acc: 98.05% | Test Acc: 94.93% | Loss: 0.0530\n",
      "Epoch [1637/2000] | Train Acc: 98.03% | Test Acc: 95.11% | Loss: 0.0529\n",
      "Epoch [1638/2000] | Train Acc: 98.04% | Test Acc: 94.97% | Loss: 0.0530\n",
      "Epoch [1639/2000] | Train Acc: 98.02% | Test Acc: 95.09% | Loss: 0.0529\n",
      "Epoch [1640/2000] | Train Acc: 98.05% | Test Acc: 94.99% | Loss: 0.0529\n",
      "Epoch [1641/2000] | Train Acc: 98.02% | Test Acc: 95.09% | Loss: 0.0529\n",
      "Epoch [1642/2000] | Train Acc: 98.06% | Test Acc: 94.94% | Loss: 0.0528\n",
      "Epoch [1643/2000] | Train Acc: 98.03% | Test Acc: 95.12% | Loss: 0.0527\n",
      "Epoch [1644/2000] | Train Acc: 98.06% | Test Acc: 94.93% | Loss: 0.0526\n",
      "Epoch [1645/2000] | Train Acc: 98.01% | Test Acc: 95.15% | Loss: 0.0526\n",
      "Epoch [1646/2000] | Train Acc: 98.05% | Test Acc: 94.97% | Loss: 0.0525\n",
      "Epoch [1647/2000] | Train Acc: 98.02% | Test Acc: 95.15% | Loss: 0.0524\n",
      "Epoch [1648/2000] | Train Acc: 98.04% | Test Acc: 95.06% | Loss: 0.0524\n",
      "Epoch [1649/2000] | Train Acc: 98.03% | Test Acc: 95.14% | Loss: 0.0523\n",
      "Epoch [1650/2000] | Train Acc: 98.03% | Test Acc: 95.06% | Loss: 0.0523\n",
      "Epoch [1651/2000] | Train Acc: 98.02% | Test Acc: 95.06% | Loss: 0.0523\n",
      "Epoch [1652/2000] | Train Acc: 98.03% | Test Acc: 95.06% | Loss: 0.0522\n",
      "Epoch [1653/2000] | Train Acc: 98.03% | Test Acc: 95.05% | Loss: 0.0522\n",
      "Epoch [1654/2000] | Train Acc: 98.03% | Test Acc: 95.14% | Loss: 0.0522\n",
      "Epoch [1655/2000] | Train Acc: 98.04% | Test Acc: 95.05% | Loss: 0.0522\n",
      "Epoch [1656/2000] | Train Acc: 98.03% | Test Acc: 95.14% | Loss: 0.0522\n",
      "Epoch [1657/2000] | Train Acc: 98.03% | Test Acc: 95.05% | Loss: 0.0522\n",
      "Epoch [1658/2000] | Train Acc: 98.03% | Test Acc: 95.15% | Loss: 0.0522\n",
      "Epoch [1659/2000] | Train Acc: 98.04% | Test Acc: 95.05% | Loss: 0.0522\n",
      "Epoch [1660/2000] | Train Acc: 98.02% | Test Acc: 95.15% | Loss: 0.0522\n",
      "Epoch [1661/2000] | Train Acc: 98.06% | Test Acc: 94.96% | Loss: 0.0523\n",
      "Epoch [1662/2000] | Train Acc: 98.02% | Test Acc: 95.12% | Loss: 0.0523\n",
      "Epoch [1663/2000] | Train Acc: 98.07% | Test Acc: 94.93% | Loss: 0.0524\n",
      "Epoch [1664/2000] | Train Acc: 98.04% | Test Acc: 95.11% | Loss: 0.0525\n",
      "Epoch [1665/2000] | Train Acc: 98.05% | Test Acc: 94.97% | Loss: 0.0527\n",
      "Epoch [1666/2000] | Train Acc: 98.05% | Test Acc: 95.05% | Loss: 0.0529\n",
      "Epoch [1667/2000] | Train Acc: 98.03% | Test Acc: 94.85% | Loss: 0.0533\n",
      "Epoch [1668/2000] | Train Acc: 98.00% | Test Acc: 95.11% | Loss: 0.0536\n",
      "Epoch [1669/2000] | Train Acc: 98.02% | Test Acc: 94.84% | Loss: 0.0542\n",
      "Epoch [1670/2000] | Train Acc: 97.89% | Test Acc: 95.09% | Loss: 0.0542\n",
      "Epoch [1671/2000] | Train Acc: 98.03% | Test Acc: 94.87% | Loss: 0.0544\n",
      "Epoch [1672/2000] | Train Acc: 97.97% | Test Acc: 95.08% | Loss: 0.0536\n",
      "Epoch [1673/2000] | Train Acc: 98.04% | Test Acc: 94.94% | Loss: 0.0530\n",
      "Epoch [1674/2000] | Train Acc: 98.02% | Test Acc: 95.12% | Loss: 0.0523\n",
      "Epoch [1675/2000] | Train Acc: 98.03% | Test Acc: 95.12% | Loss: 0.0520\n",
      "Epoch [1676/2000] | Train Acc: 98.04% | Test Acc: 94.91% | Loss: 0.0520\n",
      "Epoch [1677/2000] | Train Acc: 98.02% | Test Acc: 95.03% | Loss: 0.0523\n",
      "Epoch [1678/2000] | Train Acc: 98.05% | Test Acc: 94.91% | Loss: 0.0527\n",
      "Epoch [1679/2000] | Train Acc: 98.04% | Test Acc: 95.08% | Loss: 0.0528\n",
      "Epoch [1680/2000] | Train Acc: 98.05% | Test Acc: 94.94% | Loss: 0.0529\n",
      "Epoch [1681/2000] | Train Acc: 98.04% | Test Acc: 95.09% | Loss: 0.0526\n",
      "Epoch [1682/2000] | Train Acc: 98.06% | Test Acc: 94.96% | Loss: 0.0523\n",
      "Epoch [1683/2000] | Train Acc: 98.04% | Test Acc: 95.06% | Loss: 0.0520\n",
      "Epoch [1684/2000] | Train Acc: 98.03% | Test Acc: 95.08% | Loss: 0.0519\n",
      "Epoch [1685/2000] | Train Acc: 98.06% | Test Acc: 94.93% | Loss: 0.0519\n",
      "Epoch [1686/2000] | Train Acc: 98.03% | Test Acc: 95.05% | Loss: 0.0521\n",
      "Epoch [1687/2000] | Train Acc: 98.06% | Test Acc: 94.88% | Loss: 0.0522\n",
      "Epoch [1688/2000] | Train Acc: 98.04% | Test Acc: 95.05% | Loss: 0.0523\n",
      "Epoch [1689/2000] | Train Acc: 98.07% | Test Acc: 94.90% | Loss: 0.0523\n",
      "Epoch [1690/2000] | Train Acc: 98.04% | Test Acc: 95.09% | Loss: 0.0521\n",
      "Epoch [1691/2000] | Train Acc: 98.08% | Test Acc: 95.05% | Loss: 0.0520\n",
      "Epoch [1692/2000] | Train Acc: 98.05% | Test Acc: 95.06% | Loss: 0.0518\n",
      "Epoch [1693/2000] | Train Acc: 98.05% | Test Acc: 95.08% | Loss: 0.0518\n",
      "Epoch [1694/2000] | Train Acc: 98.05% | Test Acc: 95.00% | Loss: 0.0518\n",
      "Epoch [1695/2000] | Train Acc: 98.05% | Test Acc: 95.09% | Loss: 0.0518\n",
      "Epoch [1696/2000] | Train Acc: 98.07% | Test Acc: 94.90% | Loss: 0.0518\n",
      "Epoch [1697/2000] | Train Acc: 98.02% | Test Acc: 95.06% | Loss: 0.0519\n",
      "Epoch [1698/2000] | Train Acc: 98.08% | Test Acc: 94.88% | Loss: 0.0519\n",
      "Epoch [1699/2000] | Train Acc: 98.01% | Test Acc: 95.06% | Loss: 0.0519\n",
      "Epoch [1700/2000] | Train Acc: 98.08% | Test Acc: 94.97% | Loss: 0.0519\n",
      "Epoch [1701/2000] | Train Acc: 98.04% | Test Acc: 95.06% | Loss: 0.0518\n",
      "Epoch [1702/2000] | Train Acc: 98.07% | Test Acc: 95.02% | Loss: 0.0517\n",
      "Epoch [1703/2000] | Train Acc: 98.05% | Test Acc: 95.08% | Loss: 0.0517\n",
      "Epoch [1704/2000] | Train Acc: 98.05% | Test Acc: 95.03% | Loss: 0.0516\n",
      "Epoch [1705/2000] | Train Acc: 98.04% | Test Acc: 95.03% | Loss: 0.0516\n",
      "Epoch [1706/2000] | Train Acc: 98.04% | Test Acc: 95.09% | Loss: 0.0516\n",
      "Epoch [1707/2000] | Train Acc: 98.06% | Test Acc: 94.99% | Loss: 0.0516\n",
      "Epoch [1708/2000] | Train Acc: 98.04% | Test Acc: 95.08% | Loss: 0.0516\n",
      "Epoch [1709/2000] | Train Acc: 98.07% | Test Acc: 95.02% | Loss: 0.0516\n",
      "Epoch [1710/2000] | Train Acc: 98.06% | Test Acc: 95.03% | Loss: 0.0516\n",
      "Epoch [1711/2000] | Train Acc: 98.05% | Test Acc: 95.03% | Loss: 0.0516\n",
      "Epoch [1712/2000] | Train Acc: 98.05% | Test Acc: 95.06% | Loss: 0.0516\n",
      "Epoch [1713/2000] | Train Acc: 98.07% | Test Acc: 94.99% | Loss: 0.0516\n",
      "Epoch [1714/2000] | Train Acc: 98.05% | Test Acc: 95.08% | Loss: 0.0516\n",
      "Epoch [1715/2000] | Train Acc: 98.08% | Test Acc: 94.99% | Loss: 0.0516\n",
      "Epoch [1716/2000] | Train Acc: 98.05% | Test Acc: 95.08% | Loss: 0.0516\n",
      "Epoch [1717/2000] | Train Acc: 98.08% | Test Acc: 94.97% | Loss: 0.0516\n",
      "Epoch [1718/2000] | Train Acc: 98.04% | Test Acc: 95.06% | Loss: 0.0516\n",
      "Epoch [1719/2000] | Train Acc: 98.07% | Test Acc: 94.96% | Loss: 0.0516\n",
      "Epoch [1720/2000] | Train Acc: 98.03% | Test Acc: 95.08% | Loss: 0.0516\n",
      "Epoch [1721/2000] | Train Acc: 98.07% | Test Acc: 94.90% | Loss: 0.0516\n",
      "Epoch [1722/2000] | Train Acc: 98.02% | Test Acc: 95.08% | Loss: 0.0516\n",
      "Epoch [1723/2000] | Train Acc: 98.07% | Test Acc: 94.90% | Loss: 0.0517\n",
      "Epoch [1724/2000] | Train Acc: 98.02% | Test Acc: 95.06% | Loss: 0.0517\n",
      "Epoch [1725/2000] | Train Acc: 98.07% | Test Acc: 94.93% | Loss: 0.0517\n",
      "Epoch [1726/2000] | Train Acc: 98.04% | Test Acc: 95.05% | Loss: 0.0518\n",
      "Epoch [1727/2000] | Train Acc: 98.06% | Test Acc: 94.91% | Loss: 0.0519\n",
      "Epoch [1728/2000] | Train Acc: 98.04% | Test Acc: 95.03% | Loss: 0.0519\n",
      "Epoch [1729/2000] | Train Acc: 98.06% | Test Acc: 94.88% | Loss: 0.0520\n",
      "Epoch [1730/2000] | Train Acc: 98.06% | Test Acc: 95.05% | Loss: 0.0521\n",
      "Epoch [1731/2000] | Train Acc: 98.06% | Test Acc: 94.90% | Loss: 0.0522\n",
      "Epoch [1732/2000] | Train Acc: 98.06% | Test Acc: 95.03% | Loss: 0.0522\n",
      "Epoch [1733/2000] | Train Acc: 98.06% | Test Acc: 94.88% | Loss: 0.0522\n",
      "Epoch [1734/2000] | Train Acc: 98.06% | Test Acc: 95.00% | Loss: 0.0521\n",
      "Epoch [1735/2000] | Train Acc: 98.07% | Test Acc: 94.90% | Loss: 0.0520\n",
      "Epoch [1736/2000] | Train Acc: 98.04% | Test Acc: 95.08% | Loss: 0.0518\n",
      "Epoch [1737/2000] | Train Acc: 98.07% | Test Acc: 94.88% | Loss: 0.0516\n",
      "Epoch [1738/2000] | Train Acc: 98.02% | Test Acc: 95.05% | Loss: 0.0515\n",
      "Epoch [1739/2000] | Train Acc: 98.07% | Test Acc: 95.03% | Loss: 0.0513\n",
      "Epoch [1740/2000] | Train Acc: 98.06% | Test Acc: 95.05% | Loss: 0.0512\n",
      "Epoch [1741/2000] | Train Acc: 98.05% | Test Acc: 95.06% | Loss: 0.0512\n",
      "Epoch [1742/2000] | Train Acc: 98.07% | Test Acc: 95.02% | Loss: 0.0512\n",
      "Epoch [1743/2000] | Train Acc: 98.07% | Test Acc: 95.05% | Loss: 0.0512\n",
      "Epoch [1744/2000] | Train Acc: 98.07% | Test Acc: 94.91% | Loss: 0.0513\n",
      "Epoch [1745/2000] | Train Acc: 98.05% | Test Acc: 95.05% | Loss: 0.0513\n",
      "Epoch [1746/2000] | Train Acc: 98.08% | Test Acc: 94.88% | Loss: 0.0514\n",
      "Epoch [1747/2000] | Train Acc: 98.03% | Test Acc: 95.03% | Loss: 0.0514\n",
      "Epoch [1748/2000] | Train Acc: 98.08% | Test Acc: 94.91% | Loss: 0.0515\n",
      "Epoch [1749/2000] | Train Acc: 98.04% | Test Acc: 95.05% | Loss: 0.0515\n",
      "Epoch [1750/2000] | Train Acc: 98.08% | Test Acc: 94.90% | Loss: 0.0515\n",
      "Epoch [1751/2000] | Train Acc: 98.04% | Test Acc: 95.05% | Loss: 0.0515\n",
      "Epoch [1752/2000] | Train Acc: 98.08% | Test Acc: 94.91% | Loss: 0.0516\n",
      "Epoch [1753/2000] | Train Acc: 98.05% | Test Acc: 95.06% | Loss: 0.0515\n",
      "Epoch [1754/2000] | Train Acc: 98.08% | Test Acc: 94.91% | Loss: 0.0515\n",
      "Epoch [1755/2000] | Train Acc: 98.05% | Test Acc: 95.05% | Loss: 0.0515\n",
      "Epoch [1756/2000] | Train Acc: 98.08% | Test Acc: 94.91% | Loss: 0.0515\n",
      "Epoch [1757/2000] | Train Acc: 98.04% | Test Acc: 95.05% | Loss: 0.0514\n",
      "Epoch [1758/2000] | Train Acc: 98.08% | Test Acc: 94.88% | Loss: 0.0513\n",
      "Epoch [1759/2000] | Train Acc: 98.02% | Test Acc: 95.05% | Loss: 0.0512\n",
      "Epoch [1760/2000] | Train Acc: 98.08% | Test Acc: 94.94% | Loss: 0.0512\n",
      "Epoch [1761/2000] | Train Acc: 98.05% | Test Acc: 95.05% | Loss: 0.0511\n",
      "Epoch [1762/2000] | Train Acc: 98.07% | Test Acc: 94.99% | Loss: 0.0511\n",
      "Epoch [1763/2000] | Train Acc: 98.08% | Test Acc: 95.05% | Loss: 0.0510\n",
      "Epoch [1764/2000] | Train Acc: 98.06% | Test Acc: 95.02% | Loss: 0.0510\n",
      "Epoch [1765/2000] | Train Acc: 98.06% | Test Acc: 95.05% | Loss: 0.0510\n",
      "Epoch [1766/2000] | Train Acc: 98.07% | Test Acc: 95.03% | Loss: 0.0509\n",
      "Epoch [1767/2000] | Train Acc: 98.07% | Test Acc: 95.06% | Loss: 0.0509\n",
      "Epoch [1768/2000] | Train Acc: 98.07% | Test Acc: 95.05% | Loss: 0.0509\n",
      "Epoch [1769/2000] | Train Acc: 98.05% | Test Acc: 95.06% | Loss: 0.0509\n",
      "Epoch [1770/2000] | Train Acc: 98.06% | Test Acc: 95.03% | Loss: 0.0509\n",
      "Epoch [1771/2000] | Train Acc: 98.04% | Test Acc: 95.05% | Loss: 0.0509\n",
      "Epoch [1772/2000] | Train Acc: 98.05% | Test Acc: 95.05% | Loss: 0.0509\n",
      "Epoch [1773/2000] | Train Acc: 98.06% | Test Acc: 95.05% | Loss: 0.0508\n",
      "Epoch [1774/2000] | Train Acc: 98.05% | Test Acc: 95.06% | Loss: 0.0508\n",
      "Epoch [1775/2000] | Train Acc: 98.06% | Test Acc: 95.03% | Loss: 0.0508\n",
      "Epoch [1776/2000] | Train Acc: 98.05% | Test Acc: 95.03% | Loss: 0.0508\n",
      "Epoch [1777/2000] | Train Acc: 98.05% | Test Acc: 95.05% | Loss: 0.0508\n",
      "Epoch [1778/2000] | Train Acc: 98.07% | Test Acc: 95.02% | Loss: 0.0508\n",
      "Epoch [1779/2000] | Train Acc: 98.07% | Test Acc: 95.02% | Loss: 0.0508\n",
      "Epoch [1780/2000] | Train Acc: 98.07% | Test Acc: 95.05% | Loss: 0.0508\n",
      "Epoch [1781/2000] | Train Acc: 98.07% | Test Acc: 94.99% | Loss: 0.0508\n",
      "Epoch [1782/2000] | Train Acc: 98.08% | Test Acc: 95.08% | Loss: 0.0508\n",
      "Epoch [1783/2000] | Train Acc: 98.07% | Test Acc: 94.97% | Loss: 0.0508\n",
      "Epoch [1784/2000] | Train Acc: 98.08% | Test Acc: 95.03% | Loss: 0.0508\n",
      "Epoch [1785/2000] | Train Acc: 98.09% | Test Acc: 94.90% | Loss: 0.0509\n",
      "Epoch [1786/2000] | Train Acc: 98.04% | Test Acc: 95.06% | Loss: 0.0510\n",
      "Epoch [1787/2000] | Train Acc: 98.08% | Test Acc: 94.91% | Loss: 0.0512\n",
      "Epoch [1788/2000] | Train Acc: 98.09% | Test Acc: 95.02% | Loss: 0.0515\n",
      "Epoch [1789/2000] | Train Acc: 98.09% | Test Acc: 94.84% | Loss: 0.0522\n",
      "Epoch [1790/2000] | Train Acc: 97.98% | Test Acc: 94.99% | Loss: 0.0528\n",
      "Epoch [1791/2000] | Train Acc: 98.02% | Test Acc: 94.75% | Loss: 0.0542\n",
      "Epoch [1792/2000] | Train Acc: 97.83% | Test Acc: 94.97% | Loss: 0.0544\n",
      "Epoch [1793/2000] | Train Acc: 98.01% | Test Acc: 94.77% | Loss: 0.0550\n",
      "Epoch [1794/2000] | Train Acc: 97.90% | Test Acc: 94.99% | Loss: 0.0531\n",
      "Epoch [1795/2000] | Train Acc: 98.08% | Test Acc: 95.03% | Loss: 0.0517\n",
      "Epoch [1796/2000] | Train Acc: 98.05% | Test Acc: 94.96% | Loss: 0.0507\n",
      "Epoch [1797/2000] | Train Acc: 98.08% | Test Acc: 95.00% | Loss: 0.0509\n",
      "Epoch [1798/2000] | Train Acc: 98.09% | Test Acc: 94.91% | Loss: 0.0518\n",
      "Epoch [1799/2000] | Train Acc: 97.99% | Test Acc: 95.06% | Loss: 0.0524\n",
      "Epoch [1800/2000] | Train Acc: 98.09% | Test Acc: 94.84% | Loss: 0.0526\n",
      "Epoch [1801/2000] | Train Acc: 98.07% | Test Acc: 95.03% | Loss: 0.0516\n",
      "Epoch [1802/2000] | Train Acc: 98.08% | Test Acc: 95.03% | Loss: 0.0508\n",
      "Epoch [1803/2000] | Train Acc: 98.07% | Test Acc: 94.88% | Loss: 0.0506\n",
      "Epoch [1804/2000] | Train Acc: 98.05% | Test Acc: 95.03% | Loss: 0.0509\n",
      "Epoch [1805/2000] | Train Acc: 98.08% | Test Acc: 94.88% | Loss: 0.0515\n",
      "Epoch [1806/2000] | Train Acc: 98.10% | Test Acc: 95.06% | Loss: 0.0515\n",
      "Epoch [1807/2000] | Train Acc: 98.08% | Test Acc: 94.85% | Loss: 0.0513\n",
      "Epoch [1808/2000] | Train Acc: 98.06% | Test Acc: 95.09% | Loss: 0.0508\n",
      "Epoch [1809/2000] | Train Acc: 98.08% | Test Acc: 95.05% | Loss: 0.0505\n",
      "Epoch [1810/2000] | Train Acc: 98.07% | Test Acc: 94.91% | Loss: 0.0506\n",
      "Epoch [1811/2000] | Train Acc: 98.05% | Test Acc: 95.03% | Loss: 0.0508\n",
      "Epoch [1812/2000] | Train Acc: 98.08% | Test Acc: 94.96% | Loss: 0.0510\n",
      "Epoch [1813/2000] | Train Acc: 98.08% | Test Acc: 95.03% | Loss: 0.0509\n",
      "Epoch [1814/2000] | Train Acc: 98.08% | Test Acc: 95.00% | Loss: 0.0507\n",
      "Epoch [1815/2000] | Train Acc: 98.10% | Test Acc: 94.97% | Loss: 0.0505\n",
      "Epoch [1816/2000] | Train Acc: 98.07% | Test Acc: 95.06% | Loss: 0.0504\n",
      "Epoch [1817/2000] | Train Acc: 98.07% | Test Acc: 94.90% | Loss: 0.0505\n",
      "Epoch [1818/2000] | Train Acc: 98.05% | Test Acc: 95.05% | Loss: 0.0506\n",
      "Epoch [1819/2000] | Train Acc: 98.09% | Test Acc: 94.90% | Loss: 0.0507\n",
      "Epoch [1820/2000] | Train Acc: 98.05% | Test Acc: 95.03% | Loss: 0.0506\n",
      "Epoch [1821/2000] | Train Acc: 98.08% | Test Acc: 95.00% | Loss: 0.0505\n",
      "Epoch [1822/2000] | Train Acc: 98.09% | Test Acc: 94.99% | Loss: 0.0504\n",
      "Epoch [1823/2000] | Train Acc: 98.06% | Test Acc: 95.06% | Loss: 0.0503\n",
      "Epoch [1824/2000] | Train Acc: 98.07% | Test Acc: 95.00% | Loss: 0.0504\n",
      "Epoch [1825/2000] | Train Acc: 98.10% | Test Acc: 95.06% | Loss: 0.0504\n",
      "Epoch [1826/2000] | Train Acc: 98.09% | Test Acc: 94.91% | Loss: 0.0504\n",
      "Epoch [1827/2000] | Train Acc: 98.07% | Test Acc: 95.08% | Loss: 0.0504\n",
      "Epoch [1828/2000] | Train Acc: 98.09% | Test Acc: 94.99% | Loss: 0.0504\n",
      "Epoch [1829/2000] | Train Acc: 98.10% | Test Acc: 95.02% | Loss: 0.0503\n",
      "Epoch [1830/2000] | Train Acc: 98.07% | Test Acc: 94.97% | Loss: 0.0503\n",
      "Epoch [1831/2000] | Train Acc: 98.06% | Test Acc: 94.99% | Loss: 0.0502\n",
      "Epoch [1832/2000] | Train Acc: 98.08% | Test Acc: 95.06% | Loss: 0.0502\n",
      "Epoch [1833/2000] | Train Acc: 98.07% | Test Acc: 95.00% | Loss: 0.0503\n",
      "Epoch [1834/2000] | Train Acc: 98.10% | Test Acc: 95.06% | Loss: 0.0503\n",
      "Epoch [1835/2000] | Train Acc: 98.08% | Test Acc: 95.00% | Loss: 0.0503\n",
      "Epoch [1836/2000] | Train Acc: 98.10% | Test Acc: 95.05% | Loss: 0.0503\n",
      "Epoch [1837/2000] | Train Acc: 98.07% | Test Acc: 95.00% | Loss: 0.0502\n",
      "Epoch [1838/2000] | Train Acc: 98.09% | Test Acc: 95.03% | Loss: 0.0502\n",
      "Epoch [1839/2000] | Train Acc: 98.08% | Test Acc: 95.02% | Loss: 0.0502\n",
      "Epoch [1840/2000] | Train Acc: 98.07% | Test Acc: 95.00% | Loss: 0.0502\n",
      "Epoch [1841/2000] | Train Acc: 98.07% | Test Acc: 95.05% | Loss: 0.0501\n",
      "Epoch [1842/2000] | Train Acc: 98.07% | Test Acc: 95.00% | Loss: 0.0501\n",
      "Epoch [1843/2000] | Train Acc: 98.09% | Test Acc: 95.05% | Loss: 0.0501\n",
      "Epoch [1844/2000] | Train Acc: 98.07% | Test Acc: 95.02% | Loss: 0.0501\n",
      "Epoch [1845/2000] | Train Acc: 98.09% | Test Acc: 95.05% | Loss: 0.0501\n",
      "Epoch [1846/2000] | Train Acc: 98.07% | Test Acc: 95.00% | Loss: 0.0501\n",
      "Epoch [1847/2000] | Train Acc: 98.09% | Test Acc: 95.02% | Loss: 0.0501\n",
      "Epoch [1848/2000] | Train Acc: 98.07% | Test Acc: 94.96% | Loss: 0.0501\n",
      "Epoch [1849/2000] | Train Acc: 98.08% | Test Acc: 95.05% | Loss: 0.0501\n",
      "Epoch [1850/2000] | Train Acc: 98.07% | Test Acc: 95.00% | Loss: 0.0501\n",
      "Epoch [1851/2000] | Train Acc: 98.08% | Test Acc: 95.03% | Loss: 0.0500\n",
      "Epoch [1852/2000] | Train Acc: 98.07% | Test Acc: 95.02% | Loss: 0.0500\n",
      "Epoch [1853/2000] | Train Acc: 98.08% | Test Acc: 95.00% | Loss: 0.0500\n",
      "Epoch [1854/2000] | Train Acc: 98.07% | Test Acc: 95.03% | Loss: 0.0500\n",
      "Epoch [1855/2000] | Train Acc: 98.08% | Test Acc: 94.97% | Loss: 0.0500\n",
      "Epoch [1856/2000] | Train Acc: 98.09% | Test Acc: 95.03% | Loss: 0.0500\n",
      "Epoch [1857/2000] | Train Acc: 98.08% | Test Acc: 94.97% | Loss: 0.0500\n",
      "Epoch [1858/2000] | Train Acc: 98.09% | Test Acc: 95.02% | Loss: 0.0500\n",
      "Epoch [1859/2000] | Train Acc: 98.08% | Test Acc: 94.97% | Loss: 0.0500\n",
      "Epoch [1860/2000] | Train Acc: 98.09% | Test Acc: 95.03% | Loss: 0.0500\n",
      "Epoch [1861/2000] | Train Acc: 98.08% | Test Acc: 95.00% | Loss: 0.0500\n",
      "Epoch [1862/2000] | Train Acc: 98.09% | Test Acc: 95.03% | Loss: 0.0499\n",
      "Epoch [1863/2000] | Train Acc: 98.08% | Test Acc: 95.00% | Loss: 0.0499\n",
      "Epoch [1864/2000] | Train Acc: 98.09% | Test Acc: 95.05% | Loss: 0.0499\n",
      "Epoch [1865/2000] | Train Acc: 98.08% | Test Acc: 95.02% | Loss: 0.0499\n",
      "Epoch [1866/2000] | Train Acc: 98.09% | Test Acc: 95.05% | Loss: 0.0499\n",
      "Epoch [1867/2000] | Train Acc: 98.07% | Test Acc: 95.02% | Loss: 0.0499\n",
      "Epoch [1868/2000] | Train Acc: 98.10% | Test Acc: 95.05% | Loss: 0.0499\n",
      "Epoch [1869/2000] | Train Acc: 98.08% | Test Acc: 95.00% | Loss: 0.0499\n",
      "Epoch [1870/2000] | Train Acc: 98.10% | Test Acc: 95.03% | Loss: 0.0499\n",
      "Epoch [1871/2000] | Train Acc: 98.08% | Test Acc: 94.93% | Loss: 0.0499\n",
      "Epoch [1872/2000] | Train Acc: 98.09% | Test Acc: 95.03% | Loss: 0.0500\n",
      "Epoch [1873/2000] | Train Acc: 98.09% | Test Acc: 94.96% | Loss: 0.0500\n",
      "Epoch [1874/2000] | Train Acc: 98.07% | Test Acc: 95.05% | Loss: 0.0501\n",
      "Epoch [1875/2000] | Train Acc: 98.10% | Test Acc: 94.87% | Loss: 0.0502\n",
      "Epoch [1876/2000] | Train Acc: 98.08% | Test Acc: 95.05% | Loss: 0.0503\n",
      "Epoch [1877/2000] | Train Acc: 98.10% | Test Acc: 94.91% | Loss: 0.0505\n",
      "Epoch [1878/2000] | Train Acc: 98.12% | Test Acc: 95.02% | Loss: 0.0507\n",
      "Epoch [1879/2000] | Train Acc: 98.09% | Test Acc: 94.87% | Loss: 0.0511\n",
      "Epoch [1880/2000] | Train Acc: 98.11% | Test Acc: 95.11% | Loss: 0.0512\n",
      "Epoch [1881/2000] | Train Acc: 98.09% | Test Acc: 94.87% | Loss: 0.0516\n",
      "Epoch [1882/2000] | Train Acc: 98.09% | Test Acc: 95.03% | Loss: 0.0513\n",
      "Epoch [1883/2000] | Train Acc: 98.09% | Test Acc: 94.94% | Loss: 0.0513\n",
      "Epoch [1884/2000] | Train Acc: 98.10% | Test Acc: 95.06% | Loss: 0.0507\n",
      "Epoch [1885/2000] | Train Acc: 98.10% | Test Acc: 94.97% | Loss: 0.0502\n",
      "Epoch [1886/2000] | Train Acc: 98.09% | Test Acc: 95.05% | Loss: 0.0499\n",
      "Epoch [1887/2000] | Train Acc: 98.09% | Test Acc: 95.05% | Loss: 0.0497\n",
      "Epoch [1888/2000] | Train Acc: 98.07% | Test Acc: 94.96% | Loss: 0.0497\n",
      "Epoch [1889/2000] | Train Acc: 98.08% | Test Acc: 95.05% | Loss: 0.0499\n",
      "Epoch [1890/2000] | Train Acc: 98.09% | Test Acc: 94.88% | Loss: 0.0501\n",
      "Epoch [1891/2000] | Train Acc: 98.10% | Test Acc: 95.06% | Loss: 0.0502\n",
      "Epoch [1892/2000] | Train Acc: 98.10% | Test Acc: 94.91% | Loss: 0.0503\n",
      "Epoch [1893/2000] | Train Acc: 98.10% | Test Acc: 95.09% | Loss: 0.0502\n",
      "Epoch [1894/2000] | Train Acc: 98.11% | Test Acc: 94.93% | Loss: 0.0502\n",
      "Epoch [1895/2000] | Train Acc: 98.08% | Test Acc: 95.03% | Loss: 0.0499\n",
      "Epoch [1896/2000] | Train Acc: 98.09% | Test Acc: 95.05% | Loss: 0.0498\n",
      "Epoch [1897/2000] | Train Acc: 98.10% | Test Acc: 95.06% | Loss: 0.0496\n",
      "Epoch [1898/2000] | Train Acc: 98.09% | Test Acc: 95.05% | Loss: 0.0496\n",
      "Epoch [1899/2000] | Train Acc: 98.08% | Test Acc: 95.06% | Loss: 0.0496\n",
      "Epoch [1900/2000] | Train Acc: 98.11% | Test Acc: 95.06% | Loss: 0.0496\n",
      "Epoch [1901/2000] | Train Acc: 98.10% | Test Acc: 94.94% | Loss: 0.0497\n",
      "Epoch [1902/2000] | Train Acc: 98.09% | Test Acc: 95.05% | Loss: 0.0498\n",
      "Epoch [1903/2000] | Train Acc: 98.10% | Test Acc: 94.93% | Loss: 0.0498\n",
      "Epoch [1904/2000] | Train Acc: 98.09% | Test Acc: 95.06% | Loss: 0.0498\n",
      "Epoch [1905/2000] | Train Acc: 98.10% | Test Acc: 94.93% | Loss: 0.0498\n",
      "Epoch [1906/2000] | Train Acc: 98.08% | Test Acc: 95.05% | Loss: 0.0497\n",
      "Epoch [1907/2000] | Train Acc: 98.10% | Test Acc: 95.00% | Loss: 0.0497\n",
      "Epoch [1908/2000] | Train Acc: 98.11% | Test Acc: 95.08% | Loss: 0.0496\n",
      "Epoch [1909/2000] | Train Acc: 98.09% | Test Acc: 95.03% | Loss: 0.0495\n",
      "Epoch [1910/2000] | Train Acc: 98.10% | Test Acc: 95.06% | Loss: 0.0495\n",
      "Epoch [1911/2000] | Train Acc: 98.09% | Test Acc: 95.09% | Loss: 0.0495\n",
      "Epoch [1912/2000] | Train Acc: 98.11% | Test Acc: 95.08% | Loss: 0.0494\n",
      "Epoch [1913/2000] | Train Acc: 98.10% | Test Acc: 95.09% | Loss: 0.0494\n",
      "Epoch [1914/2000] | Train Acc: 98.09% | Test Acc: 95.03% | Loss: 0.0494\n",
      "Epoch [1915/2000] | Train Acc: 98.11% | Test Acc: 95.08% | Loss: 0.0494\n",
      "Epoch [1916/2000] | Train Acc: 98.08% | Test Acc: 95.08% | Loss: 0.0495\n",
      "Epoch [1917/2000] | Train Acc: 98.11% | Test Acc: 95.11% | Loss: 0.0495\n",
      "Epoch [1918/2000] | Train Acc: 98.09% | Test Acc: 95.02% | Loss: 0.0495\n",
      "Epoch [1919/2000] | Train Acc: 98.11% | Test Acc: 95.09% | Loss: 0.0495\n",
      "Epoch [1920/2000] | Train Acc: 98.10% | Test Acc: 94.96% | Loss: 0.0495\n",
      "Epoch [1921/2000] | Train Acc: 98.10% | Test Acc: 95.06% | Loss: 0.0495\n",
      "Epoch [1922/2000] | Train Acc: 98.11% | Test Acc: 94.93% | Loss: 0.0495\n",
      "Epoch [1923/2000] | Train Acc: 98.09% | Test Acc: 95.06% | Loss: 0.0496\n",
      "Epoch [1924/2000] | Train Acc: 98.10% | Test Acc: 94.94% | Loss: 0.0496\n",
      "Epoch [1925/2000] | Train Acc: 98.09% | Test Acc: 95.09% | Loss: 0.0496\n",
      "Epoch [1926/2000] | Train Acc: 98.10% | Test Acc: 94.96% | Loss: 0.0496\n",
      "Epoch [1927/2000] | Train Acc: 98.10% | Test Acc: 95.09% | Loss: 0.0497\n",
      "Epoch [1928/2000] | Train Acc: 98.11% | Test Acc: 94.96% | Loss: 0.0497\n",
      "Epoch [1929/2000] | Train Acc: 98.10% | Test Acc: 95.11% | Loss: 0.0497\n",
      "Epoch [1930/2000] | Train Acc: 98.12% | Test Acc: 94.93% | Loss: 0.0498\n",
      "Epoch [1931/2000] | Train Acc: 98.09% | Test Acc: 95.09% | Loss: 0.0498\n",
      "Epoch [1932/2000] | Train Acc: 98.11% | Test Acc: 94.93% | Loss: 0.0499\n",
      "Epoch [1933/2000] | Train Acc: 98.10% | Test Acc: 95.08% | Loss: 0.0498\n",
      "Epoch [1934/2000] | Train Acc: 98.11% | Test Acc: 94.91% | Loss: 0.0499\n",
      "Epoch [1935/2000] | Train Acc: 98.09% | Test Acc: 95.09% | Loss: 0.0498\n",
      "Epoch [1936/2000] | Train Acc: 98.11% | Test Acc: 94.96% | Loss: 0.0498\n",
      "Epoch [1937/2000] | Train Acc: 98.10% | Test Acc: 95.09% | Loss: 0.0497\n",
      "Epoch [1938/2000] | Train Acc: 98.11% | Test Acc: 94.94% | Loss: 0.0496\n",
      "Epoch [1939/2000] | Train Acc: 98.09% | Test Acc: 95.06% | Loss: 0.0495\n",
      "Epoch [1940/2000] | Train Acc: 98.10% | Test Acc: 94.96% | Loss: 0.0494\n",
      "Epoch [1941/2000] | Train Acc: 98.11% | Test Acc: 95.09% | Loss: 0.0493\n",
      "Epoch [1942/2000] | Train Acc: 98.10% | Test Acc: 95.03% | Loss: 0.0493\n",
      "Epoch [1943/2000] | Train Acc: 98.11% | Test Acc: 95.11% | Loss: 0.0492\n",
      "Epoch [1944/2000] | Train Acc: 98.08% | Test Acc: 95.03% | Loss: 0.0492\n",
      "Epoch [1945/2000] | Train Acc: 98.11% | Test Acc: 95.09% | Loss: 0.0492\n",
      "Epoch [1946/2000] | Train Acc: 98.10% | Test Acc: 95.11% | Loss: 0.0491\n",
      "Epoch [1947/2000] | Train Acc: 98.11% | Test Acc: 95.09% | Loss: 0.0491\n",
      "Epoch [1948/2000] | Train Acc: 98.12% | Test Acc: 95.12% | Loss: 0.0491\n",
      "Epoch [1949/2000] | Train Acc: 98.09% | Test Acc: 95.06% | Loss: 0.0491\n",
      "Epoch [1950/2000] | Train Acc: 98.11% | Test Acc: 95.12% | Loss: 0.0491\n",
      "Epoch [1951/2000] | Train Acc: 98.09% | Test Acc: 95.02% | Loss: 0.0491\n",
      "Epoch [1952/2000] | Train Acc: 98.12% | Test Acc: 95.12% | Loss: 0.0491\n",
      "Epoch [1953/2000] | Train Acc: 98.10% | Test Acc: 94.99% | Loss: 0.0491\n",
      "Epoch [1954/2000] | Train Acc: 98.12% | Test Acc: 95.11% | Loss: 0.0491\n",
      "Epoch [1955/2000] | Train Acc: 98.10% | Test Acc: 94.96% | Loss: 0.0492\n",
      "Epoch [1956/2000] | Train Acc: 98.10% | Test Acc: 95.09% | Loss: 0.0492\n",
      "Epoch [1957/2000] | Train Acc: 98.10% | Test Acc: 94.96% | Loss: 0.0493\n",
      "Epoch [1958/2000] | Train Acc: 98.11% | Test Acc: 95.09% | Loss: 0.0494\n",
      "Epoch [1959/2000] | Train Acc: 98.12% | Test Acc: 94.93% | Loss: 0.0496\n",
      "Epoch [1960/2000] | Train Acc: 98.12% | Test Acc: 95.06% | Loss: 0.0497\n",
      "Epoch [1961/2000] | Train Acc: 98.11% | Test Acc: 94.97% | Loss: 0.0500\n",
      "Epoch [1962/2000] | Train Acc: 98.14% | Test Acc: 95.08% | Loss: 0.0502\n",
      "Epoch [1963/2000] | Train Acc: 98.10% | Test Acc: 94.94% | Loss: 0.0507\n",
      "Epoch [1964/2000] | Train Acc: 98.10% | Test Acc: 95.06% | Loss: 0.0507\n",
      "Epoch [1965/2000] | Train Acc: 98.10% | Test Acc: 94.93% | Loss: 0.0509\n",
      "Epoch [1966/2000] | Train Acc: 98.13% | Test Acc: 95.05% | Loss: 0.0504\n",
      "Epoch [1967/2000] | Train Acc: 98.11% | Test Acc: 94.93% | Loss: 0.0501\n",
      "Epoch [1968/2000] | Train Acc: 98.10% | Test Acc: 95.08% | Loss: 0.0495\n",
      "Epoch [1969/2000] | Train Acc: 98.10% | Test Acc: 95.03% | Loss: 0.0491\n",
      "Epoch [1970/2000] | Train Acc: 98.11% | Test Acc: 95.02% | Loss: 0.0489\n",
      "Epoch [1971/2000] | Train Acc: 98.11% | Test Acc: 95.08% | Loss: 0.0490\n",
      "Epoch [1972/2000] | Train Acc: 98.10% | Test Acc: 94.96% | Loss: 0.0491\n",
      "Epoch [1973/2000] | Train Acc: 98.11% | Test Acc: 95.03% | Loss: 0.0493\n",
      "Epoch [1974/2000] | Train Acc: 98.12% | Test Acc: 94.91% | Loss: 0.0496\n",
      "Epoch [1975/2000] | Train Acc: 98.12% | Test Acc: 95.06% | Loss: 0.0496\n",
      "Epoch [1976/2000] | Train Acc: 98.11% | Test Acc: 94.94% | Loss: 0.0496\n",
      "Epoch [1977/2000] | Train Acc: 98.10% | Test Acc: 95.08% | Loss: 0.0493\n",
      "Epoch [1978/2000] | Train Acc: 98.11% | Test Acc: 94.97% | Loss: 0.0492\n",
      "Epoch [1979/2000] | Train Acc: 98.12% | Test Acc: 95.09% | Loss: 0.0490\n",
      "Epoch [1980/2000] | Train Acc: 98.09% | Test Acc: 95.03% | Loss: 0.0489\n",
      "Epoch [1981/2000] | Train Acc: 98.11% | Test Acc: 94.99% | Loss: 0.0488\n",
      "Epoch [1982/2000] | Train Acc: 98.12% | Test Acc: 95.03% | Loss: 0.0489\n",
      "Epoch [1983/2000] | Train Acc: 98.11% | Test Acc: 94.91% | Loss: 0.0489\n",
      "Epoch [1984/2000] | Train Acc: 98.11% | Test Acc: 95.03% | Loss: 0.0490\n",
      "Epoch [1985/2000] | Train Acc: 98.11% | Test Acc: 94.93% | Loss: 0.0491\n",
      "Epoch [1986/2000] | Train Acc: 98.12% | Test Acc: 95.03% | Loss: 0.0491\n",
      "Epoch [1987/2000] | Train Acc: 98.12% | Test Acc: 94.90% | Loss: 0.0491\n",
      "Epoch [1988/2000] | Train Acc: 98.11% | Test Acc: 95.05% | Loss: 0.0490\n",
      "Epoch [1989/2000] | Train Acc: 98.11% | Test Acc: 94.88% | Loss: 0.0490\n",
      "Epoch [1990/2000] | Train Acc: 98.12% | Test Acc: 95.08% | Loss: 0.0489\n",
      "Epoch [1991/2000] | Train Acc: 98.11% | Test Acc: 95.00% | Loss: 0.0488\n",
      "Epoch [1992/2000] | Train Acc: 98.14% | Test Acc: 95.05% | Loss: 0.0488\n",
      "Epoch [1993/2000] | Train Acc: 98.11% | Test Acc: 95.08% | Loss: 0.0487\n",
      "Epoch [1994/2000] | Train Acc: 98.13% | Test Acc: 95.05% | Loss: 0.0487\n",
      "Epoch [1995/2000] | Train Acc: 98.14% | Test Acc: 95.06% | Loss: 0.0487\n",
      "Epoch [1996/2000] | Train Acc: 98.10% | Test Acc: 95.02% | Loss: 0.0487\n",
      "Epoch [1997/2000] | Train Acc: 98.14% | Test Acc: 95.12% | Loss: 0.0487\n",
      "Epoch [1998/2000] | Train Acc: 98.11% | Test Acc: 94.94% | Loss: 0.0487\n",
      "Epoch [1999/2000] | Train Acc: 98.13% | Test Acc: 95.08% | Loss: 0.0488\n",
      "Epoch [2000/2000] | Train Acc: 98.12% | Test Acc: 94.91% | Loss: 0.0488\n",
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1200\n",
      "           1       0.79      0.87      0.83       724\n",
      "           2       0.97      0.95      0.96       800\n",
      "           3       0.99      0.99      0.99       800\n",
      "           4       1.00      0.99      1.00       800\n",
      "           5       0.99      0.98      0.99       800\n",
      "           6       0.85      0.79      0.82       800\n",
      "           7       0.98      0.99      0.99       800\n",
      "\n",
      "    accuracy                           0.95      6724\n",
      "   macro avg       0.95      0.95      0.95      6724\n",
      "weighted avg       0.95      0.95      0.95      6724\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Chọn 1 diffusion model cụ thể ===\n",
    "model_type = 'VariationalDiffusion'\n",
    "X_train_aug, y_train_aug, _, _ = generate_samples_with_diffusion(X_train_scaled, y_train, model_type)\n",
    "\n",
    "# === Feature selection ===\n",
    "X_train_selected, X_test_selected, selected_columns = feature_selection(X_train_aug, X_test_scaled, y_train_aug)\n",
    "\n",
    "# === Chuyển sang tensor và đưa lên device ===\n",
    "X_train_tensor = torch.tensor(X_train_selected, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_selected, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_aug, dtype=torch.long).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# === Khởi tạo model ===\n",
    "input_size = X_train_selected.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = len(np.unique(y_train_aug))\n",
    "model = BiLSTM(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# === Huấn luyện ===\n",
    "num_epochs = 2000\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    _, predicted_train = torch.max(outputs, 1)\n",
    "    train_acc = (predicted_train == y_train_tensor).float().mean().item()\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # Đánh giá test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model(X_test_tensor)\n",
    "        _, predicted_test = torch.max(outputs_test, 1)\n",
    "        test_acc = (predicted_test == y_test_tensor).float().mean().item()\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Acc: {train_acc*100:.2f}% | Test Acc: {test_acc*100:.2f}% | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# === Evaluation trên test set ===\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(y_test_tensor.cpu().numpy(), predicted_test.cpu().numpy(), zero_division=0))\n",
    "\n",
    "# === Plot Loss & Accuracy ===\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# --- Loss plot ---\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Train Loss', color='red')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# --- Accuracy plot ---\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, label='Train Accuracy', color='blue')\n",
    "plt.plot(epochs, test_accuracies, label='Test Accuracy', color='green')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad5eb537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các label: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7)]\n",
      "Foundation labels: [np.int64(0), np.int64(1), np.int64(2), np.int64(3)]\n",
      "Federated labels: [np.int64(4), np.int64(5), np.int64(6), np.int64(7)]\n"
     ]
    }
   ],
   "source": [
    "# Lấy danh sách label đã mã hóa (sau khi LabelEncoder)\n",
    "unique_labels = sorted(df['label'].unique())\n",
    "print(\"Các label:\", unique_labels)\n",
    "\n",
    "# Chia 4 label đầu cho foundation, 4 label sau cho federated learning\n",
    "foundation_labels = unique_labels[:4]\n",
    "federated_labels = unique_labels[4:]\n",
    "\n",
    "print(\"Foundation labels:\", foundation_labels)\n",
    "print(\"Federated labels:\", federated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "188ee12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape foundation: (17618, 89)\n",
      "label\n",
      "0    6000\n",
      "3    4000\n",
      "2    4000\n",
      "1    3618\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Lọc dữ liệu cho foundation model (chỉ chứa 4 label đầu)\n",
    "df_foundation = df[df['label'].isin(foundation_labels)].reset_index(drop=True)\n",
    "print(\"Shape foundation:\", df_foundation.shape)\n",
    "print(df_foundation['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5359d892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated client label 4: shape = (4000, 89)\n",
      "Federated client label 5: shape = (4000, 89)\n",
      "Federated client label 6: shape = (4000, 89)\n",
      "Federated client label 7: shape = (4000, 89)\n"
     ]
    }
   ],
   "source": [
    "# Tạo 4 tập federated, mỗi tập chỉ chứa 1 label\n",
    "federated_datasets = {}\n",
    "for label in federated_labels:\n",
    "    federated_datasets[label] = df[df['label'] == label].reset_index(drop=True)\n",
    "    print(f\"Federated client label {label}: shape = {federated_datasets[label].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd23288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lưu ra file CSV \n",
    "df_foundation.to_csv(\"foundation_data.csv\", index=False)\n",
    "for label, df_client in federated_datasets.items():\n",
    "    df_client.to_csv(f\"federated_client_label_{label}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78429e64",
   "metadata": {},
   "source": [
    "## 1. Tiền xử lý & huấn luyện Foundation Model\n",
    "Các bước: tách dữ liệu, chia train/test, chuẩn hóa, chọn đặc trưng, (tùy chọn) augmentation, huấn luyện mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f82be9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:110: UserWarning: Features [36 37 38 53 60 61 62] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "d:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:110: UserWarning: Features [36 37 38 53 60 61 62] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "d:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (columns): [0, 1, 2, 4, 5, 12, 13, 21, 23, 24, 26, 28, 29, 43, 45, 48, 50, 57, 58, 70, 73, 78, 80, 81, 82, 83, 84, 85, 86, 87]\n",
      "\n",
      " Number of samples before augmentation: 14094\n",
      "\n",
      "   ➤ Class 1: 2894 samples before augmentation\n",
      "   ➤ Class 0: 4800 samples before augmentation\n",
      "   ➤ Class 3: 3200 samples before augmentation\n",
      "   ➤ Class 2: 3200 samples before augmentation\n",
      "Number of samples after augmentation: 19200\n",
      "  Class 1: 4800 samples after augmentation\n",
      "  Class 0: 4800 samples after augmentation\n",
      "  Class 3: 4800 samples after augmentation\n",
      "  Class 2: 4800 samples after augmentation\n",
      "Epoch [1/400] | Train Acc: 23.09% | Test Acc: 73.50% | Loss: 1.3868\n",
      "Epoch [1/400] | Train Acc: 23.09% | Test Acc: 73.50% | Loss: 1.3868\n",
      "Epoch [10/400] | Train Acc: 66.62% | Test Acc: 82.01% | Loss: 0.8780\n",
      "Epoch [10/400] | Train Acc: 66.62% | Test Acc: 82.01% | Loss: 0.8780\n",
      "Epoch [20/400] | Train Acc: 71.01% | Test Acc: 87.88% | Loss: 0.6954\n",
      "Epoch [20/400] | Train Acc: 71.01% | Test Acc: 87.88% | Loss: 0.6954\n",
      "Epoch [30/400] | Train Acc: 74.72% | Test Acc: 90.21% | Loss: 0.6127\n",
      "Epoch [30/400] | Train Acc: 74.72% | Test Acc: 90.21% | Loss: 0.6127\n",
      "Epoch [40/400] | Train Acc: 76.20% | Test Acc: 92.17% | Loss: 0.5482\n",
      "Epoch [40/400] | Train Acc: 76.20% | Test Acc: 92.17% | Loss: 0.5482\n",
      "Epoch [50/400] | Train Acc: 78.27% | Test Acc: 94.04% | Loss: 0.5057\n",
      "Epoch [50/400] | Train Acc: 78.27% | Test Acc: 94.04% | Loss: 0.5057\n",
      "Epoch [60/400] | Train Acc: 79.17% | Test Acc: 94.47% | Loss: 0.4705\n",
      "Epoch [60/400] | Train Acc: 79.17% | Test Acc: 94.47% | Loss: 0.4705\n",
      "Epoch [70/400] | Train Acc: 80.28% | Test Acc: 95.29% | Loss: 0.4409\n",
      "Epoch [70/400] | Train Acc: 80.28% | Test Acc: 95.29% | Loss: 0.4409\n",
      "Epoch [80/400] | Train Acc: 81.32% | Test Acc: 95.83% | Loss: 0.4143\n",
      "Epoch [80/400] | Train Acc: 81.32% | Test Acc: 95.83% | Loss: 0.4143\n",
      "Epoch [90/400] | Train Acc: 82.48% | Test Acc: 96.48% | Loss: 0.3897\n",
      "Epoch [90/400] | Train Acc: 82.48% | Test Acc: 96.48% | Loss: 0.3897\n",
      "Epoch [100/400] | Train Acc: 83.78% | Test Acc: 97.05% | Loss: 0.3662\n",
      "Epoch [100/400] | Train Acc: 83.78% | Test Acc: 97.05% | Loss: 0.3662\n",
      "Epoch [110/400] | Train Acc: 85.10% | Test Acc: 97.56% | Loss: 0.3437\n",
      "Epoch [110/400] | Train Acc: 85.10% | Test Acc: 97.56% | Loss: 0.3437\n",
      "Epoch [120/400] | Train Acc: 86.45% | Test Acc: 97.96% | Loss: 0.3218\n",
      "Epoch [120/400] | Train Acc: 86.45% | Test Acc: 97.96% | Loss: 0.3218\n",
      "Epoch [130/400] | Train Acc: 87.60% | Test Acc: 98.13% | Loss: 0.3004\n",
      "Epoch [130/400] | Train Acc: 87.60% | Test Acc: 98.13% | Loss: 0.3004\n",
      "Epoch [140/400] | Train Acc: 88.97% | Test Acc: 98.27% | Loss: 0.2791\n",
      "Epoch [140/400] | Train Acc: 88.97% | Test Acc: 98.27% | Loss: 0.2791\n",
      "Epoch [150/400] | Train Acc: 90.09% | Test Acc: 98.33% | Loss: 0.2580\n",
      "Epoch [150/400] | Train Acc: 90.09% | Test Acc: 98.33% | Loss: 0.2580\n",
      "Epoch [160/400] | Train Acc: 91.39% | Test Acc: 98.44% | Loss: 0.2371\n",
      "Epoch [160/400] | Train Acc: 91.39% | Test Acc: 98.44% | Loss: 0.2371\n",
      "Epoch [170/400] | Train Acc: 92.63% | Test Acc: 98.52% | Loss: 0.2163\n",
      "Epoch [170/400] | Train Acc: 92.63% | Test Acc: 98.52% | Loss: 0.2163\n",
      "Epoch [180/400] | Train Acc: 93.94% | Test Acc: 98.69% | Loss: 0.1958\n",
      "Epoch [180/400] | Train Acc: 93.94% | Test Acc: 98.69% | Loss: 0.1958\n",
      "Epoch [190/400] | Train Acc: 95.09% | Test Acc: 98.75% | Loss: 0.1758\n",
      "Epoch [190/400] | Train Acc: 95.09% | Test Acc: 98.75% | Loss: 0.1758\n",
      "Epoch [200/400] | Train Acc: 96.22% | Test Acc: 98.78% | Loss: 0.1565\n",
      "Epoch [200/400] | Train Acc: 96.22% | Test Acc: 98.78% | Loss: 0.1565\n",
      "Epoch [210/400] | Train Acc: 97.14% | Test Acc: 98.78% | Loss: 0.1383\n",
      "Epoch [210/400] | Train Acc: 97.14% | Test Acc: 98.78% | Loss: 0.1383\n",
      "Epoch [220/400] | Train Acc: 97.86% | Test Acc: 98.81% | Loss: 0.1214\n",
      "Epoch [220/400] | Train Acc: 97.86% | Test Acc: 98.81% | Loss: 0.1214\n",
      "Epoch [230/400] | Train Acc: 98.46% | Test Acc: 98.84% | Loss: 0.1060\n",
      "Epoch [230/400] | Train Acc: 98.46% | Test Acc: 98.84% | Loss: 0.1060\n",
      "Epoch [240/400] | Train Acc: 98.90% | Test Acc: 98.84% | Loss: 0.0922\n",
      "Epoch [240/400] | Train Acc: 98.90% | Test Acc: 98.84% | Loss: 0.0922\n",
      "Epoch [250/400] | Train Acc: 99.18% | Test Acc: 98.84% | Loss: 0.0801\n",
      "Epoch [250/400] | Train Acc: 99.18% | Test Acc: 98.84% | Loss: 0.0801\n",
      "Epoch [260/400] | Train Acc: 99.45% | Test Acc: 98.86% | Loss: 0.0697\n",
      "Epoch [260/400] | Train Acc: 99.45% | Test Acc: 98.86% | Loss: 0.0697\n",
      "Epoch [270/400] | Train Acc: 99.61% | Test Acc: 98.86% | Loss: 0.0608\n",
      "Epoch [270/400] | Train Acc: 99.61% | Test Acc: 98.86% | Loss: 0.0608\n",
      "Epoch [280/400] | Train Acc: 99.66% | Test Acc: 98.89% | Loss: 0.0533\n",
      "Epoch [280/400] | Train Acc: 99.66% | Test Acc: 98.89% | Loss: 0.0533\n",
      "Epoch [290/400] | Train Acc: 99.72% | Test Acc: 98.92% | Loss: 0.0471\n",
      "Epoch [290/400] | Train Acc: 99.72% | Test Acc: 98.92% | Loss: 0.0471\n",
      "Epoch [300/400] | Train Acc: 99.76% | Test Acc: 98.95% | Loss: 0.0419\n",
      "Epoch [300/400] | Train Acc: 99.76% | Test Acc: 98.95% | Loss: 0.0419\n",
      "Epoch [310/400] | Train Acc: 99.77% | Test Acc: 98.98% | Loss: 0.0375\n",
      "Epoch [310/400] | Train Acc: 99.77% | Test Acc: 98.98% | Loss: 0.0375\n",
      "Epoch [320/400] | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0339\n",
      "Epoch [320/400] | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0339\n",
      "Epoch [330/400] | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0308\n",
      "Epoch [330/400] | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0308\n",
      "Epoch [340/400] | Train Acc: 99.78% | Test Acc: 99.04% | Loss: 0.0282\n",
      "Epoch [340/400] | Train Acc: 99.78% | Test Acc: 99.04% | Loss: 0.0282\n",
      "Epoch [350/400] | Train Acc: 99.78% | Test Acc: 99.04% | Loss: 0.0260\n",
      "Epoch [350/400] | Train Acc: 99.78% | Test Acc: 99.04% | Loss: 0.0260\n",
      "Epoch [360/400] | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0241\n",
      "Epoch [360/400] | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0241\n",
      "Epoch [370/400] | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0225\n",
      "Epoch [370/400] | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0225\n",
      "Epoch [380/400] | Train Acc: 99.80% | Test Acc: 99.04% | Loss: 0.0210\n",
      "Epoch [380/400] | Train Acc: 99.80% | Test Acc: 99.04% | Loss: 0.0210\n",
      "Epoch [390/400] | Train Acc: 99.80% | Test Acc: 99.06% | Loss: 0.0198\n",
      "Epoch [390/400] | Train Acc: 99.80% | Test Acc: 99.06% | Loss: 0.0198\n",
      "Epoch [400/400] | Train Acc: 99.80% | Test Acc: 99.06% | Loss: 0.0186\n",
      "\n",
      "Classification Report (Foundation Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      1200\n",
      "           1       0.97      1.00      0.98       724\n",
      "           2       0.99      0.97      0.98       800\n",
      "           3       1.00      1.00      1.00       800\n",
      "\n",
      "    accuracy                           0.99      3524\n",
      "   macro avg       0.99      0.99      0.99      3524\n",
      "weighted avg       0.99      0.99      0.99      3524\n",
      "\n",
      "Epoch [400/400] | Train Acc: 99.80% | Test Acc: 99.06% | Loss: 0.0186\n",
      "\n",
      "Classification Report (Foundation Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      1200\n",
      "           1       0.97      1.00      0.98       724\n",
      "           2       0.99      0.97      0.98       800\n",
      "           3       1.00      1.00      1.00       800\n",
      "\n",
      "    accuracy                           0.99      3524\n",
      "   macro avg       0.99      0.99      0.99      3524\n",
      "weighted avg       0.99      0.99      0.99      3524\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:110: UserWarning: Features [36 37 38 53 60 61 62] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "d:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (columns): [0, 1, 2, 4, 5, 12, 13, 21, 23, 24, 26, 28, 29, 43, 45, 48, 50, 57, 58, 70, 73, 78, 80, 81, 82, 83, 84, 85, 86, 87]\n",
      "\n",
      " Number of samples before augmentation: 14094\n",
      "\n",
      "   ➤ Class 1: 2894 samples before augmentation\n",
      "   ➤ Class 0: 4800 samples before augmentation\n",
      "   ➤ Class 3: 3200 samples before augmentation\n",
      "   ➤ Class 2: 3200 samples before augmentation\n",
      "Number of samples after augmentation: 19200\n",
      "  Class 1: 4800 samples after augmentation\n",
      "  Class 0: 4800 samples after augmentation\n",
      "  Class 3: 4800 samples after augmentation\n",
      "  Class 2: 4800 samples after augmentation\n",
      "Epoch [1/400] | Train Acc: 23.09% | Test Acc: 73.50% | Loss: 1.3868\n",
      "Epoch [1/400] | Train Acc: 23.09% | Test Acc: 73.50% | Loss: 1.3868\n",
      "Epoch [10/400] | Train Acc: 66.62% | Test Acc: 82.01% | Loss: 0.8780\n",
      "Epoch [10/400] | Train Acc: 66.62% | Test Acc: 82.01% | Loss: 0.8780\n",
      "Epoch [20/400] | Train Acc: 71.01% | Test Acc: 87.88% | Loss: 0.6954\n",
      "Epoch [20/400] | Train Acc: 71.01% | Test Acc: 87.88% | Loss: 0.6954\n",
      "Epoch [30/400] | Train Acc: 74.72% | Test Acc: 90.21% | Loss: 0.6127\n",
      "Epoch [30/400] | Train Acc: 74.72% | Test Acc: 90.21% | Loss: 0.6127\n",
      "Epoch [40/400] | Train Acc: 76.20% | Test Acc: 92.17% | Loss: 0.5482\n",
      "Epoch [40/400] | Train Acc: 76.20% | Test Acc: 92.17% | Loss: 0.5482\n",
      "Epoch [50/400] | Train Acc: 78.27% | Test Acc: 94.04% | Loss: 0.5057\n",
      "Epoch [50/400] | Train Acc: 78.27% | Test Acc: 94.04% | Loss: 0.5057\n",
      "Epoch [60/400] | Train Acc: 79.17% | Test Acc: 94.47% | Loss: 0.4705\n",
      "Epoch [60/400] | Train Acc: 79.17% | Test Acc: 94.47% | Loss: 0.4705\n",
      "Epoch [70/400] | Train Acc: 80.28% | Test Acc: 95.29% | Loss: 0.4409\n",
      "Epoch [70/400] | Train Acc: 80.28% | Test Acc: 95.29% | Loss: 0.4409\n",
      "Epoch [80/400] | Train Acc: 81.32% | Test Acc: 95.83% | Loss: 0.4143\n",
      "Epoch [80/400] | Train Acc: 81.32% | Test Acc: 95.83% | Loss: 0.4143\n",
      "Epoch [90/400] | Train Acc: 82.48% | Test Acc: 96.48% | Loss: 0.3897\n",
      "Epoch [90/400] | Train Acc: 82.48% | Test Acc: 96.48% | Loss: 0.3897\n",
      "Epoch [100/400] | Train Acc: 83.78% | Test Acc: 97.05% | Loss: 0.3662\n",
      "Epoch [100/400] | Train Acc: 83.78% | Test Acc: 97.05% | Loss: 0.3662\n",
      "Epoch [110/400] | Train Acc: 85.10% | Test Acc: 97.56% | Loss: 0.3437\n",
      "Epoch [110/400] | Train Acc: 85.10% | Test Acc: 97.56% | Loss: 0.3437\n",
      "Epoch [120/400] | Train Acc: 86.45% | Test Acc: 97.96% | Loss: 0.3218\n",
      "Epoch [120/400] | Train Acc: 86.45% | Test Acc: 97.96% | Loss: 0.3218\n",
      "Epoch [130/400] | Train Acc: 87.60% | Test Acc: 98.13% | Loss: 0.3004\n",
      "Epoch [130/400] | Train Acc: 87.60% | Test Acc: 98.13% | Loss: 0.3004\n",
      "Epoch [140/400] | Train Acc: 88.97% | Test Acc: 98.27% | Loss: 0.2791\n",
      "Epoch [140/400] | Train Acc: 88.97% | Test Acc: 98.27% | Loss: 0.2791\n",
      "Epoch [150/400] | Train Acc: 90.09% | Test Acc: 98.33% | Loss: 0.2580\n",
      "Epoch [150/400] | Train Acc: 90.09% | Test Acc: 98.33% | Loss: 0.2580\n",
      "Epoch [160/400] | Train Acc: 91.39% | Test Acc: 98.44% | Loss: 0.2371\n",
      "Epoch [160/400] | Train Acc: 91.39% | Test Acc: 98.44% | Loss: 0.2371\n",
      "Epoch [170/400] | Train Acc: 92.63% | Test Acc: 98.52% | Loss: 0.2163\n",
      "Epoch [170/400] | Train Acc: 92.63% | Test Acc: 98.52% | Loss: 0.2163\n",
      "Epoch [180/400] | Train Acc: 93.94% | Test Acc: 98.69% | Loss: 0.1958\n",
      "Epoch [180/400] | Train Acc: 93.94% | Test Acc: 98.69% | Loss: 0.1958\n",
      "Epoch [190/400] | Train Acc: 95.09% | Test Acc: 98.75% | Loss: 0.1758\n",
      "Epoch [190/400] | Train Acc: 95.09% | Test Acc: 98.75% | Loss: 0.1758\n",
      "Epoch [200/400] | Train Acc: 96.22% | Test Acc: 98.78% | Loss: 0.1565\n",
      "Epoch [200/400] | Train Acc: 96.22% | Test Acc: 98.78% | Loss: 0.1565\n",
      "Epoch [210/400] | Train Acc: 97.14% | Test Acc: 98.78% | Loss: 0.1383\n",
      "Epoch [210/400] | Train Acc: 97.14% | Test Acc: 98.78% | Loss: 0.1383\n",
      "Epoch [220/400] | Train Acc: 97.86% | Test Acc: 98.81% | Loss: 0.1214\n",
      "Epoch [220/400] | Train Acc: 97.86% | Test Acc: 98.81% | Loss: 0.1214\n",
      "Epoch [230/400] | Train Acc: 98.46% | Test Acc: 98.84% | Loss: 0.1060\n",
      "Epoch [230/400] | Train Acc: 98.46% | Test Acc: 98.84% | Loss: 0.1060\n",
      "Epoch [240/400] | Train Acc: 98.90% | Test Acc: 98.84% | Loss: 0.0922\n",
      "Epoch [240/400] | Train Acc: 98.90% | Test Acc: 98.84% | Loss: 0.0922\n",
      "Epoch [250/400] | Train Acc: 99.18% | Test Acc: 98.84% | Loss: 0.0801\n",
      "Epoch [250/400] | Train Acc: 99.18% | Test Acc: 98.84% | Loss: 0.0801\n",
      "Epoch [260/400] | Train Acc: 99.45% | Test Acc: 98.86% | Loss: 0.0697\n",
      "Epoch [260/400] | Train Acc: 99.45% | Test Acc: 98.86% | Loss: 0.0697\n",
      "Epoch [270/400] | Train Acc: 99.61% | Test Acc: 98.86% | Loss: 0.0608\n",
      "Epoch [270/400] | Train Acc: 99.61% | Test Acc: 98.86% | Loss: 0.0608\n",
      "Epoch [280/400] | Train Acc: 99.66% | Test Acc: 98.89% | Loss: 0.0533\n",
      "Epoch [280/400] | Train Acc: 99.66% | Test Acc: 98.89% | Loss: 0.0533\n",
      "Epoch [290/400] | Train Acc: 99.72% | Test Acc: 98.92% | Loss: 0.0471\n",
      "Epoch [290/400] | Train Acc: 99.72% | Test Acc: 98.92% | Loss: 0.0471\n",
      "Epoch [300/400] | Train Acc: 99.76% | Test Acc: 98.95% | Loss: 0.0419\n",
      "Epoch [300/400] | Train Acc: 99.76% | Test Acc: 98.95% | Loss: 0.0419\n",
      "Epoch [310/400] | Train Acc: 99.77% | Test Acc: 98.98% | Loss: 0.0375\n",
      "Epoch [310/400] | Train Acc: 99.77% | Test Acc: 98.98% | Loss: 0.0375\n",
      "Epoch [320/400] | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0339\n",
      "Epoch [320/400] | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0339\n",
      "Epoch [330/400] | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0308\n",
      "Epoch [330/400] | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0308\n",
      "Epoch [340/400] | Train Acc: 99.78% | Test Acc: 99.04% | Loss: 0.0282\n",
      "Epoch [340/400] | Train Acc: 99.78% | Test Acc: 99.04% | Loss: 0.0282\n",
      "Epoch [350/400] | Train Acc: 99.78% | Test Acc: 99.04% | Loss: 0.0260\n",
      "Epoch [350/400] | Train Acc: 99.78% | Test Acc: 99.04% | Loss: 0.0260\n",
      "Epoch [360/400] | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0241\n",
      "Epoch [360/400] | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0241\n",
      "Epoch [370/400] | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0225\n",
      "Epoch [370/400] | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0225\n",
      "Epoch [380/400] | Train Acc: 99.80% | Test Acc: 99.04% | Loss: 0.0210\n",
      "Epoch [380/400] | Train Acc: 99.80% | Test Acc: 99.04% | Loss: 0.0210\n",
      "Epoch [390/400] | Train Acc: 99.80% | Test Acc: 99.06% | Loss: 0.0198\n",
      "Epoch [390/400] | Train Acc: 99.80% | Test Acc: 99.06% | Loss: 0.0198\n",
      "Epoch [400/400] | Train Acc: 99.80% | Test Acc: 99.06% | Loss: 0.0186\n",
      "\n",
      "Classification Report (Foundation Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      1200\n",
      "           1       0.97      1.00      0.98       724\n",
      "           2       0.99      0.97      0.98       800\n",
      "           3       1.00      1.00      1.00       800\n",
      "\n",
      "    accuracy                           0.99      3524\n",
      "   macro avg       0.99      0.99      0.99      3524\n",
      "weighted avg       0.99      0.99      0.99      3524\n",
      "\n",
      "Epoch [400/400] | Train Acc: 99.80% | Test Acc: 99.06% | Loss: 0.0186\n",
      "\n",
      "Classification Report (Foundation Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      1200\n",
      "           1       0.97      1.00      0.98       724\n",
      "           2       0.99      0.97      0.98       800\n",
      "           3       1.00      1.00      1.00       800\n",
      "\n",
      "    accuracy                           0.99      3524\n",
      "   macro avg       0.99      0.99      0.99      3524\n",
      "weighted avg       0.99      0.99      0.99      3524\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Foundation Model Pipeline ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Tách dữ liệu và label\n",
    "X_found = df_foundation.drop(columns=['label']).values\n",
    "y_found = df_foundation['label'].values\n",
    "\n",
    "# 2. Chia train/test\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_found, y_found, test_size=0.2, stratify=y_found, random_state=42)\n",
    "# Xử lý NaN, inf, -inf và giới hạn giá trị\n",
    "X_train_f = np.nan_to_num(X_train_f, nan=0, posinf=0, neginf=0)\n",
    "X_test_f = np.nan_to_num(X_test_f, nan=0, posinf=0, neginf=0)\n",
    "X_train_f = np.clip(X_train_f, -1e10, 1e10)\n",
    "X_test_f = np.clip(X_test_f, -1e10, 1e10)\n",
    "\n",
    "# 3. Chuẩn hóa dữ liệu\n",
    "scaler_f = StandardScaler()\n",
    "X_train_f_scaled = scaler_f.fit_transform(X_train_f)\n",
    "X_test_f_scaled = scaler_f.transform(X_test_f)\n",
    "\n",
    "# 4. Chọn đặc trưng\n",
    "X_train_f_selected, X_test_f_selected, selected_columns_f = feature_selection(X_train_f_scaled, X_test_f_scaled, y_train_f)\n",
    "\n",
    "# 5. Augmentation\n",
    "model_type = 'DDPM' \n",
    "X_train_f_aug, y_train_f_aug, _, _ = generate_samples_with_diffusion(X_train_f_selected, y_train_f, model_type)\n",
    "\n",
    "# 6. Đưa dữ liệu sang tensor và lên device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_tensor = torch.tensor(X_train_f_aug, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_f_selected, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_f_aug, dtype=torch.long).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_f, dtype=torch.long).to(device)\n",
    "\n",
    "# 7. Định nghĩa mô hình BiLSTM\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_size = X_train_f_selected.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = len(np.unique(y_train_f_aug))\n",
    "model = BiLSTM(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "num_epochs = 400\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    _, predicted_train = torch.max(outputs, 1)\n",
    "    train_acc = (predicted_train == y_train_tensor).float().mean().item()\n",
    "    train_accuracies.append(train_acc)\n",
    "    # Đánh giá test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model(X_test_tensor)\n",
    "        _, predicted_test = torch.max(outputs_test, 1)\n",
    "        test_acc = (predicted_test == y_test_tensor).float().mean().item()\n",
    "        test_accuracies.append(test_acc)\n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Acc: {train_acc*100:.2f}% | Test Acc: {test_acc*100:.2f}% | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Đánh giá cuối cùng trên test set\n",
    "print(\"\\nClassification Report (Foundation Model):\")\n",
    "print(classification_report(y_test_tensor.cpu().numpy(), predicted_test.cpu().numpy(), zero_division=0))\n",
    "\n",
    "# Vẽ biểu đồ Loss & Accuracy\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Train Loss', color='red')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, label='Train Accuracy', color='blue')\n",
    "plt.plot(epochs, test_accuracies, label='Test Accuracy', color='green')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b575aa",
   "metadata": {},
   "source": [
    "## 2. Tiền xử lý dữ liệu cho từng client federated\n",
    "Các bước: lặp qua từng tập federated, tách dữ liệu/label, chia train/test, chuẩn hóa, chọn đặc trưng, lưu lại cho federated learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5d82cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Client label 4 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Client label 4 ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Chuẩn hóa\u001b[39;00m\n\u001b[32m     15\u001b[39m scaler_c = StandardScaler()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m X_train_c_scaled = \u001b[43mscaler_c\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_c\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m X_test_c_scaled = scaler_c.transform(X_test_c)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Chọn đặc trưng\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\base.py:892\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    877\u001b[39m         warnings.warn(\n\u001b[32m    878\u001b[39m             (\n\u001b[32m    879\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    887\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    888\u001b[39m         )\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    891\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m892\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    894\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    895\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:907\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:943\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    911\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[32m    912\u001b[39m \n\u001b[32m    913\u001b[39m \u001b[33;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m \u001b[33;03m    Fitted scaler.\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    942\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NCKH\\Blockchain_contest\\Federated_Learning\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# --- Federated Clients Preprocessing Pipeline ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "client_data = {}\n",
    "for label, df_client in federated_datasets.items():\n",
    "    print(f\"\\n--- Client label {label} ---\")\n",
    "    X_client = df_client.drop(columns=['label']).values\n",
    "    y_client = df_client['label'].values\n",
    "\n",
    "    # Chia train/test\n",
    "    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_client, y_client, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Chuẩn hóa\n",
    "    scaler_c = StandardScaler()\n",
    "    X_train_c_scaled = scaler_c.fit_transform(X_train_c)\n",
    "    X_test_c_scaled = scaler_c.transform(X_test_c)\n",
    "\n",
    "    # Chọn đặc trưng\n",
    "    X_train_c_selected, X_test_c_selected, selected_columns_c = feature_selection(X_train_c_scaled, X_test_c_scaled, y_train_c)\n",
    "\n",
    "    # Augmentation\n",
    "    model_type = 'DDPM'\n",
    "    X_train_c_aug, y_train_c_aug, _, _ = generate_samples_with_diffusion(X_train_c_selected, y_train_c, model_type)\n",
    "    \n",
    "\n",
    "    # Lưu lại cho federated learning\n",
    "    client_data[label] = {\n",
    "        'X_train': X_train_c_selected,\n",
    "        'y_train': y_train_c,\n",
    "        'X_test': X_test_c_selected,\n",
    "        'y_test': y_test_c,\n",
    "        'X_train_aug': X_train_c_aug,\n",
    "        'y_train_aug': y_train_c_aug,\n",
    "        'selected_columns': selected_columns_c,\n",
    "        'scaler': scaler_c\n",
    "    }\n",
    "    print(f\"Train shape: {X_train_c_selected.shape}, Test shape: {X_test_c_selected.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
