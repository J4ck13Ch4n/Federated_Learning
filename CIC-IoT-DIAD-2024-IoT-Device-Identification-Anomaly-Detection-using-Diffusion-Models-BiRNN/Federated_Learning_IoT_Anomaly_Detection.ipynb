{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eddf79b",
   "metadata": {},
   "source": [
    "# Nhận diện thiết bị IoT & Phát hiện bất thường bằng Federated Learning\n",
    "\n",
    "## Nội dung chính\n",
    "1. Import thư viện & tải dữ liệu\n",
    "2. Tiền xử lý dữ liệu & chọn đặc trưng\n",
    "3. Phân chia dữ liệu cho Federated Learning (IID, Non-IID, Quantity Skewed)\n",
    "4. Huấn luyện mô hình nền tảng (BiLSTM)\n",
    "5. Chuẩn bị client federated\n",
    "6. Cài đặt Federated Learning (FedAvg)\n",
    "7. Vòng lặp huấn luyện liên kết\n",
    "8. Đánh giá mô hình toàn cục\n",
    "9. Phân tích & trực quan hóa kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89de66a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (33618, 84)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flow ID</th>\n",
       "      <th>Src IP</th>\n",
       "      <th>Src Port</th>\n",
       "      <th>Dst IP</th>\n",
       "      <th>Dst Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packet</th>\n",
       "      <th>Total Bwd packets</th>\n",
       "      <th>...</th>\n",
       "      <th>Fwd Seg Size Min</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192.168.137.66-192.168.137.174-41082-80-6</td>\n",
       "      <td>192.168.137.66</td>\n",
       "      <td>41082.0</td>\n",
       "      <td>192.168.137.174</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>09/08/2022 11:27:38 AM</td>\n",
       "      <td>1527173.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192.168.137.66-192.168.137.254-55598-34287-6</td>\n",
       "      <td>192.168.137.66</td>\n",
       "      <td>55598.0</td>\n",
       "      <td>192.168.137.254</td>\n",
       "      <td>34287.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>09/08/2022 11:56:16 AM</td>\n",
       "      <td>9912071.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192.168.137.174-192.168.137.66-80-47994-6</td>\n",
       "      <td>192.168.137.174</td>\n",
       "      <td>80.0</td>\n",
       "      <td>192.168.137.66</td>\n",
       "      <td>47994.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>09/08/2022 11:29:10 AM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192.168.137.66-192.168.137.254-59336-8009-6</td>\n",
       "      <td>192.168.137.66</td>\n",
       "      <td>59336.0</td>\n",
       "      <td>192.168.137.254</td>\n",
       "      <td>8009.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>09/08/2022 11:50:16 AM</td>\n",
       "      <td>349868.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192.168.137.66-192.168.137.254-55662-8009-6</td>\n",
       "      <td>192.168.137.66</td>\n",
       "      <td>55662.0</td>\n",
       "      <td>192.168.137.254</td>\n",
       "      <td>8009.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>09/08/2022 11:49:16 AM</td>\n",
       "      <td>215841.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Flow ID           Src IP  Src Port  \\\n",
       "0     192.168.137.66-192.168.137.174-41082-80-6   192.168.137.66   41082.0   \n",
       "1  192.168.137.66-192.168.137.254-55598-34287-6   192.168.137.66   55598.0   \n",
       "2     192.168.137.174-192.168.137.66-80-47994-6  192.168.137.174      80.0   \n",
       "3   192.168.137.66-192.168.137.254-59336-8009-6   192.168.137.66   59336.0   \n",
       "4   192.168.137.66-192.168.137.254-55662-8009-6   192.168.137.66   55662.0   \n",
       "\n",
       "            Dst IP  Dst Port  Protocol               Timestamp  Flow Duration  \\\n",
       "0  192.168.137.174      80.0       6.0  09/08/2022 11:27:38 AM      1527173.0   \n",
       "1  192.168.137.254   34287.0       6.0  09/08/2022 11:56:16 AM      9912071.0   \n",
       "2   192.168.137.66   47994.0       6.0  09/08/2022 11:29:10 AM            0.0   \n",
       "3  192.168.137.254    8009.0       6.0  09/08/2022 11:50:16 AM       349868.0   \n",
       "4  192.168.137.254    8009.0       6.0  09/08/2022 11:49:16 AM       215841.0   \n",
       "\n",
       "   Total Fwd Packet  Total Bwd packets  ...  Fwd Seg Size Min  Active Mean  \\\n",
       "0              16.0                1.0  ...              20.0          0.0   \n",
       "1               7.0                2.0  ...              32.0          0.0   \n",
       "2               2.0                0.0  ...              20.0          0.0   \n",
       "3               1.0                1.0  ...              40.0          0.0   \n",
       "4               1.0                1.0  ...              40.0          0.0   \n",
       "\n",
       "   Active Std  Active Max  Active Min  Idle Mean  Idle Std  Idle Max  \\\n",
       "0         0.0         0.0         0.0        0.0       0.0       0.0   \n",
       "1         0.0         0.0         0.0        0.0       0.0       0.0   \n",
       "2         0.0         0.0         0.0        0.0       0.0       0.0   \n",
       "3         0.0         0.0         0.0        0.0       0.0       0.0   \n",
       "4         0.0         0.0         0.0        0.0       0.0       0.0   \n",
       "\n",
       "   Idle Min  label  \n",
       "0       0.0    DoS  \n",
       "1       0.0    DoS  \n",
       "2       0.0    DoS  \n",
       "3       0.0    DoS  \n",
       "4       0.0    DoS  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Import thư viện & tải dữ liệu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "df = pd.read_csv('IoTDIAD.csv')\n",
    "print('Shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8995a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0\n",
      "Label distribution:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Tiền xử lý dữ liệu & chọn đặc trưng\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "df['Year'] = df['Timestamp'].dt.year\n",
    "df['Month'] = df['Timestamp'].dt.month\n",
    "df['Day'] = df['Timestamp'].dt.day\n",
    "df['Hour'] = df['Timestamp'].dt.hour\n",
    "df['Minute'] = df['Timestamp'].dt.minute\n",
    "df['Second'] = df['Timestamp'].dt.second\n",
    "df.drop(columns=['Timestamp'], inplace=True)\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "df['Flow ID'] = df['Flow ID'].astype('category').cat.codes\n",
    "df['Src IP'] = df['Src IP'].astype('category').cat.codes\n",
    "df['Dst IP'] = df['Dst IP'].astype('category').cat.codes\n",
    "df = df.fillna(0)\n",
    "print('Missing values:', df.isnull().sum().sum())\n",
    "print('Label distribution:')\n",
    "df['label'].value_counts().plot(kind='bar')\n",
    "plt.title('Label Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ebe4d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang sử dụng GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Chuyển đổi device sang GPU (CUDA) nếu khả dụng\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Đang sử dụng GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Không tìm thấy GPU, sử dụng CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970b0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Tính weight cho toàn bộ tập dữ liệu\n",
    "label_counts = df['label'].value_counts().sort_index()\n",
    "class_weights = 1.0 / label_counts.values\n",
    "class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Khi khởi tạo loss function:\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d958bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== IID Partition ===\n",
      "client_0: 8404 samples\n",
      "  Labels: {0: np.int64(1490), 2: np.int64(1052), 6: np.int64(1023), 4: np.int64(1017), 5: np.int64(1015), 7: np.int64(1007), 3: np.int64(975), 1: np.int64(825)}\n",
      "client_1: 8404 samples\n",
      "  Labels: {0: np.int64(1520), 3: np.int64(1038), 5: np.int64(1002), 6: np.int64(1000), 2: np.int64(975), 4: np.int64(973), 7: np.int64(950), 1: np.int64(946)}\n",
      "client_2: 8404 samples\n",
      "  Labels: {0: np.int64(1456), 7: np.int64(1038), 4: np.int64(1018), 5: np.int64(1008), 6: np.int64(998), 3: np.int64(978), 2: np.int64(970), 1: np.int64(938)}\n",
      "client_3: 8406 samples\n",
      "  Labels: {0: np.int64(1534), 3: np.int64(1009), 7: np.int64(1005), 2: np.int64(1003), 4: np.int64(992), 6: np.int64(979), 5: np.int64(975), 1: np.int64(909)}\n",
      "\n",
      "Tổng số mẫu: 33618\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Non-IID by Label ===\n",
      "client_0_label_0: 6000 samples\n",
      "  Labels: {0: np.int64(6000)}\n",
      "client_1_label_1: 3618 samples\n",
      "  Labels: {1: np.int64(3618)}\n",
      "client_2_label_2: 4000 samples\n",
      "  Labels: {2: np.int64(4000)}\n",
      "client_3_label_3: 4000 samples\n",
      "  Labels: {3: np.int64(4000)}\n",
      "client_4_label_4: 4000 samples\n",
      "  Labels: {4: np.int64(4000)}\n",
      "client_5_label_5: 4000 samples\n",
      "  Labels: {5: np.int64(4000)}\n",
      "client_6_label_6: 4000 samples\n",
      "  Labels: {6: np.int64(4000)}\n",
      "client_7_label_7: 4000 samples\n",
      "  Labels: {7: np.int64(4000)}\n",
      "\n",
      "Tổng số mẫu: 33618\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Quantity Skewed Partition ===\n",
      "client_0: 898 samples\n",
      "  Labels: {0: np.int64(168), 3: np.int64(109), 7: np.int64(109), 6: np.int64(108), 5: np.int64(105), 4: np.int64(100), 1: np.int64(100), 2: np.int64(99)}\n",
      "client_1: 20044 samples\n",
      "  Labels: {0: np.int64(3542), 4: np.int64(2410), 5: np.int64(2409), 2: np.int64(2408), 6: np.int64(2396), 7: np.int64(2384), 3: np.int64(2358), 1: np.int64(2137)}\n",
      "client_2: 8385 samples\n",
      "  Labels: {0: np.int64(1508), 3: np.int64(1019), 6: np.int64(1011), 7: np.int64(996), 4: np.int64(991), 5: np.int64(987), 2: np.int64(969), 1: np.int64(904)}\n",
      "client_3: 4291 samples\n",
      "  Labels: {0: np.int64(782), 2: np.int64(524), 3: np.int64(514), 7: np.int64(511), 4: np.int64(499), 5: np.int64(499), 6: np.int64(485), 1: np.int64(477)}\n",
      "\n",
      "Tổng số mẫu: 33618\n",
      "\n",
      "client_0: 898 samples\n",
      "  Labels: {0: np.int64(168), 3: np.int64(109), 7: np.int64(109), 6: np.int64(108), 5: np.int64(105), 4: np.int64(100), 1: np.int64(100), 2: np.int64(99)}\n",
      "client_1: 20044 samples\n",
      "  Labels: {0: np.int64(3542), 4: np.int64(2410), 5: np.int64(2409), 2: np.int64(2408), 6: np.int64(2396), 7: np.int64(2384), 3: np.int64(2358), 1: np.int64(2137)}\n",
      "client_2: 8385 samples\n",
      "  Labels: {0: np.int64(1508), 3: np.int64(1019), 6: np.int64(1011), 7: np.int64(996), 4: np.int64(991), 5: np.int64(987), 2: np.int64(969), 1: np.int64(904)}\n",
      "client_3: 4291 samples\n",
      "  Labels: {0: np.int64(782), 2: np.int64(524), 3: np.int64(514), 7: np.int64(511), 4: np.int64(499), 5: np.int64(499), 6: np.int64(485), 1: np.int64(477)}\n",
      "\n",
      "Tổng số mẫu: 33618\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Phân chia dữ liệu cho Federated Learning\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.ticker as ticker\n",
    "def create_iid_partition(df, num_clients, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    df_shuffled = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    samples_per_client = len(df_shuffled) // num_clients\n",
    "    iid_clients = {}\n",
    "    for i in range(num_clients):\n",
    "        start_idx = i * samples_per_client\n",
    "        end_idx = len(df_shuffled) if i == num_clients - 1 else (i + 1) * samples_per_client\n",
    "        iid_clients[f'client_{i}'] = df_shuffled.iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "    return iid_clients\n",
    "def create_non_iid_partition_by_label(df, labels_per_client=1, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    unique_labels = sorted(df['label'].unique())\n",
    "    non_iid_clients = {}\n",
    "    if labels_per_client == 1:\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            client_data = df[df['label'] == label].reset_index(drop=True)\n",
    "            non_iid_clients[f'client_{i}_label_{label}'] = client_data\n",
    "    else:\n",
    "        random.shuffle(unique_labels)\n",
    "        num_clients = len(unique_labels) // labels_per_client\n",
    "        for i in range(num_clients):\n",
    "            client_labels = unique_labels[i*labels_per_client:(i+1)*labels_per_client]\n",
    "            client_data = df[df['label'].isin(client_labels)].reset_index(drop=True)\n",
    "            non_iid_clients[f'client_{i}_labels_{client_labels}'] = client_data\n",
    "    return non_iid_clients\n",
    "def create_quantity_skewed_partition(df, num_clients, skew_factor=0.1, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    proportions = np.random.power(skew_factor, num_clients)\n",
    "    proportions = proportions / proportions.sum()\n",
    "    df_shuffled = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    quantity_skewed_clients = {}\n",
    "    start_idx = 0\n",
    "    for i in range(num_clients):\n",
    "        num_samples = int(len(df_shuffled) * proportions[i])\n",
    "        end_idx = min(start_idx + num_samples, len(df_shuffled))\n",
    "        if i == num_clients - 1:\n",
    "            end_idx = len(df_shuffled)\n",
    "        quantity_skewed_clients[f'client_{i}'] = df_shuffled.iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "        start_idx = end_idx\n",
    "    return quantity_skewed_clients\n",
    "def analyze_partition(client_dict, title=\"Phân tích phân chia dữ liệu\"):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    total_samples = 0\n",
    "    label_distribution = defaultdict(int)\n",
    "    for client_id, client_df in client_dict.items():\n",
    "        num_samples = len(client_df)\n",
    "        total_samples += num_samples\n",
    "        client_labels = client_df['label'].value_counts()\n",
    "        print(f\"{client_id}: {num_samples} samples\")\n",
    "        print(f\"  Labels: {dict(client_labels)}\")\n",
    "        for label, count in client_labels.items():\n",
    "            label_distribution[label] += count\n",
    "    print(f\"\\nTổng số mẫu: {total_samples}\")\n",
    "    # Vẽ biểu đồ số mẫu mỗi client, trục x chỉ hiển thị số nguyên\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    client_sizes = [len(df) for df in client_dict.values()]\n",
    "    ax = plt.gca()\n",
    "    plt.bar(range(len(client_sizes)), client_sizes)\n",
    "    plt.title('Số mẫu mỗi client')\n",
    "    plt.xlabel('Client')\n",
    "    plt.ylabel('Số mẫu')\n",
    "    ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# Tạo các phân chia dữ liệu\n",
    "iid_clients = create_iid_partition(df, num_clients=4)\n",
    "analyze_partition(iid_clients, \"IID Partition\")\n",
    "non_iid_clients = create_non_iid_partition_by_label(df, labels_per_client=1)\n",
    "analyze_partition(non_iid_clients, \"Non-IID by Label\")\n",
    "quantity_skewed_clients = create_quantity_skewed_partition(df, num_clients=4, skew_factor=0.3)\n",
    "analyze_partition(quantity_skewed_clients, \"Quantity Skewed Partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94680fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Quantity Skewed Partition ===\n",
      "client_0: 1344 samples\n",
      "  Labels: {2: np.int64(168), 6: np.int64(168), 0: np.int64(168), 5: np.int64(168), 3: np.int64(168), 7: np.int64(168), 4: np.int64(168), 1: np.int64(168)}\n",
      "client_1: 28336 samples\n",
      "  Labels: {1: np.int64(3542), 5: np.int64(3542), 2: np.int64(3542), 0: np.int64(3542), 7: np.int64(3542), 4: np.int64(3542), 3: np.int64(3542), 6: np.int64(3542)}\n",
      "client_2: 12064 samples\n",
      "  Labels: {1: np.int64(1508), 3: np.int64(1508), 2: np.int64(1508), 0: np.int64(1508), 4: np.int64(1508), 7: np.int64(1508), 6: np.int64(1508), 5: np.int64(1508)}\n",
      "client_3: 6256 samples\n",
      "  Labels: {2: np.int64(782), 7: np.int64(782), 6: np.int64(782), 1: np.int64(782), 0: np.int64(782), 4: np.int64(782), 5: np.int64(782), 3: np.int64(782)}\n",
      "\n",
      "Tổng số mẫu: 48000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def oversample_client_df(client_df):\n",
    "    max_count = client_df['label'].value_counts().max()\n",
    "    dfs = []\n",
    "    for label in client_df['label'].unique():\n",
    "        df_label = client_df[client_df['label'] == label]\n",
    "        df_label_upsampled = resample(df_label, replace=True, n_samples=max_count, random_state=42)\n",
    "        dfs.append(df_label_upsampled)\n",
    "    return pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "# Áp dụng cho từng client trong quantity_skewed_clients\n",
    "for client_id in quantity_skewed_clients:\n",
    "    quantity_skewed_clients[client_id] = oversample_client_df(quantity_skewed_clients[client_id])\n",
    "analyze_partition(quantity_skewed_clients, \"Quantity Skewed Partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "970bf740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000 | Train Acc: 12.57% | Test Acc: 74.38% | Loss: 1.3843\n",
      "Epoch 2/2000 | Train Acc: 73.89% | Test Acc: 77.95% | Loss: 1.2603\n",
      "Epoch 3/2000 | Train Acc: 77.70% | Test Acc: 79.28% | Loss: 1.1528\n",
      "Epoch 4/2000 | Train Acc: 79.42% | Test Acc: 80.36% | Loss: 1.0552\n",
      "Epoch 5/2000 | Train Acc: 80.14% | Test Acc: 80.99% | Loss: 0.9633\n",
      "Epoch 6/2000 | Train Acc: 80.84% | Test Acc: 81.61% | Loss: 0.8758\n",
      "Epoch 4/2000 | Train Acc: 79.42% | Test Acc: 80.36% | Loss: 1.0552\n",
      "Epoch 5/2000 | Train Acc: 80.14% | Test Acc: 80.99% | Loss: 0.9633\n",
      "Epoch 6/2000 | Train Acc: 80.84% | Test Acc: 81.61% | Loss: 0.8758\n",
      "Epoch 7/2000 | Train Acc: 81.48% | Test Acc: 82.38% | Loss: 0.7932\n",
      "Epoch 8/2000 | Train Acc: 82.13% | Test Acc: 82.95% | Loss: 0.7164\n",
      "Epoch 9/2000 | Train Acc: 82.83% | Test Acc: 83.31% | Loss: 0.6467\n",
      "Epoch 7/2000 | Train Acc: 81.48% | Test Acc: 82.38% | Loss: 0.7932\n",
      "Epoch 8/2000 | Train Acc: 82.13% | Test Acc: 82.95% | Loss: 0.7164\n",
      "Epoch 9/2000 | Train Acc: 82.83% | Test Acc: 83.31% | Loss: 0.6467\n",
      "Epoch 10/2000 | Train Acc: 83.30% | Test Acc: 83.65% | Loss: 0.5852\n",
      "Epoch 11/2000 | Train Acc: 83.76% | Test Acc: 84.14% | Loss: 0.5330\n",
      "Epoch 12/2000 | Train Acc: 84.23% | Test Acc: 84.45% | Loss: 0.4898\n",
      "Epoch 10/2000 | Train Acc: 83.30% | Test Acc: 83.65% | Loss: 0.5852\n",
      "Epoch 11/2000 | Train Acc: 83.76% | Test Acc: 84.14% | Loss: 0.5330\n",
      "Epoch 12/2000 | Train Acc: 84.23% | Test Acc: 84.45% | Loss: 0.4898\n",
      "Epoch 13/2000 | Train Acc: 84.71% | Test Acc: 85.84% | Loss: 0.4547\n",
      "Epoch 14/2000 | Train Acc: 85.92% | Test Acc: 86.29% | Loss: 0.4259\n",
      "Epoch 15/2000 | Train Acc: 86.49% | Test Acc: 87.32% | Loss: 0.4013\n",
      "Epoch 16/2000 | Train Acc: 87.24% | Test Acc: 88.05% | Loss: 0.3795\n",
      "Epoch 13/2000 | Train Acc: 84.71% | Test Acc: 85.84% | Loss: 0.4547\n",
      "Epoch 14/2000 | Train Acc: 85.92% | Test Acc: 86.29% | Loss: 0.4259\n",
      "Epoch 15/2000 | Train Acc: 86.49% | Test Acc: 87.32% | Loss: 0.4013\n",
      "Epoch 16/2000 | Train Acc: 87.24% | Test Acc: 88.05% | Loss: 0.3795\n",
      "Epoch 17/2000 | Train Acc: 87.77% | Test Acc: 88.22% | Loss: 0.3597\n",
      "Epoch 18/2000 | Train Acc: 88.12% | Test Acc: 88.11% | Loss: 0.3421\n",
      "Epoch 19/2000 | Train Acc: 88.26% | Test Acc: 88.45% | Loss: 0.3271\n",
      "Epoch 17/2000 | Train Acc: 87.77% | Test Acc: 88.22% | Loss: 0.3597\n",
      "Epoch 18/2000 | Train Acc: 88.12% | Test Acc: 88.11% | Loss: 0.3421\n",
      "Epoch 19/2000 | Train Acc: 88.26% | Test Acc: 88.45% | Loss: 0.3271\n",
      "Epoch 20/2000 | Train Acc: 88.48% | Test Acc: 89.02% | Loss: 0.3149\n",
      "Epoch 21/2000 | Train Acc: 88.83% | Test Acc: 89.02% | Loss: 0.3049\n",
      "Epoch 22/2000 | Train Acc: 89.05% | Test Acc: 89.36% | Loss: 0.2959\n",
      "Epoch 20/2000 | Train Acc: 88.48% | Test Acc: 89.02% | Loss: 0.3149\n",
      "Epoch 21/2000 | Train Acc: 88.83% | Test Acc: 89.02% | Loss: 0.3049\n",
      "Epoch 22/2000 | Train Acc: 89.05% | Test Acc: 89.36% | Loss: 0.2959\n",
      "Epoch 23/2000 | Train Acc: 89.54% | Test Acc: 90.44% | Loss: 0.2866\n",
      "Epoch 24/2000 | Train Acc: 90.49% | Test Acc: 91.20% | Loss: 0.2767\n",
      "Epoch 25/2000 | Train Acc: 91.31% | Test Acc: 91.54% | Loss: 0.2669\n",
      "Epoch 23/2000 | Train Acc: 89.54% | Test Acc: 90.44% | Loss: 0.2866\n",
      "Epoch 24/2000 | Train Acc: 90.49% | Test Acc: 91.20% | Loss: 0.2767\n",
      "Epoch 25/2000 | Train Acc: 91.31% | Test Acc: 91.54% | Loss: 0.2669\n",
      "Epoch 26/2000 | Train Acc: 91.46% | Test Acc: 91.63% | Loss: 0.2584\n",
      "Epoch 27/2000 | Train Acc: 91.51% | Test Acc: 91.37% | Loss: 0.2517\n",
      "Epoch 28/2000 | Train Acc: 91.51% | Test Acc: 91.12% | Loss: 0.2464\n",
      "Epoch 26/2000 | Train Acc: 91.46% | Test Acc: 91.63% | Loss: 0.2584\n",
      "Epoch 27/2000 | Train Acc: 91.51% | Test Acc: 91.37% | Loss: 0.2517\n",
      "Epoch 28/2000 | Train Acc: 91.51% | Test Acc: 91.12% | Loss: 0.2464\n",
      "Epoch 29/2000 | Train Acc: 91.52% | Test Acc: 91.29% | Loss: 0.2415\n",
      "Epoch 30/2000 | Train Acc: 91.71% | Test Acc: 91.77% | Loss: 0.2362\n",
      "Epoch 31/2000 | Train Acc: 92.23% | Test Acc: 92.03% | Loss: 0.2306\n",
      "Epoch 29/2000 | Train Acc: 91.52% | Test Acc: 91.29% | Loss: 0.2415\n",
      "Epoch 30/2000 | Train Acc: 91.71% | Test Acc: 91.77% | Loss: 0.2362\n",
      "Epoch 31/2000 | Train Acc: 92.23% | Test Acc: 92.03% | Loss: 0.2306\n",
      "Epoch 32/2000 | Train Acc: 92.66% | Test Acc: 92.65% | Loss: 0.2249\n",
      "Epoch 33/2000 | Train Acc: 93.04% | Test Acc: 93.16% | Loss: 0.2195\n",
      "Epoch 34/2000 | Train Acc: 93.54% | Test Acc: 93.36% | Loss: 0.2145\n",
      "Epoch 32/2000 | Train Acc: 92.66% | Test Acc: 92.65% | Loss: 0.2249\n",
      "Epoch 33/2000 | Train Acc: 93.04% | Test Acc: 93.16% | Loss: 0.2195\n",
      "Epoch 34/2000 | Train Acc: 93.54% | Test Acc: 93.36% | Loss: 0.2145\n",
      "Epoch 35/2000 | Train Acc: 93.61% | Test Acc: 93.53% | Loss: 0.2096\n",
      "Epoch 36/2000 | Train Acc: 93.76% | Test Acc: 93.70% | Loss: 0.2046\n",
      "Epoch 37/2000 | Train Acc: 94.01% | Test Acc: 93.79% | Loss: 0.1994\n",
      "Epoch 35/2000 | Train Acc: 93.61% | Test Acc: 93.53% | Loss: 0.2096\n",
      "Epoch 36/2000 | Train Acc: 93.76% | Test Acc: 93.70% | Loss: 0.2046\n",
      "Epoch 37/2000 | Train Acc: 94.01% | Test Acc: 93.79% | Loss: 0.1994\n",
      "Epoch 38/2000 | Train Acc: 94.21% | Test Acc: 93.81% | Loss: 0.1945\n",
      "Epoch 39/2000 | Train Acc: 94.36% | Test Acc: 94.01% | Loss: 0.1900\n",
      "Epoch 40/2000 | Train Acc: 94.46% | Test Acc: 94.10% | Loss: 0.1858\n",
      "Epoch 38/2000 | Train Acc: 94.21% | Test Acc: 93.81% | Loss: 0.1945\n",
      "Epoch 39/2000 | Train Acc: 94.36% | Test Acc: 94.01% | Loss: 0.1900\n",
      "Epoch 40/2000 | Train Acc: 94.46% | Test Acc: 94.10% | Loss: 0.1858\n",
      "Epoch 41/2000 | Train Acc: 94.64% | Test Acc: 94.27% | Loss: 0.1817\n",
      "Epoch 42/2000 | Train Acc: 94.86% | Test Acc: 94.41% | Loss: 0.1774\n",
      "Epoch 43/2000 | Train Acc: 95.04% | Test Acc: 94.61% | Loss: 0.1731\n",
      "Epoch 41/2000 | Train Acc: 94.64% | Test Acc: 94.27% | Loss: 0.1817\n",
      "Epoch 42/2000 | Train Acc: 94.86% | Test Acc: 94.41% | Loss: 0.1774\n",
      "Epoch 43/2000 | Train Acc: 95.04% | Test Acc: 94.61% | Loss: 0.1731\n",
      "Epoch 44/2000 | Train Acc: 95.11% | Test Acc: 94.69% | Loss: 0.1690\n",
      "Epoch 45/2000 | Train Acc: 95.25% | Test Acc: 94.72% | Loss: 0.1651\n",
      "Epoch 46/2000 | Train Acc: 95.39% | Test Acc: 94.69% | Loss: 0.1615\n",
      "Epoch 47/2000 | Train Acc: 95.44% | Test Acc: 94.81% | Loss: 0.1578\n",
      "Epoch 44/2000 | Train Acc: 95.11% | Test Acc: 94.69% | Loss: 0.1690\n",
      "Epoch 45/2000 | Train Acc: 95.25% | Test Acc: 94.72% | Loss: 0.1651\n",
      "Epoch 46/2000 | Train Acc: 95.39% | Test Acc: 94.69% | Loss: 0.1615\n",
      "Epoch 47/2000 | Train Acc: 95.44% | Test Acc: 94.81% | Loss: 0.1578\n",
      "Epoch 48/2000 | Train Acc: 95.57% | Test Acc: 94.92% | Loss: 0.1542\n",
      "Epoch 49/2000 | Train Acc: 95.68% | Test Acc: 94.95% | Loss: 0.1506\n",
      "Epoch 50/2000 | Train Acc: 95.78% | Test Acc: 95.15% | Loss: 0.1472\n",
      "Epoch 48/2000 | Train Acc: 95.57% | Test Acc: 94.92% | Loss: 0.1542\n",
      "Epoch 49/2000 | Train Acc: 95.68% | Test Acc: 94.95% | Loss: 0.1506\n",
      "Epoch 50/2000 | Train Acc: 95.78% | Test Acc: 95.15% | Loss: 0.1472\n",
      "Epoch 51/2000 | Train Acc: 95.84% | Test Acc: 95.18% | Loss: 0.1438\n",
      "Epoch 52/2000 | Train Acc: 95.93% | Test Acc: 95.37% | Loss: 0.1405\n",
      "Epoch 53/2000 | Train Acc: 96.01% | Test Acc: 95.43% | Loss: 0.1372\n",
      "Epoch 51/2000 | Train Acc: 95.84% | Test Acc: 95.18% | Loss: 0.1438\n",
      "Epoch 52/2000 | Train Acc: 95.93% | Test Acc: 95.37% | Loss: 0.1405\n",
      "Epoch 53/2000 | Train Acc: 96.01% | Test Acc: 95.43% | Loss: 0.1372\n",
      "Epoch 54/2000 | Train Acc: 96.10% | Test Acc: 95.52% | Loss: 0.1339\n",
      "Epoch 55/2000 | Train Acc: 96.23% | Test Acc: 95.54% | Loss: 0.1308\n",
      "Epoch 56/2000 | Train Acc: 96.32% | Test Acc: 95.66% | Loss: 0.1278\n",
      "Epoch 54/2000 | Train Acc: 96.10% | Test Acc: 95.52% | Loss: 0.1339\n",
      "Epoch 55/2000 | Train Acc: 96.23% | Test Acc: 95.54% | Loss: 0.1308\n",
      "Epoch 56/2000 | Train Acc: 96.32% | Test Acc: 95.66% | Loss: 0.1278\n",
      "Epoch 57/2000 | Train Acc: 96.42% | Test Acc: 95.83% | Loss: 0.1248\n",
      "Epoch 58/2000 | Train Acc: 96.52% | Test Acc: 95.91% | Loss: 0.1219\n",
      "Epoch 59/2000 | Train Acc: 96.61% | Test Acc: 96.03% | Loss: 0.1190\n",
      "Epoch 57/2000 | Train Acc: 96.42% | Test Acc: 95.83% | Loss: 0.1248\n",
      "Epoch 58/2000 | Train Acc: 96.52% | Test Acc: 95.91% | Loss: 0.1219\n",
      "Epoch 59/2000 | Train Acc: 96.61% | Test Acc: 96.03% | Loss: 0.1190\n",
      "Epoch 60/2000 | Train Acc: 96.67% | Test Acc: 96.20% | Loss: 0.1162\n",
      "Epoch 61/2000 | Train Acc: 96.74% | Test Acc: 96.31% | Loss: 0.1135\n",
      "Epoch 62/2000 | Train Acc: 96.82% | Test Acc: 96.40% | Loss: 0.1107\n",
      "Epoch 60/2000 | Train Acc: 96.67% | Test Acc: 96.20% | Loss: 0.1162\n",
      "Epoch 61/2000 | Train Acc: 96.74% | Test Acc: 96.31% | Loss: 0.1135\n",
      "Epoch 62/2000 | Train Acc: 96.82% | Test Acc: 96.40% | Loss: 0.1107\n",
      "Epoch 63/2000 | Train Acc: 96.88% | Test Acc: 96.42% | Loss: 0.1081\n",
      "Epoch 64/2000 | Train Acc: 96.99% | Test Acc: 96.45% | Loss: 0.1055\n",
      "Epoch 65/2000 | Train Acc: 97.11% | Test Acc: 96.51% | Loss: 0.1030\n",
      "Epoch 63/2000 | Train Acc: 96.88% | Test Acc: 96.42% | Loss: 0.1081\n",
      "Epoch 64/2000 | Train Acc: 96.99% | Test Acc: 96.45% | Loss: 0.1055\n",
      "Epoch 65/2000 | Train Acc: 97.11% | Test Acc: 96.51% | Loss: 0.1030\n",
      "Epoch 66/2000 | Train Acc: 97.16% | Test Acc: 96.51% | Loss: 0.1005\n",
      "Epoch 67/2000 | Train Acc: 97.20% | Test Acc: 96.54% | Loss: 0.0981\n",
      "Epoch 68/2000 | Train Acc: 97.24% | Test Acc: 96.59% | Loss: 0.0958\n",
      "Epoch 66/2000 | Train Acc: 97.16% | Test Acc: 96.51% | Loss: 0.1005\n",
      "Epoch 67/2000 | Train Acc: 97.20% | Test Acc: 96.54% | Loss: 0.0981\n",
      "Epoch 68/2000 | Train Acc: 97.24% | Test Acc: 96.59% | Loss: 0.0958\n",
      "Epoch 69/2000 | Train Acc: 97.33% | Test Acc: 96.65% | Loss: 0.0935\n",
      "Epoch 70/2000 | Train Acc: 97.37% | Test Acc: 96.77% | Loss: 0.0912\n",
      "Epoch 71/2000 | Train Acc: 97.45% | Test Acc: 96.88% | Loss: 0.0890\n",
      "Epoch 69/2000 | Train Acc: 97.33% | Test Acc: 96.65% | Loss: 0.0935\n",
      "Epoch 70/2000 | Train Acc: 97.37% | Test Acc: 96.77% | Loss: 0.0912\n",
      "Epoch 71/2000 | Train Acc: 97.45% | Test Acc: 96.88% | Loss: 0.0890\n",
      "Epoch 72/2000 | Train Acc: 97.52% | Test Acc: 96.99% | Loss: 0.0869\n",
      "Epoch 73/2000 | Train Acc: 97.58% | Test Acc: 97.08% | Loss: 0.0848\n",
      "Epoch 74/2000 | Train Acc: 97.64% | Test Acc: 97.19% | Loss: 0.0828\n",
      "Epoch 72/2000 | Train Acc: 97.52% | Test Acc: 96.99% | Loss: 0.0869\n",
      "Epoch 73/2000 | Train Acc: 97.58% | Test Acc: 97.08% | Loss: 0.0848\n",
      "Epoch 74/2000 | Train Acc: 97.64% | Test Acc: 97.19% | Loss: 0.0828\n",
      "Epoch 75/2000 | Train Acc: 97.70% | Test Acc: 97.25% | Loss: 0.0808\n",
      "Epoch 76/2000 | Train Acc: 97.78% | Test Acc: 97.28% | Loss: 0.0789\n",
      "Epoch 77/2000 | Train Acc: 97.84% | Test Acc: 97.30% | Loss: 0.0770\n",
      "Epoch 75/2000 | Train Acc: 97.70% | Test Acc: 97.25% | Loss: 0.0808\n",
      "Epoch 76/2000 | Train Acc: 97.78% | Test Acc: 97.28% | Loss: 0.0789\n",
      "Epoch 77/2000 | Train Acc: 97.84% | Test Acc: 97.30% | Loss: 0.0770\n",
      "Epoch 78/2000 | Train Acc: 97.89% | Test Acc: 97.33% | Loss: 0.0752\n",
      "Epoch 79/2000 | Train Acc: 97.92% | Test Acc: 97.33% | Loss: 0.0734\n",
      "Epoch 80/2000 | Train Acc: 98.01% | Test Acc: 97.36% | Loss: 0.0717\n",
      "Epoch 78/2000 | Train Acc: 97.89% | Test Acc: 97.33% | Loss: 0.0752\n",
      "Epoch 79/2000 | Train Acc: 97.92% | Test Acc: 97.33% | Loss: 0.0734\n",
      "Epoch 80/2000 | Train Acc: 98.01% | Test Acc: 97.36% | Loss: 0.0717\n",
      "Epoch 81/2000 | Train Acc: 98.06% | Test Acc: 97.47% | Loss: 0.0700\n",
      "Epoch 82/2000 | Train Acc: 98.09% | Test Acc: 97.47% | Loss: 0.0684\n",
      "Epoch 83/2000 | Train Acc: 98.17% | Test Acc: 97.50% | Loss: 0.0669\n",
      "Epoch 81/2000 | Train Acc: 98.06% | Test Acc: 97.47% | Loss: 0.0700\n",
      "Epoch 82/2000 | Train Acc: 98.09% | Test Acc: 97.47% | Loss: 0.0684\n",
      "Epoch 83/2000 | Train Acc: 98.17% | Test Acc: 97.50% | Loss: 0.0669\n",
      "Epoch 84/2000 | Train Acc: 98.18% | Test Acc: 97.56% | Loss: 0.0653\n",
      "Epoch 85/2000 | Train Acc: 98.23% | Test Acc: 97.56% | Loss: 0.0639\n",
      "Epoch 86/2000 | Train Acc: 98.26% | Test Acc: 97.59% | Loss: 0.0624\n",
      "Epoch 84/2000 | Train Acc: 98.18% | Test Acc: 97.56% | Loss: 0.0653\n",
      "Epoch 85/2000 | Train Acc: 98.23% | Test Acc: 97.56% | Loss: 0.0639\n",
      "Epoch 86/2000 | Train Acc: 98.26% | Test Acc: 97.59% | Loss: 0.0624\n",
      "Epoch 87/2000 | Train Acc: 98.28% | Test Acc: 97.62% | Loss: 0.0611\n",
      "Epoch 88/2000 | Train Acc: 98.33% | Test Acc: 97.64% | Loss: 0.0597\n",
      "Epoch 89/2000 | Train Acc: 98.37% | Test Acc: 97.70% | Loss: 0.0584\n",
      "Epoch 87/2000 | Train Acc: 98.28% | Test Acc: 97.62% | Loss: 0.0611\n",
      "Epoch 88/2000 | Train Acc: 98.33% | Test Acc: 97.64% | Loss: 0.0597\n",
      "Epoch 89/2000 | Train Acc: 98.37% | Test Acc: 97.70% | Loss: 0.0584\n",
      "Epoch 90/2000 | Train Acc: 98.40% | Test Acc: 97.76% | Loss: 0.0572\n",
      "Epoch 91/2000 | Train Acc: 98.47% | Test Acc: 97.76% | Loss: 0.0559\n",
      "Epoch 92/2000 | Train Acc: 98.52% | Test Acc: 97.76% | Loss: 0.0548\n",
      "Epoch 90/2000 | Train Acc: 98.40% | Test Acc: 97.76% | Loss: 0.0572\n",
      "Epoch 91/2000 | Train Acc: 98.47% | Test Acc: 97.76% | Loss: 0.0559\n",
      "Epoch 92/2000 | Train Acc: 98.52% | Test Acc: 97.76% | Loss: 0.0548\n",
      "Epoch 93/2000 | Train Acc: 98.55% | Test Acc: 97.79% | Loss: 0.0536\n",
      "Epoch 94/2000 | Train Acc: 98.57% | Test Acc: 97.79% | Loss: 0.0525\n",
      "Epoch 95/2000 | Train Acc: 98.63% | Test Acc: 97.81% | Loss: 0.0514\n",
      "Epoch 93/2000 | Train Acc: 98.55% | Test Acc: 97.79% | Loss: 0.0536\n",
      "Epoch 94/2000 | Train Acc: 98.57% | Test Acc: 97.79% | Loss: 0.0525\n",
      "Epoch 95/2000 | Train Acc: 98.63% | Test Acc: 97.81% | Loss: 0.0514\n",
      "Epoch 96/2000 | Train Acc: 98.67% | Test Acc: 97.93% | Loss: 0.0504\n",
      "Epoch 97/2000 | Train Acc: 98.72% | Test Acc: 97.96% | Loss: 0.0494\n",
      "Epoch 98/2000 | Train Acc: 98.76% | Test Acc: 97.96% | Loss: 0.0484\n",
      "Epoch 96/2000 | Train Acc: 98.67% | Test Acc: 97.93% | Loss: 0.0504\n",
      "Epoch 97/2000 | Train Acc: 98.72% | Test Acc: 97.96% | Loss: 0.0494\n",
      "Epoch 98/2000 | Train Acc: 98.76% | Test Acc: 97.96% | Loss: 0.0484\n",
      "Epoch 99/2000 | Train Acc: 98.79% | Test Acc: 97.99% | Loss: 0.0475\n",
      "Epoch 100/2000 | Train Acc: 98.84% | Test Acc: 97.99% | Loss: 0.0466\n",
      "Epoch 101/2000 | Train Acc: 98.88% | Test Acc: 98.01% | Loss: 0.0457\n",
      "Epoch 99/2000 | Train Acc: 98.79% | Test Acc: 97.99% | Loss: 0.0475\n",
      "Epoch 100/2000 | Train Acc: 98.84% | Test Acc: 97.99% | Loss: 0.0466\n",
      "Epoch 101/2000 | Train Acc: 98.88% | Test Acc: 98.01% | Loss: 0.0457\n",
      "Epoch 102/2000 | Train Acc: 98.91% | Test Acc: 98.04% | Loss: 0.0448\n",
      "Epoch 103/2000 | Train Acc: 98.93% | Test Acc: 98.04% | Loss: 0.0440\n",
      "Epoch 104/2000 | Train Acc: 98.98% | Test Acc: 98.07% | Loss: 0.0432\n",
      "Epoch 105/2000 | Train Acc: 98.99% | Test Acc: 98.07% | Loss: 0.0424\n",
      "Epoch 102/2000 | Train Acc: 98.91% | Test Acc: 98.04% | Loss: 0.0448\n",
      "Epoch 103/2000 | Train Acc: 98.93% | Test Acc: 98.04% | Loss: 0.0440\n",
      "Epoch 104/2000 | Train Acc: 98.98% | Test Acc: 98.07% | Loss: 0.0432\n",
      "Epoch 105/2000 | Train Acc: 98.99% | Test Acc: 98.07% | Loss: 0.0424\n",
      "Epoch 106/2000 | Train Acc: 99.01% | Test Acc: 98.07% | Loss: 0.0417\n",
      "Epoch 107/2000 | Train Acc: 99.04% | Test Acc: 98.07% | Loss: 0.0409\n",
      "Epoch 108/2000 | Train Acc: 99.06% | Test Acc: 98.07% | Loss: 0.0402\n",
      "Epoch 106/2000 | Train Acc: 99.01% | Test Acc: 98.07% | Loss: 0.0417\n",
      "Epoch 107/2000 | Train Acc: 99.04% | Test Acc: 98.07% | Loss: 0.0409\n",
      "Epoch 108/2000 | Train Acc: 99.06% | Test Acc: 98.07% | Loss: 0.0402\n",
      "Epoch 109/2000 | Train Acc: 99.06% | Test Acc: 98.07% | Loss: 0.0395\n",
      "Epoch 110/2000 | Train Acc: 99.12% | Test Acc: 98.13% | Loss: 0.0388\n",
      "Epoch 111/2000 | Train Acc: 99.13% | Test Acc: 98.16% | Loss: 0.0382\n",
      "Epoch 109/2000 | Train Acc: 99.06% | Test Acc: 98.07% | Loss: 0.0395\n",
      "Epoch 110/2000 | Train Acc: 99.12% | Test Acc: 98.13% | Loss: 0.0388\n",
      "Epoch 111/2000 | Train Acc: 99.13% | Test Acc: 98.16% | Loss: 0.0382\n",
      "Epoch 112/2000 | Train Acc: 99.14% | Test Acc: 98.18% | Loss: 0.0375\n",
      "Epoch 113/2000 | Train Acc: 99.16% | Test Acc: 98.21% | Loss: 0.0369\n",
      "Epoch 114/2000 | Train Acc: 99.18% | Test Acc: 98.21% | Loss: 0.0363\n",
      "Epoch 112/2000 | Train Acc: 99.14% | Test Acc: 98.18% | Loss: 0.0375\n",
      "Epoch 113/2000 | Train Acc: 99.16% | Test Acc: 98.21% | Loss: 0.0369\n",
      "Epoch 114/2000 | Train Acc: 99.18% | Test Acc: 98.21% | Loss: 0.0363\n",
      "Epoch 115/2000 | Train Acc: 99.20% | Test Acc: 98.30% | Loss: 0.0357\n",
      "Epoch 116/2000 | Train Acc: 99.21% | Test Acc: 98.33% | Loss: 0.0351\n",
      "Epoch 117/2000 | Train Acc: 99.22% | Test Acc: 98.33% | Loss: 0.0346\n",
      "Epoch 115/2000 | Train Acc: 99.20% | Test Acc: 98.30% | Loss: 0.0357\n",
      "Epoch 116/2000 | Train Acc: 99.21% | Test Acc: 98.33% | Loss: 0.0351\n",
      "Epoch 117/2000 | Train Acc: 99.22% | Test Acc: 98.33% | Loss: 0.0346\n",
      "Epoch 118/2000 | Train Acc: 99.23% | Test Acc: 98.35% | Loss: 0.0340\n",
      "Epoch 119/2000 | Train Acc: 99.24% | Test Acc: 98.35% | Loss: 0.0335\n",
      "Epoch 120/2000 | Train Acc: 99.26% | Test Acc: 98.35% | Loss: 0.0330\n",
      "Epoch 118/2000 | Train Acc: 99.23% | Test Acc: 98.35% | Loss: 0.0340\n",
      "Epoch 119/2000 | Train Acc: 99.24% | Test Acc: 98.35% | Loss: 0.0335\n",
      "Epoch 120/2000 | Train Acc: 99.26% | Test Acc: 98.35% | Loss: 0.0330\n",
      "Epoch 121/2000 | Train Acc: 99.28% | Test Acc: 98.35% | Loss: 0.0325\n",
      "Epoch 122/2000 | Train Acc: 99.29% | Test Acc: 98.38% | Loss: 0.0320\n",
      "Epoch 123/2000 | Train Acc: 99.30% | Test Acc: 98.38% | Loss: 0.0315\n",
      "Epoch 121/2000 | Train Acc: 99.28% | Test Acc: 98.35% | Loss: 0.0325\n",
      "Epoch 122/2000 | Train Acc: 99.29% | Test Acc: 98.38% | Loss: 0.0320\n",
      "Epoch 123/2000 | Train Acc: 99.30% | Test Acc: 98.38% | Loss: 0.0315\n",
      "Epoch 124/2000 | Train Acc: 99.30% | Test Acc: 98.41% | Loss: 0.0311\n",
      "Epoch 125/2000 | Train Acc: 99.31% | Test Acc: 98.41% | Loss: 0.0306\n",
      "Epoch 126/2000 | Train Acc: 99.33% | Test Acc: 98.44% | Loss: 0.0302\n",
      "Epoch 124/2000 | Train Acc: 99.30% | Test Acc: 98.41% | Loss: 0.0311\n",
      "Epoch 125/2000 | Train Acc: 99.31% | Test Acc: 98.41% | Loss: 0.0306\n",
      "Epoch 126/2000 | Train Acc: 99.33% | Test Acc: 98.44% | Loss: 0.0302\n",
      "Epoch 127/2000 | Train Acc: 99.33% | Test Acc: 98.44% | Loss: 0.0297\n",
      "Epoch 128/2000 | Train Acc: 99.35% | Test Acc: 98.44% | Loss: 0.0293\n",
      "Epoch 129/2000 | Train Acc: 99.35% | Test Acc: 98.44% | Loss: 0.0289\n",
      "Epoch 127/2000 | Train Acc: 99.33% | Test Acc: 98.44% | Loss: 0.0297\n",
      "Epoch 128/2000 | Train Acc: 99.35% | Test Acc: 98.44% | Loss: 0.0293\n",
      "Epoch 129/2000 | Train Acc: 99.35% | Test Acc: 98.44% | Loss: 0.0289\n",
      "Epoch 130/2000 | Train Acc: 99.36% | Test Acc: 98.44% | Loss: 0.0285\n",
      "Epoch 131/2000 | Train Acc: 99.38% | Test Acc: 98.47% | Loss: 0.0281\n",
      "Epoch 132/2000 | Train Acc: 99.38% | Test Acc: 98.50% | Loss: 0.0278\n",
      "Epoch 130/2000 | Train Acc: 99.36% | Test Acc: 98.44% | Loss: 0.0285\n",
      "Epoch 131/2000 | Train Acc: 99.38% | Test Acc: 98.47% | Loss: 0.0281\n",
      "Epoch 132/2000 | Train Acc: 99.38% | Test Acc: 98.50% | Loss: 0.0278\n",
      "Epoch 133/2000 | Train Acc: 99.38% | Test Acc: 98.50% | Loss: 0.0274\n",
      "Epoch 134/2000 | Train Acc: 99.40% | Test Acc: 98.55% | Loss: 0.0270\n",
      "Epoch 135/2000 | Train Acc: 99.40% | Test Acc: 98.55% | Loss: 0.0267\n",
      "Epoch 133/2000 | Train Acc: 99.38% | Test Acc: 98.50% | Loss: 0.0274\n",
      "Epoch 134/2000 | Train Acc: 99.40% | Test Acc: 98.55% | Loss: 0.0270\n",
      "Epoch 135/2000 | Train Acc: 99.40% | Test Acc: 98.55% | Loss: 0.0267\n",
      "Epoch 136/2000 | Train Acc: 99.42% | Test Acc: 98.61% | Loss: 0.0264\n",
      "Epoch 137/2000 | Train Acc: 99.44% | Test Acc: 98.58% | Loss: 0.0260\n",
      "Epoch 138/2000 | Train Acc: 99.45% | Test Acc: 98.58% | Loss: 0.0257\n",
      "Epoch 136/2000 | Train Acc: 99.42% | Test Acc: 98.61% | Loss: 0.0264\n",
      "Epoch 137/2000 | Train Acc: 99.44% | Test Acc: 98.58% | Loss: 0.0260\n",
      "Epoch 138/2000 | Train Acc: 99.45% | Test Acc: 98.58% | Loss: 0.0257\n",
      "Epoch 139/2000 | Train Acc: 99.45% | Test Acc: 98.58% | Loss: 0.0254\n",
      "Epoch 140/2000 | Train Acc: 99.47% | Test Acc: 98.58% | Loss: 0.0251\n",
      "Epoch 141/2000 | Train Acc: 99.49% | Test Acc: 98.58% | Loss: 0.0248\n",
      "Epoch 139/2000 | Train Acc: 99.45% | Test Acc: 98.58% | Loss: 0.0254\n",
      "Epoch 140/2000 | Train Acc: 99.47% | Test Acc: 98.58% | Loss: 0.0251\n",
      "Epoch 141/2000 | Train Acc: 99.49% | Test Acc: 98.58% | Loss: 0.0248\n",
      "Epoch 142/2000 | Train Acc: 99.50% | Test Acc: 98.58% | Loss: 0.0245\n",
      "Epoch 143/2000 | Train Acc: 99.51% | Test Acc: 98.61% | Loss: 0.0242\n",
      "Epoch 144/2000 | Train Acc: 99.51% | Test Acc: 98.61% | Loss: 0.0239\n",
      "Epoch 142/2000 | Train Acc: 99.50% | Test Acc: 98.58% | Loss: 0.0245\n",
      "Epoch 143/2000 | Train Acc: 99.51% | Test Acc: 98.61% | Loss: 0.0242\n",
      "Epoch 144/2000 | Train Acc: 99.51% | Test Acc: 98.61% | Loss: 0.0239\n",
      "Epoch 145/2000 | Train Acc: 99.53% | Test Acc: 98.64% | Loss: 0.0236\n",
      "Epoch 146/2000 | Train Acc: 99.55% | Test Acc: 98.64% | Loss: 0.0234\n",
      "Epoch 147/2000 | Train Acc: 99.55% | Test Acc: 98.64% | Loss: 0.0231\n",
      "Epoch 145/2000 | Train Acc: 99.53% | Test Acc: 98.64% | Loss: 0.0236\n",
      "Epoch 146/2000 | Train Acc: 99.55% | Test Acc: 98.64% | Loss: 0.0234\n",
      "Epoch 147/2000 | Train Acc: 99.55% | Test Acc: 98.64% | Loss: 0.0231\n",
      "Epoch 148/2000 | Train Acc: 99.56% | Test Acc: 98.64% | Loss: 0.0228\n",
      "Epoch 149/2000 | Train Acc: 99.58% | Test Acc: 98.64% | Loss: 0.0226\n",
      "Epoch 150/2000 | Train Acc: 99.58% | Test Acc: 98.64% | Loss: 0.0224\n",
      "Epoch 148/2000 | Train Acc: 99.56% | Test Acc: 98.64% | Loss: 0.0228\n",
      "Epoch 149/2000 | Train Acc: 99.58% | Test Acc: 98.64% | Loss: 0.0226\n",
      "Epoch 150/2000 | Train Acc: 99.58% | Test Acc: 98.64% | Loss: 0.0224\n",
      "Epoch 151/2000 | Train Acc: 99.58% | Test Acc: 98.64% | Loss: 0.0221\n",
      "Epoch 152/2000 | Train Acc: 99.58% | Test Acc: 98.64% | Loss: 0.0219\n",
      "Epoch 153/2000 | Train Acc: 99.59% | Test Acc: 98.64% | Loss: 0.0217\n",
      "Epoch 154/2000 | Train Acc: 99.60% | Test Acc: 98.67% | Loss: 0.0214\n",
      "Epoch 151/2000 | Train Acc: 99.58% | Test Acc: 98.64% | Loss: 0.0221\n",
      "Epoch 152/2000 | Train Acc: 99.58% | Test Acc: 98.64% | Loss: 0.0219\n",
      "Epoch 153/2000 | Train Acc: 99.59% | Test Acc: 98.64% | Loss: 0.0217\n",
      "Epoch 154/2000 | Train Acc: 99.60% | Test Acc: 98.67% | Loss: 0.0214\n",
      "Epoch 155/2000 | Train Acc: 99.61% | Test Acc: 98.67% | Loss: 0.0212\n",
      "Epoch 156/2000 | Train Acc: 99.61% | Test Acc: 98.67% | Loss: 0.0210\n",
      "Epoch 157/2000 | Train Acc: 99.61% | Test Acc: 98.67% | Loss: 0.0208\n",
      "Epoch 155/2000 | Train Acc: 99.61% | Test Acc: 98.67% | Loss: 0.0212\n",
      "Epoch 156/2000 | Train Acc: 99.61% | Test Acc: 98.67% | Loss: 0.0210\n",
      "Epoch 157/2000 | Train Acc: 99.61% | Test Acc: 98.67% | Loss: 0.0208\n",
      "Epoch 158/2000 | Train Acc: 99.62% | Test Acc: 98.67% | Loss: 0.0206\n",
      "Epoch 159/2000 | Train Acc: 99.62% | Test Acc: 98.67% | Loss: 0.0204\n",
      "Epoch 160/2000 | Train Acc: 99.62% | Test Acc: 98.67% | Loss: 0.0202\n",
      "Epoch 161/2000 | Train Acc: 99.62% | Test Acc: 98.67% | Loss: 0.0200\n",
      "Epoch 158/2000 | Train Acc: 99.62% | Test Acc: 98.67% | Loss: 0.0206\n",
      "Epoch 159/2000 | Train Acc: 99.62% | Test Acc: 98.67% | Loss: 0.0204\n",
      "Epoch 160/2000 | Train Acc: 99.62% | Test Acc: 98.67% | Loss: 0.0202\n",
      "Epoch 161/2000 | Train Acc: 99.62% | Test Acc: 98.67% | Loss: 0.0200\n",
      "Epoch 162/2000 | Train Acc: 99.62% | Test Acc: 98.67% | Loss: 0.0198\n",
      "Epoch 163/2000 | Train Acc: 99.63% | Test Acc: 98.69% | Loss: 0.0197\n",
      "Epoch 164/2000 | Train Acc: 99.63% | Test Acc: 98.75% | Loss: 0.0195\n",
      "Epoch 162/2000 | Train Acc: 99.62% | Test Acc: 98.67% | Loss: 0.0198\n",
      "Epoch 163/2000 | Train Acc: 99.63% | Test Acc: 98.69% | Loss: 0.0197\n",
      "Epoch 164/2000 | Train Acc: 99.63% | Test Acc: 98.75% | Loss: 0.0195\n",
      "Epoch 165/2000 | Train Acc: 99.64% | Test Acc: 98.75% | Loss: 0.0193\n",
      "Epoch 166/2000 | Train Acc: 99.65% | Test Acc: 98.75% | Loss: 0.0191\n",
      "Epoch 167/2000 | Train Acc: 99.65% | Test Acc: 98.75% | Loss: 0.0190\n",
      "Epoch 168/2000 | Train Acc: 99.65% | Test Acc: 98.78% | Loss: 0.0188\n",
      "Epoch 165/2000 | Train Acc: 99.64% | Test Acc: 98.75% | Loss: 0.0193\n",
      "Epoch 166/2000 | Train Acc: 99.65% | Test Acc: 98.75% | Loss: 0.0191\n",
      "Epoch 167/2000 | Train Acc: 99.65% | Test Acc: 98.75% | Loss: 0.0190\n",
      "Epoch 168/2000 | Train Acc: 99.65% | Test Acc: 98.78% | Loss: 0.0188\n",
      "Epoch 169/2000 | Train Acc: 99.65% | Test Acc: 98.78% | Loss: 0.0187\n",
      "Epoch 170/2000 | Train Acc: 99.65% | Test Acc: 98.78% | Loss: 0.0185\n",
      "Epoch 171/2000 | Train Acc: 99.65% | Test Acc: 98.78% | Loss: 0.0184\n",
      "Epoch 169/2000 | Train Acc: 99.65% | Test Acc: 98.78% | Loss: 0.0187\n",
      "Epoch 170/2000 | Train Acc: 99.65% | Test Acc: 98.78% | Loss: 0.0185\n",
      "Epoch 171/2000 | Train Acc: 99.65% | Test Acc: 98.78% | Loss: 0.0184\n",
      "Epoch 172/2000 | Train Acc: 99.65% | Test Acc: 98.78% | Loss: 0.0182\n",
      "Epoch 173/2000 | Train Acc: 99.66% | Test Acc: 98.81% | Loss: 0.0181\n",
      "Epoch 174/2000 | Train Acc: 99.67% | Test Acc: 98.81% | Loss: 0.0179\n",
      "Epoch 175/2000 | Train Acc: 99.68% | Test Acc: 98.84% | Loss: 0.0178\n",
      "Epoch 172/2000 | Train Acc: 99.65% | Test Acc: 98.78% | Loss: 0.0182\n",
      "Epoch 173/2000 | Train Acc: 99.66% | Test Acc: 98.81% | Loss: 0.0181\n",
      "Epoch 174/2000 | Train Acc: 99.67% | Test Acc: 98.81% | Loss: 0.0179\n",
      "Epoch 175/2000 | Train Acc: 99.68% | Test Acc: 98.84% | Loss: 0.0178\n",
      "Epoch 176/2000 | Train Acc: 99.68% | Test Acc: 98.84% | Loss: 0.0176\n",
      "Epoch 177/2000 | Train Acc: 99.68% | Test Acc: 98.86% | Loss: 0.0175\n",
      "Epoch 178/2000 | Train Acc: 99.68% | Test Acc: 98.86% | Loss: 0.0174\n",
      "Epoch 176/2000 | Train Acc: 99.68% | Test Acc: 98.84% | Loss: 0.0176\n",
      "Epoch 177/2000 | Train Acc: 99.68% | Test Acc: 98.86% | Loss: 0.0175\n",
      "Epoch 178/2000 | Train Acc: 99.68% | Test Acc: 98.86% | Loss: 0.0174\n",
      "Epoch 179/2000 | Train Acc: 99.68% | Test Acc: 98.86% | Loss: 0.0172\n",
      "Epoch 180/2000 | Train Acc: 99.68% | Test Acc: 98.86% | Loss: 0.0171\n",
      "Epoch 181/2000 | Train Acc: 99.68% | Test Acc: 98.86% | Loss: 0.0170\n",
      "Epoch 179/2000 | Train Acc: 99.68% | Test Acc: 98.86% | Loss: 0.0172\n",
      "Epoch 180/2000 | Train Acc: 99.68% | Test Acc: 98.86% | Loss: 0.0171\n",
      "Epoch 181/2000 | Train Acc: 99.68% | Test Acc: 98.86% | Loss: 0.0170\n",
      "Epoch 182/2000 | Train Acc: 99.68% | Test Acc: 98.89% | Loss: 0.0169\n",
      "Epoch 183/2000 | Train Acc: 99.68% | Test Acc: 98.92% | Loss: 0.0167\n",
      "Epoch 184/2000 | Train Acc: 99.68% | Test Acc: 98.95% | Loss: 0.0166\n",
      "Epoch 182/2000 | Train Acc: 99.68% | Test Acc: 98.89% | Loss: 0.0169\n",
      "Epoch 183/2000 | Train Acc: 99.68% | Test Acc: 98.92% | Loss: 0.0167\n",
      "Epoch 184/2000 | Train Acc: 99.68% | Test Acc: 98.95% | Loss: 0.0166\n",
      "Epoch 185/2000 | Train Acc: 99.69% | Test Acc: 98.95% | Loss: 0.0165\n",
      "Epoch 186/2000 | Train Acc: 99.69% | Test Acc: 98.95% | Loss: 0.0164\n",
      "Epoch 187/2000 | Train Acc: 99.69% | Test Acc: 98.95% | Loss: 0.0163\n",
      "Epoch 185/2000 | Train Acc: 99.69% | Test Acc: 98.95% | Loss: 0.0165\n",
      "Epoch 186/2000 | Train Acc: 99.69% | Test Acc: 98.95% | Loss: 0.0164\n",
      "Epoch 187/2000 | Train Acc: 99.69% | Test Acc: 98.95% | Loss: 0.0163\n",
      "Epoch 188/2000 | Train Acc: 99.69% | Test Acc: 98.95% | Loss: 0.0162\n",
      "Epoch 189/2000 | Train Acc: 99.69% | Test Acc: 98.95% | Loss: 0.0161\n",
      "Epoch 190/2000 | Train Acc: 99.69% | Test Acc: 98.95% | Loss: 0.0160\n",
      "Epoch 188/2000 | Train Acc: 99.69% | Test Acc: 98.95% | Loss: 0.0162\n",
      "Epoch 189/2000 | Train Acc: 99.69% | Test Acc: 98.95% | Loss: 0.0161\n",
      "Epoch 190/2000 | Train Acc: 99.69% | Test Acc: 98.95% | Loss: 0.0160\n",
      "Epoch 191/2000 | Train Acc: 99.69% | Test Acc: 99.01% | Loss: 0.0158\n",
      "Epoch 192/2000 | Train Acc: 99.69% | Test Acc: 99.01% | Loss: 0.0157\n",
      "Epoch 193/2000 | Train Acc: 99.70% | Test Acc: 99.01% | Loss: 0.0156\n",
      "Epoch 191/2000 | Train Acc: 99.69% | Test Acc: 99.01% | Loss: 0.0158\n",
      "Epoch 192/2000 | Train Acc: 99.69% | Test Acc: 99.01% | Loss: 0.0157\n",
      "Epoch 193/2000 | Train Acc: 99.70% | Test Acc: 99.01% | Loss: 0.0156\n",
      "Epoch 194/2000 | Train Acc: 99.70% | Test Acc: 99.01% | Loss: 0.0155\n",
      "Epoch 195/2000 | Train Acc: 99.71% | Test Acc: 99.01% | Loss: 0.0154\n",
      "Epoch 196/2000 | Train Acc: 99.72% | Test Acc: 99.01% | Loss: 0.0153\n",
      "Epoch 194/2000 | Train Acc: 99.70% | Test Acc: 99.01% | Loss: 0.0155\n",
      "Epoch 195/2000 | Train Acc: 99.71% | Test Acc: 99.01% | Loss: 0.0154\n",
      "Epoch 196/2000 | Train Acc: 99.72% | Test Acc: 99.01% | Loss: 0.0153\n",
      "Epoch 197/2000 | Train Acc: 99.72% | Test Acc: 99.01% | Loss: 0.0153\n",
      "Epoch 198/2000 | Train Acc: 99.74% | Test Acc: 99.01% | Loss: 0.0152\n",
      "Epoch 199/2000 | Train Acc: 99.74% | Test Acc: 99.01% | Loss: 0.0151\n",
      "Epoch 200/2000 | Train Acc: 99.74% | Test Acc: 99.01% | Loss: 0.0150\n",
      "Epoch 197/2000 | Train Acc: 99.72% | Test Acc: 99.01% | Loss: 0.0153\n",
      "Epoch 198/2000 | Train Acc: 99.74% | Test Acc: 99.01% | Loss: 0.0152\n",
      "Epoch 199/2000 | Train Acc: 99.74% | Test Acc: 99.01% | Loss: 0.0151\n",
      "Epoch 200/2000 | Train Acc: 99.74% | Test Acc: 99.01% | Loss: 0.0150\n",
      "Epoch 201/2000 | Train Acc: 99.74% | Test Acc: 99.01% | Loss: 0.0149\n",
      "Epoch 202/2000 | Train Acc: 99.74% | Test Acc: 99.01% | Loss: 0.0148\n",
      "Epoch 203/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0147\n",
      "Epoch 201/2000 | Train Acc: 99.74% | Test Acc: 99.01% | Loss: 0.0149\n",
      "Epoch 202/2000 | Train Acc: 99.74% | Test Acc: 99.01% | Loss: 0.0148\n",
      "Epoch 203/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0147\n",
      "Epoch 204/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0146\n",
      "Epoch 205/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0145\n",
      "Epoch 206/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0144\n",
      "Epoch 204/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0146\n",
      "Epoch 205/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0145\n",
      "Epoch 206/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0144\n",
      "Epoch 207/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0144\n",
      "Epoch 208/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0143\n",
      "Epoch 209/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0142\n",
      "Epoch 207/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0144\n",
      "Epoch 208/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0143\n",
      "Epoch 209/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0142\n",
      "Epoch 210/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0141\n",
      "Epoch 211/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0140\n",
      "Epoch 212/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0140\n",
      "Epoch 210/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0141\n",
      "Epoch 211/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0140\n",
      "Epoch 212/2000 | Train Acc: 99.75% | Test Acc: 99.01% | Loss: 0.0140\n",
      "Epoch 213/2000 | Train Acc: 99.76% | Test Acc: 99.01% | Loss: 0.0139\n",
      "Epoch 214/2000 | Train Acc: 99.76% | Test Acc: 99.01% | Loss: 0.0138\n",
      "Epoch 215/2000 | Train Acc: 99.76% | Test Acc: 99.01% | Loss: 0.0137\n",
      "Epoch 213/2000 | Train Acc: 99.76% | Test Acc: 99.01% | Loss: 0.0139\n",
      "Epoch 214/2000 | Train Acc: 99.76% | Test Acc: 99.01% | Loss: 0.0138\n",
      "Epoch 215/2000 | Train Acc: 99.76% | Test Acc: 99.01% | Loss: 0.0137\n",
      "Epoch 216/2000 | Train Acc: 99.77% | Test Acc: 99.01% | Loss: 0.0137\n",
      "Epoch 217/2000 | Train Acc: 99.77% | Test Acc: 99.01% | Loss: 0.0136\n",
      "Epoch 218/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0135\n",
      "Epoch 216/2000 | Train Acc: 99.77% | Test Acc: 99.01% | Loss: 0.0137\n",
      "Epoch 217/2000 | Train Acc: 99.77% | Test Acc: 99.01% | Loss: 0.0136\n",
      "Epoch 218/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0135\n",
      "Epoch 219/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0135\n",
      "Epoch 220/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0134\n",
      "Epoch 221/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0133\n",
      "Epoch 219/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0135\n",
      "Epoch 220/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0134\n",
      "Epoch 221/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0133\n",
      "Epoch 222/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0132\n",
      "Epoch 223/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0132\n",
      "Epoch 224/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0131\n",
      "Epoch 222/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0132\n",
      "Epoch 223/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0132\n",
      "Epoch 224/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0131\n",
      "Epoch 225/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0130\n",
      "Epoch 226/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0130\n",
      "Epoch 227/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0129\n",
      "Epoch 228/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0129\n",
      "Epoch 225/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0130\n",
      "Epoch 226/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0130\n",
      "Epoch 227/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0129\n",
      "Epoch 228/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0129\n",
      "Epoch 229/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0128\n",
      "Epoch 230/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0127\n",
      "Epoch 231/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0127\n",
      "Epoch 229/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0128\n",
      "Epoch 230/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0127\n",
      "Epoch 231/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0127\n",
      "Epoch 232/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0126\n",
      "Epoch 233/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0125\n",
      "Epoch 234/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0125\n",
      "Epoch 232/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0126\n",
      "Epoch 233/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0125\n",
      "Epoch 234/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0125\n",
      "Epoch 235/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0124\n",
      "Epoch 236/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0124\n",
      "Epoch 237/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0123\n",
      "Epoch 235/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0124\n",
      "Epoch 236/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0124\n",
      "Epoch 237/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0123\n",
      "Epoch 238/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0123\n",
      "Epoch 239/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0122\n",
      "Epoch 240/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0121\n",
      "Epoch 238/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0123\n",
      "Epoch 239/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0122\n",
      "Epoch 240/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0121\n",
      "Epoch 241/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0121\n",
      "Epoch 242/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0120\n",
      "Epoch 243/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0120\n",
      "Epoch 241/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0121\n",
      "Epoch 242/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0120\n",
      "Epoch 243/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0120\n",
      "Epoch 244/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0119\n",
      "Epoch 245/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0119\n",
      "Epoch 246/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0118\n",
      "Epoch 244/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0119\n",
      "Epoch 245/2000 | Train Acc: 99.78% | Test Acc: 99.01% | Loss: 0.0119\n",
      "Epoch 246/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0118\n",
      "Epoch 247/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0118\n",
      "Epoch 248/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0117\n",
      "Epoch 249/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0117\n",
      "Epoch 250/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0116\n",
      "Epoch 247/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0118\n",
      "Epoch 248/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0117\n",
      "Epoch 249/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0117\n",
      "Epoch 250/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0116\n",
      "Epoch 251/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0116\n",
      "Epoch 252/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0115\n",
      "Epoch 253/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0115\n",
      "Epoch 251/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0116\n",
      "Epoch 252/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0115\n",
      "Epoch 253/2000 | Train Acc: 99.78% | Test Acc: 98.98% | Loss: 0.0115\n",
      "Epoch 254/2000 | Train Acc: 99.79% | Test Acc: 98.98% | Loss: 0.0114\n",
      "Epoch 255/2000 | Train Acc: 99.79% | Test Acc: 98.98% | Loss: 0.0114\n",
      "Epoch 256/2000 | Train Acc: 99.79% | Test Acc: 98.98% | Loss: 0.0113\n",
      "Epoch 254/2000 | Train Acc: 99.79% | Test Acc: 98.98% | Loss: 0.0114\n",
      "Epoch 255/2000 | Train Acc: 99.79% | Test Acc: 98.98% | Loss: 0.0114\n",
      "Epoch 256/2000 | Train Acc: 99.79% | Test Acc: 98.98% | Loss: 0.0113\n",
      "Epoch 257/2000 | Train Acc: 99.79% | Test Acc: 98.98% | Loss: 0.0113\n",
      "Epoch 258/2000 | Train Acc: 99.79% | Test Acc: 98.98% | Loss: 0.0112\n",
      "Epoch 259/2000 | Train Acc: 99.79% | Test Acc: 98.98% | Loss: 0.0112\n",
      "Epoch 257/2000 | Train Acc: 99.79% | Test Acc: 98.98% | Loss: 0.0113\n",
      "Epoch 258/2000 | Train Acc: 99.79% | Test Acc: 98.98% | Loss: 0.0112\n",
      "Epoch 259/2000 | Train Acc: 99.79% | Test Acc: 98.98% | Loss: 0.0112\n",
      "Epoch 260/2000 | Train Acc: 99.79% | Test Acc: 98.98% | Loss: 0.0111\n",
      "Epoch 261/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0111\n",
      "Epoch 262/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0110\n",
      "Epoch 260/2000 | Train Acc: 99.79% | Test Acc: 98.98% | Loss: 0.0111\n",
      "Epoch 261/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0111\n",
      "Epoch 262/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0110\n",
      "Epoch 263/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0110\n",
      "Epoch 264/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0110\n",
      "Epoch 265/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0109\n",
      "Epoch 263/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0110\n",
      "Epoch 264/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0110\n",
      "Epoch 265/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0109\n",
      "Epoch 266/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0109\n",
      "Epoch 267/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0108\n",
      "Epoch 268/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0108\n",
      "Epoch 266/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0109\n",
      "Epoch 267/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0108\n",
      "Epoch 268/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0108\n",
      "Epoch 269/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0107\n",
      "Epoch 270/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0107\n",
      "Epoch 271/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0107\n",
      "Epoch 269/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0107\n",
      "Epoch 270/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0107\n",
      "Epoch 271/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0107\n",
      "Epoch 272/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0106\n",
      "Epoch 273/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0106\n",
      "Epoch 274/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0105\n",
      "Epoch 272/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0106\n",
      "Epoch 273/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0106\n",
      "Epoch 274/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0105\n",
      "Epoch 275/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0105\n",
      "Epoch 276/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0105\n",
      "Epoch 277/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0104\n",
      "Epoch 278/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0104\n",
      "Epoch 275/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0105\n",
      "Epoch 276/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0105\n",
      "Epoch 277/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0104\n",
      "Epoch 278/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0104\n",
      "Epoch 279/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0103\n",
      "Epoch 280/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0103\n",
      "Epoch 281/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0103\n",
      "Epoch 279/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0103\n",
      "Epoch 280/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0103\n",
      "Epoch 281/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0103\n",
      "Epoch 282/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0102\n",
      "Epoch 283/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0102\n",
      "Epoch 284/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0101\n",
      "Epoch 282/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0102\n",
      "Epoch 283/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0102\n",
      "Epoch 284/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0101\n",
      "Epoch 285/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0101\n",
      "Epoch 286/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0101\n",
      "Epoch 287/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0100\n",
      "Epoch 285/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0101\n",
      "Epoch 286/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0101\n",
      "Epoch 287/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0100\n",
      "Epoch 288/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0100\n",
      "Epoch 289/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0099\n",
      "Epoch 290/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0099\n",
      "Epoch 288/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0100\n",
      "Epoch 289/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0099\n",
      "Epoch 290/2000 | Train Acc: 99.79% | Test Acc: 99.04% | Loss: 0.0099\n",
      "Epoch 291/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0099\n",
      "Epoch 292/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0098\n",
      "Epoch 293/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0098\n",
      "Epoch 291/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0099\n",
      "Epoch 292/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0098\n",
      "Epoch 293/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0098\n",
      "Epoch 294/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0098\n",
      "Epoch 295/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0097\n",
      "Epoch 296/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0097\n",
      "Epoch 297/2000 | Train Acc: 99.80% | Test Acc: 99.01% | Loss: 0.0097\n",
      "Epoch 294/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0098\n",
      "Epoch 295/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0097\n",
      "Epoch 296/2000 | Train Acc: 99.79% | Test Acc: 99.01% | Loss: 0.0097\n",
      "Epoch 297/2000 | Train Acc: 99.80% | Test Acc: 99.01% | Loss: 0.0097\n",
      "Epoch 298/2000 | Train Acc: 99.80% | Test Acc: 99.01% | Loss: 0.0096\n",
      "Epoch 299/2000 | Train Acc: 99.80% | Test Acc: 99.01% | Loss: 0.0096\n",
      "Epoch 300/2000 | Train Acc: 99.80% | Test Acc: 99.01% | Loss: 0.0096\n",
      "Epoch 298/2000 | Train Acc: 99.80% | Test Acc: 99.01% | Loss: 0.0096\n",
      "Epoch 299/2000 | Train Acc: 99.80% | Test Acc: 99.01% | Loss: 0.0096\n",
      "Epoch 300/2000 | Train Acc: 99.80% | Test Acc: 99.01% | Loss: 0.0096\n",
      "Epoch 301/2000 | Train Acc: 99.80% | Test Acc: 99.01% | Loss: 0.0095\n",
      "Epoch 302/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0095\n",
      "Epoch 303/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0095\n",
      "Epoch 301/2000 | Train Acc: 99.80% | Test Acc: 99.01% | Loss: 0.0095\n",
      "Epoch 302/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0095\n",
      "Epoch 303/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0095\n",
      "Epoch 304/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0094\n",
      "Epoch 305/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0094\n",
      "Epoch 306/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0094\n",
      "Epoch 304/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0094\n",
      "Epoch 305/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0094\n",
      "Epoch 306/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0094\n",
      "Epoch 307/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0093\n",
      "Epoch 308/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0093\n",
      "Epoch 309/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0093\n",
      "Epoch 307/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0093\n",
      "Epoch 308/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0093\n",
      "Epoch 309/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0093\n",
      "Epoch 310/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0092\n",
      "Epoch 311/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0092\n",
      "Epoch 312/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0092\n",
      "Epoch 310/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0092\n",
      "Epoch 311/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0092\n",
      "Epoch 312/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0092\n",
      "Epoch 313/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0091\n",
      "Epoch 314/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0091\n",
      "Epoch 315/2000 | Train Acc: 99.82% | Test Acc: 99.01% | Loss: 0.0091\n",
      "Epoch 316/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0090\n",
      "Epoch 313/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0091\n",
      "Epoch 314/2000 | Train Acc: 99.81% | Test Acc: 99.01% | Loss: 0.0091\n",
      "Epoch 315/2000 | Train Acc: 99.82% | Test Acc: 99.01% | Loss: 0.0091\n",
      "Epoch 316/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0090\n",
      "Epoch 317/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0090\n",
      "Epoch 318/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0090\n",
      "Epoch 319/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0090\n",
      "Epoch 320/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0089\n",
      "Epoch 317/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0090\n",
      "Epoch 318/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0090\n",
      "Epoch 319/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0090\n",
      "Epoch 320/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0089\n",
      "Epoch 321/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0089\n",
      "Epoch 322/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0089\n",
      "Epoch 323/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0088\n",
      "Epoch 324/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0088\n",
      "Epoch 321/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0089\n",
      "Epoch 322/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0089\n",
      "Epoch 323/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0088\n",
      "Epoch 324/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0088\n",
      "Epoch 325/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0088\n",
      "Epoch 326/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0087\n",
      "Epoch 327/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0087\n",
      "Epoch 325/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0088\n",
      "Epoch 326/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0087\n",
      "Epoch 327/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0087\n",
      "Epoch 328/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0087\n",
      "Epoch 329/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0087\n",
      "Epoch 330/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0086\n",
      "Epoch 331/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0086\n",
      "Epoch 328/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0087\n",
      "Epoch 329/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0087\n",
      "Epoch 330/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0086\n",
      "Epoch 331/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0086\n",
      "Epoch 332/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0086\n",
      "Epoch 333/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0086\n",
      "Epoch 334/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0085\n",
      "Epoch 332/2000 | Train Acc: 99.82% | Test Acc: 99.04% | Loss: 0.0086\n",
      "Epoch 333/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0086\n",
      "Epoch 334/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0085\n",
      "Epoch 335/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0085\n",
      "Epoch 336/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0085\n",
      "Epoch 337/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0084\n",
      "Epoch 338/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0084\n",
      "Epoch 335/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0085\n",
      "Epoch 336/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0085\n",
      "Epoch 337/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0084\n",
      "Epoch 338/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0084\n",
      "Epoch 339/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0084\n",
      "Epoch 340/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0084\n",
      "Epoch 341/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0083\n",
      "Epoch 342/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0083\n",
      "Epoch 339/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0084\n",
      "Epoch 340/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0084\n",
      "Epoch 341/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0083\n",
      "Epoch 342/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0083\n",
      "Epoch 343/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0083\n",
      "Epoch 344/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0083\n",
      "Epoch 345/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0082\n",
      "Epoch 343/2000 | Train Acc: 99.82% | Test Acc: 99.06% | Loss: 0.0083\n",
      "Epoch 344/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0083\n",
      "Epoch 345/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0082\n",
      "Epoch 346/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0082\n",
      "Epoch 347/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0082\n",
      "Epoch 348/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0082\n",
      "Epoch 349/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0081\n",
      "Epoch 346/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0082\n",
      "Epoch 347/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0082\n",
      "Epoch 348/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0082\n",
      "Epoch 349/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0081\n",
      "Epoch 350/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0081\n",
      "Epoch 351/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0081\n",
      "Epoch 352/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0081\n",
      "Epoch 350/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0081\n",
      "Epoch 351/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0081\n",
      "Epoch 352/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0081\n",
      "Epoch 353/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0080\n",
      "Epoch 354/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0080\n",
      "Epoch 355/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0080\n",
      "Epoch 353/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0080\n",
      "Epoch 354/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0080\n",
      "Epoch 355/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0080\n",
      "Epoch 356/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0080\n",
      "Epoch 357/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0079\n",
      "Epoch 358/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0079\n",
      "Epoch 356/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0080\n",
      "Epoch 357/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0079\n",
      "Epoch 358/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0079\n",
      "Epoch 359/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0079\n",
      "Epoch 360/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0079\n",
      "Epoch 361/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0078\n",
      "Epoch 359/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0079\n",
      "Epoch 360/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0079\n",
      "Epoch 361/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0078\n",
      "Epoch 362/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0078\n",
      "Epoch 363/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0078\n",
      "Epoch 364/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0078\n",
      "Epoch 362/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0078\n",
      "Epoch 363/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0078\n",
      "Epoch 364/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0078\n",
      "Epoch 365/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0078\n",
      "Epoch 366/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0077\n",
      "Epoch 367/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0077\n",
      "Epoch 368/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0077\n",
      "Epoch 365/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0078\n",
      "Epoch 366/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0077\n",
      "Epoch 367/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0077\n",
      "Epoch 368/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0077\n",
      "Epoch 369/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0077\n",
      "Epoch 370/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0076\n",
      "Epoch 371/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0076\n",
      "Epoch 369/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0077\n",
      "Epoch 370/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0076\n",
      "Epoch 371/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0076\n",
      "Epoch 372/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0076\n",
      "Epoch 373/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0076\n",
      "Epoch 374/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0076\n",
      "Epoch 372/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0076\n",
      "Epoch 373/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0076\n",
      "Epoch 374/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0076\n",
      "Epoch 375/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0075\n",
      "Epoch 376/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0075\n",
      "Epoch 377/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0075\n",
      "Epoch 378/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0075\n",
      "Epoch 375/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0075\n",
      "Epoch 376/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0075\n",
      "Epoch 377/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0075\n",
      "Epoch 378/2000 | Train Acc: 99.83% | Test Acc: 99.06% | Loss: 0.0075\n",
      "Epoch 379/2000 | Train Acc: 99.83% | Test Acc: 99.09% | Loss: 0.0074\n",
      "Epoch 380/2000 | Train Acc: 99.83% | Test Acc: 99.09% | Loss: 0.0074\n",
      "Epoch 381/2000 | Train Acc: 99.83% | Test Acc: 99.09% | Loss: 0.0074\n",
      "Epoch 379/2000 | Train Acc: 99.83% | Test Acc: 99.09% | Loss: 0.0074\n",
      "Epoch 380/2000 | Train Acc: 99.83% | Test Acc: 99.09% | Loss: 0.0074\n",
      "Epoch 381/2000 | Train Acc: 99.83% | Test Acc: 99.09% | Loss: 0.0074\n",
      "Epoch 382/2000 | Train Acc: 99.83% | Test Acc: 99.09% | Loss: 0.0074\n",
      "Epoch 383/2000 | Train Acc: 99.83% | Test Acc: 99.09% | Loss: 0.0074\n",
      "Epoch 384/2000 | Train Acc: 99.83% | Test Acc: 99.09% | Loss: 0.0073\n",
      "Epoch 382/2000 | Train Acc: 99.83% | Test Acc: 99.09% | Loss: 0.0074\n",
      "Epoch 383/2000 | Train Acc: 99.83% | Test Acc: 99.09% | Loss: 0.0074\n",
      "Epoch 384/2000 | Train Acc: 99.83% | Test Acc: 99.09% | Loss: 0.0073\n",
      "Epoch 385/2000 | Train Acc: 99.83% | Test Acc: 99.09% | Loss: 0.0073\n",
      "Epoch 386/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0073\n",
      "Epoch 387/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0073\n",
      "Epoch 385/2000 | Train Acc: 99.83% | Test Acc: 99.09% | Loss: 0.0073\n",
      "Epoch 386/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0073\n",
      "Epoch 387/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0073\n",
      "Epoch 388/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0073\n",
      "Epoch 389/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0072\n",
      "Epoch 390/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0072\n",
      "Epoch 388/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0073\n",
      "Epoch 389/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0072\n",
      "Epoch 390/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0072\n",
      "Epoch 391/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0072\n",
      "Epoch 392/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0072\n",
      "Epoch 393/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0072\n",
      "Epoch 391/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0072\n",
      "Epoch 392/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0072\n",
      "Epoch 393/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0072\n",
      "Epoch 394/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0071\n",
      "Epoch 395/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0071\n",
      "Epoch 396/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0071\n",
      "Epoch 394/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0071\n",
      "Epoch 395/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0071\n",
      "Epoch 396/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0071\n",
      "Epoch 397/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0071\n",
      "Epoch 398/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0071\n",
      "Epoch 399/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0070\n",
      "Epoch 397/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0071\n",
      "Epoch 398/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0071\n",
      "Epoch 399/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0070\n",
      "Epoch 400/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0070\n",
      "Epoch 401/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0070\n",
      "Epoch 402/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0070\n",
      "Epoch 400/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0070\n",
      "Epoch 401/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0070\n",
      "Epoch 402/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0070\n",
      "Epoch 403/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0070\n",
      "Epoch 404/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0070\n",
      "Epoch 405/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0069\n",
      "Epoch 403/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0070\n",
      "Epoch 404/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0070\n",
      "Epoch 405/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0069\n",
      "Epoch 406/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0069\n",
      "Epoch 407/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0069\n",
      "Epoch 408/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0069\n",
      "Epoch 406/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0069\n",
      "Epoch 407/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0069\n",
      "Epoch 408/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0069\n",
      "Epoch 409/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0069\n",
      "Epoch 410/2000 | Train Acc: 99.85% | Test Acc: 99.09% | Loss: 0.0068\n",
      "Epoch 411/2000 | Train Acc: 99.85% | Test Acc: 99.09% | Loss: 0.0068\n",
      "Epoch 409/2000 | Train Acc: 99.84% | Test Acc: 99.09% | Loss: 0.0069\n",
      "Epoch 410/2000 | Train Acc: 99.85% | Test Acc: 99.09% | Loss: 0.0068\n",
      "Epoch 411/2000 | Train Acc: 99.85% | Test Acc: 99.09% | Loss: 0.0068\n",
      "Epoch 412/2000 | Train Acc: 99.85% | Test Acc: 99.09% | Loss: 0.0068\n",
      "Epoch 413/2000 | Train Acc: 99.85% | Test Acc: 99.09% | Loss: 0.0068\n",
      "Epoch 414/2000 | Train Acc: 99.85% | Test Acc: 99.09% | Loss: 0.0068\n",
      "Epoch 412/2000 | Train Acc: 99.85% | Test Acc: 99.09% | Loss: 0.0068\n",
      "Epoch 413/2000 | Train Acc: 99.85% | Test Acc: 99.09% | Loss: 0.0068\n",
      "Epoch 414/2000 | Train Acc: 99.85% | Test Acc: 99.09% | Loss: 0.0068\n",
      "Epoch 415/2000 | Train Acc: 99.85% | Test Acc: 99.09% | Loss: 0.0068\n",
      "Epoch 416/2000 | Train Acc: 99.85% | Test Acc: 99.09% | Loss: 0.0067\n",
      "Epoch 417/2000 | Train Acc: 99.86% | Test Acc: 99.12% | Loss: 0.0067\n",
      "Epoch 415/2000 | Train Acc: 99.85% | Test Acc: 99.09% | Loss: 0.0068\n",
      "Epoch 416/2000 | Train Acc: 99.85% | Test Acc: 99.09% | Loss: 0.0067\n",
      "Epoch 417/2000 | Train Acc: 99.86% | Test Acc: 99.12% | Loss: 0.0067\n",
      "Epoch 418/2000 | Train Acc: 99.86% | Test Acc: 99.12% | Loss: 0.0067\n",
      "Epoch 419/2000 | Train Acc: 99.86% | Test Acc: 99.12% | Loss: 0.0067\n",
      "Epoch 420/2000 | Train Acc: 99.86% | Test Acc: 99.12% | Loss: 0.0067\n",
      "Epoch 418/2000 | Train Acc: 99.86% | Test Acc: 99.12% | Loss: 0.0067\n",
      "Epoch 419/2000 | Train Acc: 99.86% | Test Acc: 99.12% | Loss: 0.0067\n",
      "Epoch 420/2000 | Train Acc: 99.86% | Test Acc: 99.12% | Loss: 0.0067\n",
      "Epoch 421/2000 | Train Acc: 99.86% | Test Acc: 99.12% | Loss: 0.0067\n",
      "Epoch 422/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0066\n",
      "Epoch 423/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0066\n",
      "Epoch 421/2000 | Train Acc: 99.86% | Test Acc: 99.12% | Loss: 0.0067\n",
      "Epoch 422/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0066\n",
      "Epoch 423/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0066\n",
      "Epoch 424/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0066\n",
      "Epoch 425/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0066\n",
      "Epoch 426/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0066\n",
      "Epoch 424/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0066\n",
      "Epoch 425/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0066\n",
      "Epoch 426/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0066\n",
      "Epoch 427/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0066\n",
      "Epoch 428/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0065\n",
      "Epoch 429/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0065\n",
      "Epoch 427/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0066\n",
      "Epoch 428/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0065\n",
      "Epoch 429/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0065\n",
      "Epoch 430/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0065\n",
      "Epoch 431/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0065\n",
      "Epoch 432/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0065\n",
      "Epoch 430/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0065\n",
      "Epoch 431/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0065\n",
      "Epoch 432/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0065\n",
      "Epoch 433/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0065\n",
      "Epoch 434/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0064\n",
      "Epoch 435/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0064\n",
      "Epoch 433/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0065\n",
      "Epoch 434/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0064\n",
      "Epoch 435/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0064\n",
      "Epoch 436/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0064\n",
      "Epoch 437/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0064\n",
      "Epoch 438/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0064\n",
      "Epoch 436/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0064\n",
      "Epoch 437/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0064\n",
      "Epoch 438/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0064\n",
      "Epoch 439/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0064\n",
      "Epoch 440/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0064\n",
      "Epoch 441/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0063\n",
      "Epoch 439/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0064\n",
      "Epoch 440/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0064\n",
      "Epoch 441/2000 | Train Acc: 99.86% | Test Acc: 99.09% | Loss: 0.0063\n",
      "Epoch 442/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0063\n",
      "Epoch 443/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0063\n",
      "Epoch 444/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0063\n",
      "Epoch 442/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0063\n",
      "Epoch 443/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0063\n",
      "Epoch 444/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0063\n",
      "Epoch 445/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0063\n",
      "Epoch 446/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0063\n",
      "Epoch 447/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0062\n",
      "Epoch 445/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0063\n",
      "Epoch 446/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0063\n",
      "Epoch 447/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0062\n",
      "Epoch 448/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0062\n",
      "Epoch 449/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0062\n",
      "Epoch 450/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0062\n",
      "Epoch 448/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0062\n",
      "Epoch 449/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0062\n",
      "Epoch 450/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0062\n",
      "Epoch 451/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0062\n",
      "Epoch 452/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0062\n",
      "Epoch 453/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0062\n",
      "Epoch 451/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0062\n",
      "Epoch 452/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0062\n",
      "Epoch 453/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0062\n",
      "Epoch 454/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0061\n",
      "Epoch 455/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0061\n",
      "Epoch 456/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0061\n",
      "Epoch 454/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0061\n",
      "Epoch 455/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0061\n",
      "Epoch 456/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0061\n",
      "Epoch 457/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0061\n",
      "Epoch 458/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0061\n",
      "Epoch 459/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0061\n",
      "Epoch 457/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0061\n",
      "Epoch 458/2000 | Train Acc: 99.87% | Test Acc: 99.09% | Loss: 0.0061\n",
      "Epoch 459/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0061\n",
      "Epoch 460/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0061\n",
      "Epoch 461/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 462/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 460/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0061\n",
      "Epoch 461/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 462/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 463/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 464/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 465/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 463/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 464/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 465/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 466/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 467/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 468/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 466/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 467/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 468/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0060\n",
      "Epoch 469/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 470/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 471/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 469/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 470/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 471/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 472/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 473/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 474/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 472/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 473/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 474/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 475/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 476/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 477/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 475/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 476/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0059\n",
      "Epoch 477/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 478/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 479/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 480/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 478/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 479/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 480/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 481/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 482/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 483/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 481/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 482/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 483/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 484/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 485/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 486/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 484/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0058\n",
      "Epoch 485/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 486/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 487/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 488/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 489/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 487/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 488/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 489/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 490/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 491/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 492/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 490/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 491/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 492/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0057\n",
      "Epoch 493/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 494/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 495/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 493/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 494/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 495/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 496/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 497/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 498/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 496/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 497/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 498/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 499/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 500/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 501/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 499/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 500/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 501/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0056\n",
      "Epoch 502/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 503/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 504/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 502/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 503/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 504/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 505/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 506/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 507/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 505/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 506/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 507/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 508/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 509/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 510/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 508/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 509/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 510/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0055\n",
      "Epoch 511/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0054\n",
      "Epoch 512/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0054\n",
      "Epoch 513/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0054\n",
      "Epoch 511/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0054\n",
      "Epoch 512/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0054\n",
      "Epoch 513/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0054\n",
      "Epoch 514/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0054\n",
      "Epoch 515/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0054\n",
      "Epoch 516/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0054\n",
      "Epoch 514/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0054\n",
      "Epoch 515/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0054\n",
      "Epoch 516/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0054\n",
      "Epoch 517/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0054\n",
      "Epoch 518/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0054\n",
      "Epoch 519/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0054\n",
      "Epoch 517/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0054\n",
      "Epoch 518/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0054\n",
      "Epoch 519/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0054\n",
      "Epoch 520/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0053\n",
      "Epoch 521/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0053\n",
      "Epoch 522/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0053\n",
      "Epoch 520/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0053\n",
      "Epoch 521/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0053\n",
      "Epoch 522/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0053\n",
      "Epoch 523/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0053\n",
      "Epoch 524/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0053\n",
      "Epoch 525/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0053\n",
      "Epoch 523/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0053\n",
      "Epoch 524/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0053\n",
      "Epoch 525/2000 | Train Acc: 99.87% | Test Acc: 99.15% | Loss: 0.0053\n",
      "Epoch 526/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0053\n",
      "Epoch 527/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0053\n",
      "Epoch 528/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0053\n",
      "Epoch 526/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0053\n",
      "Epoch 527/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0053\n",
      "Epoch 528/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0053\n",
      "Epoch 529/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0053\n",
      "Epoch 530/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 531/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 529/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0053\n",
      "Epoch 530/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 531/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 532/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 533/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 534/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 532/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 533/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 534/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 535/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 536/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 537/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 535/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 536/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 537/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 538/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 539/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 540/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 538/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 539/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0052\n",
      "Epoch 540/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 541/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 542/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 543/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 541/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 542/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 543/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 544/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 545/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 546/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 544/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 545/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 546/2000 | Train Acc: 99.87% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 547/2000 | Train Acc: 99.88% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 548/2000 | Train Acc: 99.88% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 549/2000 | Train Acc: 99.88% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 547/2000 | Train Acc: 99.88% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 548/2000 | Train Acc: 99.88% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 549/2000 | Train Acc: 99.88% | Test Acc: 99.12% | Loss: 0.0051\n",
      "Epoch 550/2000 | Train Acc: 99.88% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 551/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 552/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 550/2000 | Train Acc: 99.88% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 551/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 552/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 553/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 554/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 555/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 553/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 554/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 555/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 556/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 557/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 558/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 556/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 557/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 558/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 559/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 560/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 561/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 559/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 560/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0050\n",
      "Epoch 561/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 562/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 563/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 564/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 562/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 563/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 564/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 565/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 566/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 567/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 565/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 566/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 567/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 568/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 569/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 570/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 568/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 569/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 570/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 571/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 572/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 573/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 571/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0049\n",
      "Epoch 572/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 573/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 574/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 575/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 576/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 574/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 575/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 576/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 577/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 578/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 579/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 577/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 578/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 579/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 580/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 581/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 582/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0048\n",
      "Epoch 580/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 581/2000 | Train Acc: 99.89% | Test Acc: 99.12% | Loss: 0.0048\n",
      "Epoch 582/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0048\n",
      "Epoch 583/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0048\n",
      "Epoch 584/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 585/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 583/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0048\n",
      "Epoch 584/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 585/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 586/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 587/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 588/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 586/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 587/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 588/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 589/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 590/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 591/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 589/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 590/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 591/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 592/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 593/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 594/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 592/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 593/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 594/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 595/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 596/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 597/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 595/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0047\n",
      "Epoch 596/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 597/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 598/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 599/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 600/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 601/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 598/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 599/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 600/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 601/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 602/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 603/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 604/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 602/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 603/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 604/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 605/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 606/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 607/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 608/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 605/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 606/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 607/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0046\n",
      "Epoch 608/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 609/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 610/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 611/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 609/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 610/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 611/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 612/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 613/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 614/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 612/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 613/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 614/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 615/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 616/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 617/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 618/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 615/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 616/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 617/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 618/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 619/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 620/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 621/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 619/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 620/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0045\n",
      "Epoch 621/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 622/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 623/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 624/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 622/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 623/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 624/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 625/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 626/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 627/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 625/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 626/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 627/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 628/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 629/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 630/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 628/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 629/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 630/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 631/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 632/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 633/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 631/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 632/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 633/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 634/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 635/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 636/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 634/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0044\n",
      "Epoch 635/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 636/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 637/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 638/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 639/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 640/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 637/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 638/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 639/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 640/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 641/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 642/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 643/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 641/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 642/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 643/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 644/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 645/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 646/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 644/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 645/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 646/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 647/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 648/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 649/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0042\n",
      "Epoch 647/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 648/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0043\n",
      "Epoch 649/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0042\n",
      "Epoch 650/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0042\n",
      "Epoch 651/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0042\n",
      "Epoch 652/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0042\n",
      "Epoch 653/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0042\n",
      "Epoch 650/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0042\n",
      "Epoch 651/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0042\n",
      "Epoch 652/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0042\n",
      "Epoch 653/2000 | Train Acc: 99.89% | Test Acc: 99.09% | Loss: 0.0042\n",
      "Epoch 654/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 655/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 656/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 654/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 655/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 656/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 657/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 658/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 659/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 660/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 657/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 658/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 659/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 660/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 661/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 662/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 663/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 664/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 661/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 662/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0042\n",
      "Epoch 663/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 664/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 665/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 666/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 667/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 665/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 666/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 667/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 668/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 669/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 670/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 668/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 669/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 670/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 671/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 672/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 673/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 671/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 672/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 673/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 674/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 675/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 676/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 677/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 674/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 675/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 676/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 677/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0041\n",
      "Epoch 678/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 679/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 680/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 681/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 678/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 679/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 680/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 681/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 682/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 683/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 684/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 685/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 682/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 683/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 684/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 685/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 686/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 687/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 688/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 689/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 686/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 687/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 688/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 689/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 690/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 691/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0040\n",
      "Epoch 692/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0040\n",
      "Epoch 693/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0040\n",
      "Epoch 690/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0040\n",
      "Epoch 691/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0040\n",
      "Epoch 692/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0040\n",
      "Epoch 693/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0040\n",
      "Epoch 694/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 695/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 696/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 697/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 694/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 695/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 696/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 697/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 698/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 699/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 700/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0039\n",
      "Epoch 698/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 699/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 700/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0039\n",
      "Epoch 701/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 702/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0039\n",
      "Epoch 703/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 704/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0039\n",
      "Epoch 701/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 702/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0039\n",
      "Epoch 703/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 704/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0039\n",
      "Epoch 705/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 706/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0039\n",
      "Epoch 707/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0039\n",
      "Epoch 708/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0039\n",
      "Epoch 705/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0039\n",
      "Epoch 706/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0039\n",
      "Epoch 707/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0039\n",
      "Epoch 708/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0039\n",
      "Epoch 709/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0039\n",
      "Epoch 710/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0038\n",
      "Epoch 711/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0038\n",
      "Epoch 712/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0038\n",
      "Epoch 709/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0039\n",
      "Epoch 710/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0038\n",
      "Epoch 711/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0038\n",
      "Epoch 712/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0038\n",
      "Epoch 713/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0038\n",
      "Epoch 714/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0038\n",
      "Epoch 715/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 716/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 713/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0038\n",
      "Epoch 714/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0038\n",
      "Epoch 715/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 716/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 717/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 718/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 719/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 720/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 717/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 718/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 719/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 720/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 721/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 722/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0038\n",
      "Epoch 723/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 724/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 721/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 722/2000 | Train Acc: 99.89% | Test Acc: 99.06% | Loss: 0.0038\n",
      "Epoch 723/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 724/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 725/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 726/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 727/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 725/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 726/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0038\n",
      "Epoch 727/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 728/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 729/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 730/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 728/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 729/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 730/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 731/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 732/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 733/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 731/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 732/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 733/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 734/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 735/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 736/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 734/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 735/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 736/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 737/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 738/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 739/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 740/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 737/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 738/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 739/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 740/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 741/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 742/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 743/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 744/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 741/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 742/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 743/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0037\n",
      "Epoch 744/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 745/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 746/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 747/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 748/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 745/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 746/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 747/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 748/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 749/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 750/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 751/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 752/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 749/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 750/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 751/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 752/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 753/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 754/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 755/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 756/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 753/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 754/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 755/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 756/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 757/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 758/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 759/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 760/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 757/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 758/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 759/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 760/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 761/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 762/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 763/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 764/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 761/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0036\n",
      "Epoch 762/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 763/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 764/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 765/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 766/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 767/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 768/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 765/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 766/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 767/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 768/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 769/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 770/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 771/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 772/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 769/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 770/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 771/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 772/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 773/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 774/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 775/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 776/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 773/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 774/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 775/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 776/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 777/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 778/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 779/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 777/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 778/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 779/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 780/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 781/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 782/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 780/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0035\n",
      "Epoch 781/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 782/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 783/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 784/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 785/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 783/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 784/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 785/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 786/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 787/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 788/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 789/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 786/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 787/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 788/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 789/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 790/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 791/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 792/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 790/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 791/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 792/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 793/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 794/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 795/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 793/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 794/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 795/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 796/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 797/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 798/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 796/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 797/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 798/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 799/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 800/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 801/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 799/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0034\n",
      "Epoch 800/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 801/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 802/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 803/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 804/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 802/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 803/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 804/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 805/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 806/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 807/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 805/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 806/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 807/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 808/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 809/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 810/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 811/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 808/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 809/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 810/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 811/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 812/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 813/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 814/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 815/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 812/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 813/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 814/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 815/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 816/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 817/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 818/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 816/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 817/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 818/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 819/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 820/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 821/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 822/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 819/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0033\n",
      "Epoch 820/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 821/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 822/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 823/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 824/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 825/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 823/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 824/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 825/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 826/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 827/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 828/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 829/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 826/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 827/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 828/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 829/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 830/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 831/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 832/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 833/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 830/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 831/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 832/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 833/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 834/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 835/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 836/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 837/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 834/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 835/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 836/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 837/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 838/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 839/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 840/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 841/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 838/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 839/2000 | Train Acc: 99.89% | Test Acc: 99.04% | Loss: 0.0032\n",
      "Epoch 840/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 841/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 842/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 843/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 844/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 842/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 843/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 844/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 845/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 846/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 847/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 848/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 845/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 846/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 847/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 848/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 849/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 850/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 851/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 849/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 850/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 851/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 852/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 853/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 854/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 852/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 853/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 854/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 855/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 856/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 857/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 858/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 855/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 856/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 857/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 858/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 859/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 860/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 861/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 862/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 859/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 860/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 861/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0031\n",
      "Epoch 862/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 863/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 864/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 865/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 866/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 863/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 864/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 865/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 866/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 867/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 868/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 869/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 870/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 867/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 868/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 869/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 870/2000 | Train Acc: 99.90% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 871/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 872/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 873/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 874/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 871/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 872/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 873/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 874/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 875/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 876/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 877/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 878/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 875/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 876/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 877/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 878/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 879/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 880/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 881/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 882/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 879/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 880/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 881/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 882/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 883/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 884/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 885/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 886/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 883/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0030\n",
      "Epoch 884/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 885/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 886/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 887/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 888/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 889/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 887/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 888/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 889/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 890/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 891/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 892/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 893/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 890/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 891/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 892/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 893/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 894/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 895/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 896/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 897/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 894/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 895/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 896/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 897/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 898/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 899/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 900/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 898/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 899/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 900/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 901/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 902/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 903/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 901/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 902/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 903/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 904/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 905/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 906/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 904/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 905/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 906/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 907/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 908/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 909/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 907/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0029\n",
      "Epoch 908/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 909/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 910/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 911/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 912/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 913/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 910/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 911/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 912/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 913/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 914/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 915/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 916/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 917/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 914/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 915/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 916/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 917/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 918/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 919/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 920/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 921/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 918/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 919/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 920/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 921/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 922/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 923/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 924/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 922/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 923/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 924/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 925/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 926/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 927/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 925/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 926/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 927/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 928/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 929/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 930/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 928/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 929/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 930/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 931/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 932/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 933/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 934/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 931/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0028\n",
      "Epoch 932/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 933/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 934/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 935/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 936/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 937/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 938/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 935/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 936/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 937/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 938/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 939/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 940/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 941/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 942/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 939/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 940/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 941/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 942/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 943/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 944/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 945/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 943/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 944/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 945/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 946/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 947/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 948/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 949/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 946/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 947/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 948/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 949/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 950/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 951/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 952/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 950/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 951/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 952/2000 | Train Acc: 99.91% | Test Acc: 99.04% | Loss: 0.0027\n",
      "Epoch 953/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0027\n",
      "Epoch 954/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0027\n",
      "Epoch 955/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0027\n",
      "Epoch 953/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0027\n",
      "Epoch 954/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0027\n",
      "Epoch 955/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0027\n",
      "Epoch 956/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0027\n",
      "Epoch 957/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0027\n",
      "Epoch 958/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 956/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0027\n",
      "Epoch 957/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0027\n",
      "Epoch 958/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 959/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 960/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 961/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 962/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 959/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 960/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 961/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 962/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 963/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 964/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 965/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 966/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 963/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 964/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 965/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 966/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 967/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 968/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 969/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 970/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 967/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 968/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 969/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 970/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 971/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 972/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 973/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 971/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 972/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 973/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 974/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 975/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 976/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 977/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 974/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 975/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 976/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 977/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 978/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 979/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 980/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 981/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 978/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 979/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 980/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 981/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 982/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 983/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 984/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 985/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 982/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 983/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 984/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0026\n",
      "Epoch 985/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 986/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 987/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 988/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 986/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 987/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 988/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 989/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 990/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 991/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 992/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 989/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 990/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 991/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 992/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 993/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 994/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 995/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 993/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 994/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 995/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 996/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 997/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 998/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 999/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 996/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 997/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 998/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 999/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1000/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1001/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1002/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1000/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1001/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1002/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1003/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1004/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1005/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1003/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1004/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1005/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1006/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1007/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1008/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1006/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1007/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1008/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1009/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1010/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1011/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1009/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1010/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1011/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1012/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1013/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1014/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1015/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1012/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1013/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0025\n",
      "Epoch 1014/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1015/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1016/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1017/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1018/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1019/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1016/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1017/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1018/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1019/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1020/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1021/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1022/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1020/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1021/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1022/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1023/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1024/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1025/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1023/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1024/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1025/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1026/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1027/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1028/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1026/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1027/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1028/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1029/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1030/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1031/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1029/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1030/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1031/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1032/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1033/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1034/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1032/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1033/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1034/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1035/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1036/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1037/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1035/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1036/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1037/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1038/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1039/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1040/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1038/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1039/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1040/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1041/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1042/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1043/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1041/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1042/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0024\n",
      "Epoch 1043/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1044/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1045/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1046/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1047/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1044/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1045/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1046/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1047/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1048/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1049/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1050/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1051/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1048/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1049/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1050/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1051/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1052/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1053/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1054/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1055/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1052/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1053/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1054/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1055/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1056/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1057/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1058/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1059/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1056/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1057/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1058/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1059/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1060/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1061/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1062/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1060/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1061/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1062/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1063/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1064/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1065/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1063/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1064/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1065/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1066/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1067/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1068/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1069/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1066/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1067/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1068/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1069/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1070/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1071/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1072/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1073/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1070/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1071/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1072/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1073/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1074/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1075/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1076/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1074/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0023\n",
      "Epoch 1075/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1076/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1077/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1078/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1079/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1077/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1078/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1079/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1080/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1081/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1082/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1080/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1081/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1082/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1083/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1084/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1085/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1083/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1084/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1085/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1086/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1087/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1088/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1086/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1087/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1088/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1089/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1090/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1091/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1092/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1089/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1090/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1091/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1092/2000 | Train Acc: 99.91% | Test Acc: 99.09% | Loss: 0.0022\n",
      "Epoch 1093/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1094/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1095/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1096/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1093/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1094/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1095/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1096/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1097/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1098/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1099/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1100/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1097/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1098/2000 | Train Acc: 99.91% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1099/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1100/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1101/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1102/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1103/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1101/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1102/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1103/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1104/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1105/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1106/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1107/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1104/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1105/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1106/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1107/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0022\n",
      "Epoch 1108/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1109/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1110/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1111/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1108/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1109/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1110/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1111/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1112/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1113/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1114/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1112/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1113/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1114/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1115/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1116/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1117/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1118/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1115/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1116/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1117/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1118/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1119/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1120/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1121/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1119/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1120/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1121/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1122/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1123/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1124/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1122/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1123/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1124/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1125/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1126/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1127/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1128/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1125/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1126/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1127/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1128/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1129/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1130/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1131/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1132/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1129/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1130/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1131/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1132/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1133/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1134/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1135/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1136/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1133/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1134/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1135/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1136/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1137/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1138/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1139/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1137/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1138/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1139/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1140/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1141/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1142/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1140/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1141/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1142/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0021\n",
      "Epoch 1143/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1144/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1145/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1143/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1144/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1145/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1146/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1147/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1148/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1146/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1147/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1148/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1149/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1150/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1151/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1152/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1149/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1150/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1151/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1152/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1153/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1154/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1155/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1153/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1154/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1155/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1156/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1157/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1158/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1159/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1156/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1157/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1158/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1159/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1160/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1161/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1162/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1163/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1160/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1161/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1162/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1163/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1164/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1165/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1166/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1164/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1165/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1166/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1167/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1168/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1169/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1167/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1168/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1169/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1170/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1171/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1172/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1173/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1170/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1171/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1172/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1173/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1174/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1175/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1176/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1174/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1175/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1176/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1177/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1178/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1179/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1177/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1178/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0020\n",
      "Epoch 1179/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1180/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1181/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1182/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1180/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1181/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1182/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1183/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1184/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1185/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1183/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1184/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1185/2000 | Train Acc: 99.92% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1186/2000 | Train Acc: 99.93% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1187/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1188/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1186/2000 | Train Acc: 99.93% | Test Acc: 99.06% | Loss: 0.0019\n",
      "Epoch 1187/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1188/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1189/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1190/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1191/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1189/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1190/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1191/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1192/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1193/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1194/2000 | Train Acc: 99.93% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1192/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1193/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1194/2000 | Train Acc: 99.93% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1195/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1196/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1197/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1195/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1196/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1197/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1198/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1199/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1200/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1201/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1198/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1199/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1200/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1201/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1202/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1203/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1204/2000 | Train Acc: 99.93% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1205/2000 | Train Acc: 99.93% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1202/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1203/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1204/2000 | Train Acc: 99.93% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1205/2000 | Train Acc: 99.93% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1206/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1207/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1208/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1209/2000 | Train Acc: 99.93% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1206/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1207/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1208/2000 | Train Acc: 99.92% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1209/2000 | Train Acc: 99.93% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1210/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1211/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1212/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1213/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1210/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1211/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1212/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1213/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1214/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1215/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1216/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1217/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1214/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1215/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1216/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0019\n",
      "Epoch 1217/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1218/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1219/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1220/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1218/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1219/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1220/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1221/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1222/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1223/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1221/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1222/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1223/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1224/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1225/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1226/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1227/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1224/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1225/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1226/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1227/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1228/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1229/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1230/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1228/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1229/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1230/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1231/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1232/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1233/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1231/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1232/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1233/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1234/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1235/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1236/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1234/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1235/2000 | Train Acc: 99.94% | Test Acc: 99.09% | Loss: 0.0018\n",
      "Epoch 1236/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1237/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1238/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1239/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1240/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1237/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1238/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1239/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1240/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1241/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1242/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1243/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1244/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1241/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1242/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1243/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1244/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1245/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1246/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1247/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1248/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1245/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1246/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1247/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1248/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1249/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1250/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1251/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1252/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1249/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1250/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1251/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1252/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1253/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1254/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1255/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1253/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1254/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1255/2000 | Train Acc: 99.94% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1256/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1257/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1258/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1256/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1257/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0018\n",
      "Epoch 1258/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1259/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1260/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1261/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1262/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1259/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1260/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1261/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1262/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1263/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1264/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1265/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1263/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1264/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1265/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1266/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1267/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1268/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1266/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1267/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1268/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1269/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1270/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1271/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1269/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1270/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1271/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1272/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1273/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1274/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1272/2000 | Train Acc: 99.95% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1273/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1274/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1275/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1276/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1277/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1278/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1275/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1276/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1277/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1278/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1279/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1280/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1281/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1282/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1279/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1280/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1281/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1282/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1283/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1284/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1285/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1283/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1284/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1285/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1286/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1287/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1288/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1286/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1287/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1288/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1289/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1290/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1291/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1289/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1290/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1291/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1292/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1293/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1294/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1292/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1293/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1294/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1295/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1296/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1297/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1295/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1296/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1297/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1298/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1299/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1300/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1298/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1299/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1300/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0017\n",
      "Epoch 1301/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1302/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1303/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1304/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1301/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1302/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1303/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1304/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1305/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1306/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1307/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1305/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1306/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1307/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1308/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1309/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1310/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1308/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1309/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1310/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1311/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1312/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1313/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1311/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1312/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1313/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1314/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1315/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1316/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1314/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1315/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1316/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1317/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1318/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1319/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1317/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1318/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1319/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1320/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1321/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1322/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1320/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1321/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1322/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1323/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1324/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1325/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1323/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1324/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1325/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1326/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1327/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1328/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1329/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1326/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1327/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1328/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1329/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1330/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1331/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1332/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1330/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1331/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1332/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1333/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1334/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1335/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1333/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1334/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1335/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1336/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1337/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1338/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1336/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1337/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1338/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1339/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1340/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1341/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1339/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1340/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1341/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1342/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1343/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1344/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1345/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1342/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1343/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1344/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1345/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1346/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1347/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1348/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1346/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1347/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0016\n",
      "Epoch 1348/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1349/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1350/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1351/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1349/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1350/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1351/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1352/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1353/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1354/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1352/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1353/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1354/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1355/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1356/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1357/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1355/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1356/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1357/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1358/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1359/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1360/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1358/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1359/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1360/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1361/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1362/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1363/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1361/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1362/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1363/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1364/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1365/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1366/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1364/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1365/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1366/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1367/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1368/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1369/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1370/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1367/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1368/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1369/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1370/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1371/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1372/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1373/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1371/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1372/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1373/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1374/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1375/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1376/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1374/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1375/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1376/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1377/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1378/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1379/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1377/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1378/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1379/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1380/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1381/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1382/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1383/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1380/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1381/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1382/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1383/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1384/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1385/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1386/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1384/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1385/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1386/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1387/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1388/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1389/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1387/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1388/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1389/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1390/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1391/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1392/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1393/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1390/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1391/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1392/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1393/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1394/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1395/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1396/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1394/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1395/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1396/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1397/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1398/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1399/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1397/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1398/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0015\n",
      "Epoch 1399/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1400/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1401/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1402/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1400/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1401/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1402/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1403/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1404/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1405/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1403/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1404/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1405/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1406/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1407/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1408/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1406/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1407/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1408/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1409/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1410/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1411/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1409/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1410/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1411/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1412/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1413/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1414/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1412/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1413/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1414/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1415/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1416/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1417/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1415/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1416/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1417/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1418/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1419/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1420/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1418/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1419/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1420/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1421/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1422/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1423/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1421/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1422/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1423/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1424/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1425/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1426/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1424/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1425/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1426/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1427/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1428/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1429/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1427/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1428/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1429/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1430/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1431/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1432/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1430/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1431/2000 | Train Acc: 99.96% | Test Acc: 99.06% | Loss: 0.0014\n",
      "Epoch 1432/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1433/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1434/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1435/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1433/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1434/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1435/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1436/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1437/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1438/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1436/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1437/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1438/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1439/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1440/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1441/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1439/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1440/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1441/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1442/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1443/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1444/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1442/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1443/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1444/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1445/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1446/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1447/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1448/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1445/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1446/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1447/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1448/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1449/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1450/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1451/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1449/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1450/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1451/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1452/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1453/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1454/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1452/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1453/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0014\n",
      "Epoch 1454/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1455/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1456/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1457/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1455/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1456/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1457/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1458/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1459/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1460/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1458/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1459/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1460/2000 | Train Acc: 99.96% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1461/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1462/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1463/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1461/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1462/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1463/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1464/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1465/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1466/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1464/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1465/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1466/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1467/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1468/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1469/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1467/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1468/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1469/2000 | Train Acc: 99.97% | Test Acc: 99.09% | Loss: 0.0013\n",
      "Epoch 1470/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1471/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1472/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1470/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1471/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1472/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1473/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1474/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1475/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1473/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1474/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1475/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1476/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1477/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1478/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1476/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1477/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1478/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1479/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1480/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1481/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1479/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1480/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1481/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1482/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1483/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1484/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1482/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1483/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1484/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1485/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1486/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1487/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1485/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1486/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1487/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1488/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1489/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1490/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1488/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1489/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1490/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1491/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1492/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1493/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1491/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1492/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1493/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1494/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1495/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1496/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1494/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1495/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1496/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1497/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1498/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1499/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1497/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1498/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1499/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1500/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1501/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1502/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1500/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1501/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1502/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1503/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1504/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1505/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1503/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1504/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1505/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1506/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1507/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1508/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1506/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1507/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1508/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1509/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1510/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1511/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1509/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1510/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0013\n",
      "Epoch 1511/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1512/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1513/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1514/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1512/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1513/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1514/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1515/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1516/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1517/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1515/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1516/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1517/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1518/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1519/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1520/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1518/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1519/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1520/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1521/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1522/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1523/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1521/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1522/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1523/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1524/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1525/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1526/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1524/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1525/2000 | Train Acc: 99.97% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1526/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1527/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1528/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1529/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1530/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1527/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1528/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1529/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1530/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1531/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1532/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1533/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1531/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1532/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1533/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1534/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1535/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1536/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1534/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1535/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1536/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1537/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1538/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1539/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1537/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1538/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1539/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1540/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1541/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1542/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1540/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1541/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1542/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1543/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1544/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1545/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1543/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1544/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1545/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1546/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1547/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1548/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1546/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1547/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1548/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1549/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1550/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1551/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1549/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1550/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1551/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1552/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1553/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1554/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1552/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1553/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1554/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1555/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1556/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1557/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1555/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1556/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1557/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1558/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1559/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1560/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1558/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1559/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1560/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1561/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1562/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1563/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1561/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1562/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1563/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1564/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1565/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1566/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1564/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1565/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1566/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1567/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1568/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1569/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1567/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1568/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1569/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1570/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1571/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1572/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1570/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1571/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1572/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1573/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1574/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1575/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1573/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1574/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0012\n",
      "Epoch 1575/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1576/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1577/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1578/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1576/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1577/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1578/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1579/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1580/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1581/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1579/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1580/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1581/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1582/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1583/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1584/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1582/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1583/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1584/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1585/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1586/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1587/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1585/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1586/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1587/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1588/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1589/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1590/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1588/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1589/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1590/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1591/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1592/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1593/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1591/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1592/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1593/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1594/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1595/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1596/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1594/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1595/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1596/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1597/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1598/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1599/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1597/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1598/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1599/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1600/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1601/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1602/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1600/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1601/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1602/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1603/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1604/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1605/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1603/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1604/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1605/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1606/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1607/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1608/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1606/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1607/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1608/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1609/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1610/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1611/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1609/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1610/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1611/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1612/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1613/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1614/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1612/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1613/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1614/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1615/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1616/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1617/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1615/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1616/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1617/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1618/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1619/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1620/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1618/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1619/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1620/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1621/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1622/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1623/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1621/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1622/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1623/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1624/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1625/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1626/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1624/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1625/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1626/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1627/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1628/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1629/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1627/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1628/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1629/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1630/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1631/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1632/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1630/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1631/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1632/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1633/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1634/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1635/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1633/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1634/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1635/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1636/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1637/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1638/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1636/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1637/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1638/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1639/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1640/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1641/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1639/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1640/2000 | Train Acc: 99.98% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1641/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1642/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1643/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1644/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1642/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1643/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1644/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1645/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1646/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1647/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1645/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1646/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1647/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1648/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1649/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1650/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1648/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1649/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0011\n",
      "Epoch 1650/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1651/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1652/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1653/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1651/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1652/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1653/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1654/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1655/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1656/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1654/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1655/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1656/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1657/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1658/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1659/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1657/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1658/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1659/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1660/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1661/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1662/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1660/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1661/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1662/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1663/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1664/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1665/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1663/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1664/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1665/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1666/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1667/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1668/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1666/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1667/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1668/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1669/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1670/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1671/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1669/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1670/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1671/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1672/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1673/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1674/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1672/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1673/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1674/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1675/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1676/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1677/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1675/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1676/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1677/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1678/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1679/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1680/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1678/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1679/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1680/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1681/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1682/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1683/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1681/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1682/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1683/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1684/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1685/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1686/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1684/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1685/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1686/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1687/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1688/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1689/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1687/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1688/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1689/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1690/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1691/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1692/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1690/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1691/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1692/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1693/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1694/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1695/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1693/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1694/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1695/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1696/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1697/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1698/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1696/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1697/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1698/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1699/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1700/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1701/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1699/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1700/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1701/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1702/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1703/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1704/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1702/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1703/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1704/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1705/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1706/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1707/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1705/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1706/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1707/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1708/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1709/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1710/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1708/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1709/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1710/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1711/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1712/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1713/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1711/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1712/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1713/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1714/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1715/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1716/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1714/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1715/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1716/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1717/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1718/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1719/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1717/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1718/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1719/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1720/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1721/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1722/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1720/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1721/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1722/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1723/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1724/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1725/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1723/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1724/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1725/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1726/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1727/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1728/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1726/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1727/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1728/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1729/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1730/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1731/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1729/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1730/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1731/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0010\n",
      "Epoch 1732/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1733/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1734/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1732/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1733/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1734/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1735/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1736/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1737/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1735/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1736/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1737/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0010\n",
      "Epoch 1738/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1739/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1740/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1738/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1739/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1740/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1741/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1742/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1743/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1741/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1742/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1743/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1744/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1745/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1746/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1744/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1745/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1746/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1747/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1748/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1749/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1747/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1748/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1749/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1750/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1751/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0009\n",
      "Epoch 1752/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1750/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1751/2000 | Train Acc: 99.99% | Test Acc: 99.12% | Loss: 0.0009\n",
      "Epoch 1752/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1753/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1754/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1755/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1753/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1754/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1755/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1756/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1757/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1758/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1756/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1757/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1758/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1759/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1760/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1761/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1759/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1760/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1761/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1762/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1763/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1764/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1762/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1763/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1764/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1765/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1766/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1767/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1765/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1766/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1767/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1768/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1769/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1770/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1768/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1769/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1770/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1771/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1772/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1773/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1771/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1772/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1773/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1774/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1775/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1776/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1774/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1775/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1776/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1777/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1778/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1779/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1777/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1778/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1779/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1780/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1781/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1782/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1780/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1781/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1782/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1783/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1784/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1785/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1783/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1784/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1785/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1786/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1787/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1788/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1786/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1787/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1788/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1789/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1790/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1791/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1789/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1790/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1791/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1792/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1793/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1794/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1792/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1793/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1794/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1795/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1796/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1797/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1795/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1796/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1797/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1798/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1799/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1800/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1798/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1799/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1800/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1801/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1802/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1803/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1801/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1802/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1803/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1804/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1805/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1806/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1804/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1805/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1806/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1807/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1808/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1809/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1807/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1808/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1809/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1810/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1811/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1812/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1810/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1811/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1812/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1813/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1814/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1815/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1813/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1814/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1815/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1816/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1817/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1818/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1816/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1817/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1818/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1819/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1820/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1821/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1819/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1820/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1821/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1822/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1823/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1824/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1822/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1823/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1824/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1825/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1826/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1827/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1825/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1826/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1827/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1828/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1829/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1830/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1828/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1829/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1830/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1831/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1832/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1833/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1831/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1832/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1833/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1834/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1835/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1836/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1834/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1835/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1836/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1837/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1838/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1839/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1837/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1838/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1839/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1840/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1841/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1842/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1840/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1841/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0009\n",
      "Epoch 1842/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1843/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1844/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1845/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1843/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1844/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1845/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1846/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1847/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1848/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1846/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1847/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1848/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1849/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1850/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1851/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1849/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1850/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1851/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1852/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1853/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1854/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1852/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1853/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1854/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1855/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1856/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1857/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1855/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1856/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1857/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1858/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1859/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1860/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1858/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1859/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1860/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1861/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1862/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1863/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1861/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1862/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1863/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1864/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1865/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1866/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1864/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1865/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1866/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1867/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1868/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1869/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1867/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1868/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1869/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1870/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1871/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1872/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1870/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1871/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1872/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1873/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1874/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1875/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1873/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1874/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1875/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1876/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1877/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1878/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1876/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1877/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1878/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1879/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1880/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1881/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1879/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1880/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1881/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1882/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1883/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1884/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1882/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1883/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1884/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1885/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1886/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1887/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1885/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1886/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1887/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1888/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1889/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1890/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1888/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1889/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1890/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1891/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1892/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1893/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1891/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1892/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1893/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1894/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1895/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1896/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1894/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1895/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1896/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1897/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1898/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1899/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1897/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1898/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1899/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1900/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1901/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1902/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1900/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1901/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1902/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1903/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1904/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1905/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1903/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1904/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1905/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1906/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1907/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1908/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1906/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1907/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1908/2000 | Train Acc: 99.99% | Test Acc: 99.15% | Loss: 0.0008\n",
      "Epoch 1909/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1910/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1911/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1909/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1910/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1911/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1912/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1913/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1914/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1912/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1913/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1914/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1915/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1916/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1917/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1915/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1916/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1917/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1918/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1919/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1920/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1918/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1919/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1920/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1921/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1922/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1923/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1921/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1922/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1923/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1924/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1925/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1926/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1924/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1925/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1926/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1927/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1928/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1929/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1927/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1928/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1929/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1930/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1931/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1932/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1930/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1931/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1932/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1933/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1934/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1935/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1933/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1934/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1935/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1936/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1937/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1938/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1936/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1937/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1938/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1939/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1940/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1941/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1939/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1940/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1941/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1942/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1943/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1944/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1942/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1943/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1944/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1945/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1946/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1947/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1945/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1946/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1947/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1948/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1949/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1950/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1948/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1949/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1950/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1951/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1952/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1953/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1951/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1952/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1953/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1954/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1955/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1956/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1954/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1955/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1956/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1957/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1958/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1959/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1957/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1958/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1959/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1960/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1961/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1962/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1960/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1961/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1962/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1963/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1964/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1965/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1963/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1964/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1965/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1966/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1967/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1968/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1966/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1967/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1968/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1969/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1970/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1971/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1969/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0008\n",
      "Epoch 1970/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1971/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1972/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1973/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1974/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1972/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1973/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1974/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1975/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1976/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1977/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1975/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1976/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1977/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1978/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1979/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1980/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1978/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1979/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1980/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1981/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1982/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1983/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1981/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1982/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1983/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1984/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1985/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1986/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1984/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1985/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1986/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1987/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1988/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1989/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1987/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1988/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1989/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1990/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1991/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1992/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1990/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1991/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1992/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1993/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1994/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1995/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1993/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1994/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1995/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1996/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1997/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1998/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1996/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1997/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1998/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 1999/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 2000/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "\n",
      "Classification Report (Foundation Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1200\n",
      "           1       0.98      1.00      0.99       724\n",
      "           2       1.00      0.97      0.98       800\n",
      "           3       1.00      1.00      1.00       800\n",
      "\n",
      "    accuracy                           0.99      3524\n",
      "   macro avg       0.99      0.99      0.99      3524\n",
      "weighted avg       0.99      0.99      0.99      3524\n",
      "\n",
      "Epoch 1999/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "Epoch 2000/2000 | Train Acc: 99.99% | Test Acc: 99.18% | Loss: 0.0007\n",
      "\n",
      "Classification Report (Foundation Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1200\n",
      "           1       0.98      1.00      0.99       724\n",
      "           2       1.00      0.97      0.98       800\n",
      "           3       1.00      1.00      1.00       800\n",
      "\n",
      "    accuracy                           0.99      3524\n",
      "   macro avg       0.99      0.99      0.99      3524\n",
      "weighted avg       0.99      0.99      0.99      3524\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Huấn luyện mô hình nền tảng (BiLSTM)\n",
    "foundation_labels = sorted(df['label'].unique())[:4]\n",
    "df_foundation = df[df['label'].isin(foundation_labels)].reset_index(drop=True)\n",
    "X_found = df_foundation.drop(columns=['label']).values\n",
    "y_found = df_foundation['label'].values\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_found, y_found, test_size=0.2, stratify=y_found, random_state=42)\n",
    "# Xử lý NaN, infinity, giá trị quá lớn/tràn\n",
    "X_train_f = np.nan_to_num(X_train_f, nan=0, posinf=0, neginf=0)\n",
    "X_test_f = np.nan_to_num(X_test_f, nan=0, posinf=0, neginf=0)\n",
    "X_train_f = np.clip(X_train_f, -1e10, 1e10)\n",
    "X_test_f = np.clip(X_test_f, -1e10, 1e10)\n",
    "\n",
    "scaler_f = StandardScaler()\n",
    "X_train_f_scaled = scaler_f.fit_transform(X_train_f)\n",
    "X_test_f_scaled = scaler_f.transform(X_test_f)\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "input_size = X_train_f_scaled.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = len(np.unique(y_train_f))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BiLSTM(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "num_epochs = 2000\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "X_train_tensor = torch.tensor(X_train_f_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_f_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_f, dtype=torch.long).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_f, dtype=torch.long).to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    _, predicted_train = torch.max(outputs, 1)\n",
    "    train_acc = (predicted_train == y_train_tensor).float().mean().item()\n",
    "    train_accuracies.append(train_acc)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model(X_test_tensor)\n",
    "        _, predicted_test = torch.max(outputs_test, 1)\n",
    "        test_acc = (predicted_test == y_test_tensor).float().mean().item()\n",
    "        test_accuracies.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Acc: {train_acc*100:.2f}% | Test Acc: {test_acc*100:.2f}% | Loss: {loss.item():.4f}\")\n",
    "print(\"\\nClassification Report (Foundation Model):\")\n",
    "print(classification_report(y_test_tensor.cpu().numpy(), predicted_test.cpu().numpy(), zero_division=0))\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss', color='red')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs+1), train_accuracies, label='Train Accuracy', color='blue')\n",
    "plt.plot(range(1, num_epochs+1), test_accuracies, label='Test Accuracy', color='green')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "472bd014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client label 4: Train shape = (3200, 88), Test shape = (800, 88)\n",
      "Client label 5: Train shape = (3200, 88), Test shape = (800, 88)\n",
      "Client label 6: Train shape = (3200, 88), Test shape = (800, 88)\n",
      "Client label 7: Train shape = (3200, 88), Test shape = (800, 88)\n"
     ]
    }
   ],
   "source": [
    "# 5. Chuẩn bị client federated\n",
    "federated_labels = sorted(df['label'].unique())[4:]\n",
    "client_data = {}\n",
    "selected_columns = list(range(input_size))  # Sử dụng toàn bộ đặc trưng đã chọn từ foundation\n",
    "for label in federated_labels:\n",
    "    df_client = df[df['label'] == label].reset_index(drop=True)\n",
    "    X_client = df_client.drop(columns=['label']).values\n",
    "    y_client = df_client['label'].values\n",
    "    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_client, y_client, test_size=0.2, random_state=42)\n",
    "    # Xử lý NaN, infinity, giá trị quá lớn/tràn cho từng client\n",
    "    X_train_c = np.nan_to_num(X_train_c, nan=0, posinf=0, neginf=0)\n",
    "    X_test_c = np.nan_to_num(X_test_c, nan=0, posinf=0, neginf=0)\n",
    "    X_train_c = np.clip(X_train_c, -1e10, 1e10)\n",
    "    X_test_c = np.clip(X_test_c, -1e10, 1e10)\n",
    "    \n",
    "    scaler_c = StandardScaler()\n",
    "    X_train_c_scaled = scaler_c.fit_transform(X_train_c)\n",
    "    X_test_c_scaled = scaler_c.transform(X_test_c)\n",
    "    X_train_c_selected = X_train_c_scaled[:, selected_columns]\n",
    "    X_test_c_selected = X_test_c_scaled[:, selected_columns]\n",
    "    client_data[label] = {\n",
    "        'X_train': X_train_c_selected,\n",
    "        'y_train': y_train_c,\n",
    "        'X_test': X_test_c_selected,\n",
    "        'y_test': y_test_c,\n",
    "        'scaler': scaler_c\n",
    "    }\n",
    "    print(f\"Client label {label}: Train shape = {X_train_c_selected.shape}, Test shape = {X_test_c_selected.shape}\")\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim=32):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd3c96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Cài đặt Federated Learning (FedAvg)\n",
    "import copy\n",
    "class FederatedClient:\n",
    "    def __init__(self, client_id, client_data, model_class, model_params):\n",
    "        self.client_id = client_id\n",
    "        self.raw_data = client_data\n",
    "        self.model_class = model_class\n",
    "        self.model_params = model_params\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.prepare_local_data()\n",
    "    def prepare_local_data(self):\n",
    "        X = self.raw_data.drop(columns=['label']).values\n",
    "        y = self.raw_data['label'].values\n",
    "        X = np.nan_to_num(X, nan=0, posinf=0, neginf=0)\n",
    "        X = np.clip(X, -1e10, 1e10)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "        self.X_train_tensor = torch.tensor(self.X_train_scaled, dtype=torch.float32).to(self.device)\n",
    "        self.X_test_tensor = torch.tensor(self.X_test_scaled, dtype=torch.float32).to(self.device)\n",
    "        self.y_train_tensor = torch.tensor(self.y_train, dtype=torch.long).to(self.device)\n",
    "        self.y_test_tensor = torch.tensor(self.y_test, dtype=torch.long).to(self.device)\n",
    "    def local_train(self, global_weights, local_epochs=5, lr=0.001):\n",
    "        local_model = self.model_class(**self.model_params).to(self.device)\n",
    "        local_model.load_state_dict(global_weights)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(local_model.parameters(), lr=lr)\n",
    "        for epoch in range(local_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = local_model(self.X_train_tensor.unsqueeze(1))\n",
    "            loss = criterion(outputs, self.y_train_tensor)\n",
    "            if torch.isfinite(loss):\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(local_model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "        updated_weights = {k: v.cpu().clone() for k, v in local_model.state_dict().items()}\n",
    "        return updated_weights\n",
    "class FederatedServer:\n",
    "    def __init__(self, model_class, model_params):\n",
    "        self.model_class = model_class\n",
    "        self.model_params = model_params\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.global_model = model_class(**model_params).to(self.device)\n",
    "        self.global_weights = {k: v.cpu().clone() for k, v in self.global_model.state_dict().items()}\n",
    "    def aggregate_weights(self, client_weights_list, client_sample_sizes=None):\n",
    "        if not client_weights_list:\n",
    "            return self.global_weights\n",
    "        if client_sample_sizes is None:\n",
    "            # Mặc định: trung bình đều\n",
    "            aggregated_weights = copy.deepcopy(client_weights_list[0])\n",
    "            for key in aggregated_weights.keys():\n",
    "                for i in range(1, len(client_weights_list)):\n",
    "                    aggregated_weights[key] += client_weights_list[i][key]\n",
    "                aggregated_weights[key] = aggregated_weights[key] / len(client_weights_list)\n",
    "            return aggregated_weights\n",
    "        # Weighted FedAvg\n",
    "        total_samples = sum(client_sample_sizes)\n",
    "        aggregated_weights = copy.deepcopy(client_weights_list[0])\n",
    "        for key in aggregated_weights.keys():\n",
    "            aggregated_weights[key] = client_weights_list[0][key] * (client_sample_sizes[0] / total_samples)\n",
    "            for i in range(1, len(client_weights_list)):\n",
    "                aggregated_weights[key] += client_weights_list[i][key] * (client_sample_sizes[i] / total_samples)\n",
    "        return aggregated_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f2b4f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FedAvg Round 1/400 ---\n",
      "Round 1: Mean Global Test Accuracy = 0.5114 | Mean Global Test Loss = 1.9322\n",
      "\n",
      "--- FedAvg Round 2/400 ---\n",
      "Round 1: Mean Global Test Accuracy = 0.5114 | Mean Global Test Loss = 1.9322\n",
      "\n",
      "--- FedAvg Round 2/400 ---\n",
      "Round 2: Mean Global Test Accuracy = 0.5205 | Mean Global Test Loss = 1.7911\n",
      "\n",
      "--- FedAvg Round 3/400 ---\n",
      "Round 2: Mean Global Test Accuracy = 0.5205 | Mean Global Test Loss = 1.7911\n",
      "\n",
      "--- FedAvg Round 3/400 ---\n",
      "Round 3: Mean Global Test Accuracy = 0.5347 | Mean Global Test Loss = 1.6456\n",
      "\n",
      "--- FedAvg Round 4/400 ---\n",
      "Round 3: Mean Global Test Accuracy = 0.5347 | Mean Global Test Loss = 1.6456\n",
      "\n",
      "--- FedAvg Round 4/400 ---\n",
      "Round 4: Mean Global Test Accuracy = 0.5607 | Mean Global Test Loss = 1.4985\n",
      "\n",
      "--- FedAvg Round 5/400 ---\n",
      "Round 4: Mean Global Test Accuracy = 0.5607 | Mean Global Test Loss = 1.4985\n",
      "\n",
      "--- FedAvg Round 5/400 ---\n",
      "Round 5: Mean Global Test Accuracy = 0.5890 | Mean Global Test Loss = 1.3559\n",
      "\n",
      "--- FedAvg Round 6/400 ---\n",
      "Round 5: Mean Global Test Accuracy = 0.5890 | Mean Global Test Loss = 1.3559\n",
      "\n",
      "--- FedAvg Round 6/400 ---\n",
      "Round 6: Mean Global Test Accuracy = 0.6205 | Mean Global Test Loss = 1.2248\n",
      "\n",
      "--- FedAvg Round 7/400 ---\n",
      "Round 6: Mean Global Test Accuracy = 0.6205 | Mean Global Test Loss = 1.2248\n",
      "\n",
      "--- FedAvg Round 7/400 ---\n",
      "Round 7: Mean Global Test Accuracy = 0.6413 | Mean Global Test Loss = 1.1102\n",
      "\n",
      "--- FedAvg Round 8/400 ---\n",
      "Round 7: Mean Global Test Accuracy = 0.6413 | Mean Global Test Loss = 1.1102\n",
      "\n",
      "--- FedAvg Round 8/400 ---\n",
      "Round 8: Mean Global Test Accuracy = 0.6655 | Mean Global Test Loss = 1.0133\n",
      "\n",
      "--- FedAvg Round 9/400 ---\n",
      "Round 8: Mean Global Test Accuracy = 0.6655 | Mean Global Test Loss = 1.0133\n",
      "\n",
      "--- FedAvg Round 9/400 ---\n",
      "Round 9: Mean Global Test Accuracy = 0.6820 | Mean Global Test Loss = 0.9323\n",
      "\n",
      "--- FedAvg Round 10/400 ---\n",
      "Round 9: Mean Global Test Accuracy = 0.6820 | Mean Global Test Loss = 0.9323\n",
      "\n",
      "--- FedAvg Round 10/400 ---\n",
      "Round 10: Mean Global Test Accuracy = 0.7035 | Mean Global Test Loss = 0.8640\n",
      "\n",
      "--- FedAvg Round 11/400 ---\n",
      "Round 10: Mean Global Test Accuracy = 0.7035 | Mean Global Test Loss = 0.8640\n",
      "\n",
      "--- FedAvg Round 11/400 ---\n",
      "Round 11: Mean Global Test Accuracy = 0.7196 | Mean Global Test Loss = 0.8057\n",
      "\n",
      "--- FedAvg Round 12/400 ---\n",
      "Round 11: Mean Global Test Accuracy = 0.7196 | Mean Global Test Loss = 0.8057\n",
      "\n",
      "--- FedAvg Round 12/400 ---\n",
      "Round 12: Mean Global Test Accuracy = 0.7371 | Mean Global Test Loss = 0.7545\n",
      "\n",
      "--- FedAvg Round 13/400 ---\n",
      "Round 12: Mean Global Test Accuracy = 0.7371 | Mean Global Test Loss = 0.7545\n",
      "\n",
      "--- FedAvg Round 13/400 ---\n",
      "Round 13: Mean Global Test Accuracy = 0.7523 | Mean Global Test Loss = 0.7087\n",
      "\n",
      "--- FedAvg Round 14/400 ---\n",
      "Round 13: Mean Global Test Accuracy = 0.7523 | Mean Global Test Loss = 0.7087\n",
      "\n",
      "--- FedAvg Round 14/400 ---\n",
      "Round 14: Mean Global Test Accuracy = 0.7671 | Mean Global Test Loss = 0.6673\n",
      "\n",
      "--- FedAvg Round 15/400 ---\n",
      "Round 14: Mean Global Test Accuracy = 0.7671 | Mean Global Test Loss = 0.6673\n",
      "\n",
      "--- FedAvg Round 15/400 ---\n",
      "Round 15: Mean Global Test Accuracy = 0.7809 | Mean Global Test Loss = 0.6296\n",
      "\n",
      "--- FedAvg Round 16/400 ---\n",
      "Round 15: Mean Global Test Accuracy = 0.7809 | Mean Global Test Loss = 0.6296\n",
      "\n",
      "--- FedAvg Round 16/400 ---\n",
      "Round 16: Mean Global Test Accuracy = 0.8001 | Mean Global Test Loss = 0.5951\n",
      "\n",
      "--- FedAvg Round 17/400 ---\n",
      "Round 16: Mean Global Test Accuracy = 0.8001 | Mean Global Test Loss = 0.5951\n",
      "\n",
      "--- FedAvg Round 17/400 ---\n",
      "Round 17: Mean Global Test Accuracy = 0.8131 | Mean Global Test Loss = 0.5634\n",
      "\n",
      "--- FedAvg Round 18/400 ---\n",
      "Round 17: Mean Global Test Accuracy = 0.8131 | Mean Global Test Loss = 0.5634\n",
      "\n",
      "--- FedAvg Round 18/400 ---\n",
      "Round 18: Mean Global Test Accuracy = 0.8233 | Mean Global Test Loss = 0.5343\n",
      "\n",
      "--- FedAvg Round 19/400 ---\n",
      "Round 18: Mean Global Test Accuracy = 0.8233 | Mean Global Test Loss = 0.5343\n",
      "\n",
      "--- FedAvg Round 19/400 ---\n",
      "Round 19: Mean Global Test Accuracy = 0.8301 | Mean Global Test Loss = 0.5078\n",
      "\n",
      "--- FedAvg Round 20/400 ---\n",
      "Round 19: Mean Global Test Accuracy = 0.8301 | Mean Global Test Loss = 0.5078\n",
      "\n",
      "--- FedAvg Round 20/400 ---\n",
      "Round 20: Mean Global Test Accuracy = 0.8346 | Mean Global Test Loss = 0.4836\n",
      "\n",
      "--- FedAvg Round 21/400 ---\n",
      "Round 20: Mean Global Test Accuracy = 0.8346 | Mean Global Test Loss = 0.4836\n",
      "\n",
      "--- FedAvg Round 21/400 ---\n",
      "Round 21: Mean Global Test Accuracy = 0.8388 | Mean Global Test Loss = 0.4617\n",
      "\n",
      "--- FedAvg Round 22/400 ---\n",
      "Round 21: Mean Global Test Accuracy = 0.8388 | Mean Global Test Loss = 0.4617\n",
      "\n",
      "--- FedAvg Round 22/400 ---\n",
      "Round 22: Mean Global Test Accuracy = 0.8429 | Mean Global Test Loss = 0.4416\n",
      "\n",
      "--- FedAvg Round 23/400 ---\n",
      "Round 22: Mean Global Test Accuracy = 0.8429 | Mean Global Test Loss = 0.4416\n",
      "\n",
      "--- FedAvg Round 23/400 ---\n",
      "Round 23: Mean Global Test Accuracy = 0.8459 | Mean Global Test Loss = 0.4233\n",
      "\n",
      "--- FedAvg Round 24/400 ---\n",
      "Round 23: Mean Global Test Accuracy = 0.8459 | Mean Global Test Loss = 0.4233\n",
      "\n",
      "--- FedAvg Round 24/400 ---\n",
      "Round 24: Mean Global Test Accuracy = 0.8485 | Mean Global Test Loss = 0.4064\n",
      "\n",
      "--- FedAvg Round 25/400 ---\n",
      "Round 24: Mean Global Test Accuracy = 0.8485 | Mean Global Test Loss = 0.4064\n",
      "\n",
      "--- FedAvg Round 25/400 ---\n",
      "Round 25: Mean Global Test Accuracy = 0.8520 | Mean Global Test Loss = 0.3910\n",
      "\n",
      "--- FedAvg Round 26/400 ---\n",
      "Round 25: Mean Global Test Accuracy = 0.8520 | Mean Global Test Loss = 0.3910\n",
      "\n",
      "--- FedAvg Round 26/400 ---\n",
      "Round 26: Mean Global Test Accuracy = 0.8575 | Mean Global Test Loss = 0.3768\n",
      "\n",
      "--- FedAvg Round 27/400 ---\n",
      "Round 26: Mean Global Test Accuracy = 0.8575 | Mean Global Test Loss = 0.3768\n",
      "\n",
      "--- FedAvg Round 27/400 ---\n",
      "Round 27: Mean Global Test Accuracy = 0.8611 | Mean Global Test Loss = 0.3639\n",
      "\n",
      "--- FedAvg Round 28/400 ---\n",
      "Round 27: Mean Global Test Accuracy = 0.8611 | Mean Global Test Loss = 0.3639\n",
      "\n",
      "--- FedAvg Round 28/400 ---\n",
      "Round 28: Mean Global Test Accuracy = 0.8659 | Mean Global Test Loss = 0.3520\n",
      "\n",
      "--- FedAvg Round 29/400 ---\n",
      "Round 28: Mean Global Test Accuracy = 0.8659 | Mean Global Test Loss = 0.3520\n",
      "\n",
      "--- FedAvg Round 29/400 ---\n",
      "Round 29: Mean Global Test Accuracy = 0.8693 | Mean Global Test Loss = 0.3411\n",
      "\n",
      "--- FedAvg Round 30/400 ---\n",
      "Round 29: Mean Global Test Accuracy = 0.8693 | Mean Global Test Loss = 0.3411\n",
      "\n",
      "--- FedAvg Round 30/400 ---\n",
      "Round 30: Mean Global Test Accuracy = 0.8717 | Mean Global Test Loss = 0.3310\n",
      "\n",
      "--- FedAvg Round 31/400 ---\n",
      "Round 30: Mean Global Test Accuracy = 0.8717 | Mean Global Test Loss = 0.3310\n",
      "\n",
      "--- FedAvg Round 31/400 ---\n",
      "Round 31: Mean Global Test Accuracy = 0.8743 | Mean Global Test Loss = 0.3217\n",
      "\n",
      "--- FedAvg Round 32/400 ---\n",
      "Round 31: Mean Global Test Accuracy = 0.8743 | Mean Global Test Loss = 0.3217\n",
      "\n",
      "--- FedAvg Round 32/400 ---\n",
      "Round 32: Mean Global Test Accuracy = 0.8773 | Mean Global Test Loss = 0.3130\n",
      "\n",
      "--- FedAvg Round 33/400 ---\n",
      "Round 32: Mean Global Test Accuracy = 0.8773 | Mean Global Test Loss = 0.3130\n",
      "\n",
      "--- FedAvg Round 33/400 ---\n",
      "Round 33: Mean Global Test Accuracy = 0.8810 | Mean Global Test Loss = 0.3048\n",
      "\n",
      "--- FedAvg Round 34/400 ---\n",
      "Round 33: Mean Global Test Accuracy = 0.8810 | Mean Global Test Loss = 0.3048\n",
      "\n",
      "--- FedAvg Round 34/400 ---\n",
      "Round 34: Mean Global Test Accuracy = 0.8831 | Mean Global Test Loss = 0.2971\n",
      "\n",
      "--- FedAvg Round 35/400 ---\n",
      "Round 34: Mean Global Test Accuracy = 0.8831 | Mean Global Test Loss = 0.2971\n",
      "\n",
      "--- FedAvg Round 35/400 ---\n",
      "Round 35: Mean Global Test Accuracy = 0.8847 | Mean Global Test Loss = 0.2898\n",
      "\n",
      "--- FedAvg Round 36/400 ---\n",
      "Round 35: Mean Global Test Accuracy = 0.8847 | Mean Global Test Loss = 0.2898\n",
      "\n",
      "--- FedAvg Round 36/400 ---\n",
      "Round 36: Mean Global Test Accuracy = 0.8867 | Mean Global Test Loss = 0.2828\n",
      "\n",
      "--- FedAvg Round 37/400 ---\n",
      "Round 36: Mean Global Test Accuracy = 0.8867 | Mean Global Test Loss = 0.2828\n",
      "\n",
      "--- FedAvg Round 37/400 ---\n",
      "Round 37: Mean Global Test Accuracy = 0.8890 | Mean Global Test Loss = 0.2761\n",
      "\n",
      "--- FedAvg Round 38/400 ---\n",
      "Round 37: Mean Global Test Accuracy = 0.8890 | Mean Global Test Loss = 0.2761\n",
      "\n",
      "--- FedAvg Round 38/400 ---\n",
      "Round 38: Mean Global Test Accuracy = 0.8901 | Mean Global Test Loss = 0.2697\n",
      "\n",
      "--- FedAvg Round 39/400 ---\n",
      "Round 38: Mean Global Test Accuracy = 0.8901 | Mean Global Test Loss = 0.2697\n",
      "\n",
      "--- FedAvg Round 39/400 ---\n",
      "Round 39: Mean Global Test Accuracy = 0.8920 | Mean Global Test Loss = 0.2636\n",
      "\n",
      "--- FedAvg Round 40/400 ---\n",
      "Round 39: Mean Global Test Accuracy = 0.8920 | Mean Global Test Loss = 0.2636\n",
      "\n",
      "--- FedAvg Round 40/400 ---\n",
      "Round 40: Mean Global Test Accuracy = 0.8946 | Mean Global Test Loss = 0.2576\n",
      "\n",
      "--- FedAvg Round 41/400 ---\n",
      "Round 40: Mean Global Test Accuracy = 0.8946 | Mean Global Test Loss = 0.2576\n",
      "\n",
      "--- FedAvg Round 41/400 ---\n",
      "Round 41: Mean Global Test Accuracy = 0.8967 | Mean Global Test Loss = 0.2520\n",
      "\n",
      "--- FedAvg Round 42/400 ---\n",
      "Round 41: Mean Global Test Accuracy = 0.8967 | Mean Global Test Loss = 0.2520\n",
      "\n",
      "--- FedAvg Round 42/400 ---\n",
      "Round 42: Mean Global Test Accuracy = 0.8978 | Mean Global Test Loss = 0.2465\n",
      "\n",
      "--- FedAvg Round 43/400 ---\n",
      "Round 42: Mean Global Test Accuracy = 0.8978 | Mean Global Test Loss = 0.2465\n",
      "\n",
      "--- FedAvg Round 43/400 ---\n",
      "Round 43: Mean Global Test Accuracy = 0.8988 | Mean Global Test Loss = 0.2413\n",
      "\n",
      "--- FedAvg Round 44/400 ---\n",
      "Round 43: Mean Global Test Accuracy = 0.8988 | Mean Global Test Loss = 0.2413\n",
      "\n",
      "--- FedAvg Round 44/400 ---\n",
      "Round 44: Mean Global Test Accuracy = 0.9013 | Mean Global Test Loss = 0.2364\n",
      "\n",
      "--- FedAvg Round 45/400 ---\n",
      "Round 44: Mean Global Test Accuracy = 0.9013 | Mean Global Test Loss = 0.2364\n",
      "\n",
      "--- FedAvg Round 45/400 ---\n",
      "Round 45: Mean Global Test Accuracy = 0.9039 | Mean Global Test Loss = 0.2316\n",
      "\n",
      "--- FedAvg Round 46/400 ---\n",
      "Round 45: Mean Global Test Accuracy = 0.9039 | Mean Global Test Loss = 0.2316\n",
      "\n",
      "--- FedAvg Round 46/400 ---\n",
      "Round 46: Mean Global Test Accuracy = 0.9054 | Mean Global Test Loss = 0.2271\n",
      "\n",
      "--- FedAvg Round 47/400 ---\n",
      "Round 46: Mean Global Test Accuracy = 0.9054 | Mean Global Test Loss = 0.2271\n",
      "\n",
      "--- FedAvg Round 47/400 ---\n",
      "Round 47: Mean Global Test Accuracy = 0.9075 | Mean Global Test Loss = 0.2227\n",
      "\n",
      "--- FedAvg Round 48/400 ---\n",
      "Round 47: Mean Global Test Accuracy = 0.9075 | Mean Global Test Loss = 0.2227\n",
      "\n",
      "--- FedAvg Round 48/400 ---\n",
      "Round 48: Mean Global Test Accuracy = 0.9090 | Mean Global Test Loss = 0.2186\n",
      "\n",
      "--- FedAvg Round 49/400 ---\n",
      "Round 48: Mean Global Test Accuracy = 0.9090 | Mean Global Test Loss = 0.2186\n",
      "\n",
      "--- FedAvg Round 49/400 ---\n",
      "Round 49: Mean Global Test Accuracy = 0.9103 | Mean Global Test Loss = 0.2146\n",
      "\n",
      "--- FedAvg Round 50/400 ---\n",
      "Round 49: Mean Global Test Accuracy = 0.9103 | Mean Global Test Loss = 0.2146\n",
      "\n",
      "--- FedAvg Round 50/400 ---\n",
      "Round 50: Mean Global Test Accuracy = 0.9117 | Mean Global Test Loss = 0.2108\n",
      "\n",
      "--- FedAvg Round 51/400 ---\n",
      "Round 50: Mean Global Test Accuracy = 0.9117 | Mean Global Test Loss = 0.2108\n",
      "\n",
      "--- FedAvg Round 51/400 ---\n",
      "Round 51: Mean Global Test Accuracy = 0.9127 | Mean Global Test Loss = 0.2072\n",
      "\n",
      "--- FedAvg Round 52/400 ---\n",
      "Round 51: Mean Global Test Accuracy = 0.9127 | Mean Global Test Loss = 0.2072\n",
      "\n",
      "--- FedAvg Round 52/400 ---\n",
      "Round 52: Mean Global Test Accuracy = 0.9135 | Mean Global Test Loss = 0.2037\n",
      "\n",
      "--- FedAvg Round 53/400 ---\n",
      "Round 52: Mean Global Test Accuracy = 0.9135 | Mean Global Test Loss = 0.2037\n",
      "\n",
      "--- FedAvg Round 53/400 ---\n",
      "Round 53: Mean Global Test Accuracy = 0.9145 | Mean Global Test Loss = 0.2004\n",
      "\n",
      "--- FedAvg Round 54/400 ---\n",
      "Round 53: Mean Global Test Accuracy = 0.9145 | Mean Global Test Loss = 0.2004\n",
      "\n",
      "--- FedAvg Round 54/400 ---\n",
      "Round 54: Mean Global Test Accuracy = 0.9152 | Mean Global Test Loss = 0.1973\n",
      "\n",
      "--- FedAvg Round 55/400 ---\n",
      "Round 54: Mean Global Test Accuracy = 0.9152 | Mean Global Test Loss = 0.1973\n",
      "\n",
      "--- FedAvg Round 55/400 ---\n",
      "Round 55: Mean Global Test Accuracy = 0.9159 | Mean Global Test Loss = 0.1943\n",
      "\n",
      "--- FedAvg Round 56/400 ---\n",
      "Round 55: Mean Global Test Accuracy = 0.9159 | Mean Global Test Loss = 0.1943\n",
      "\n",
      "--- FedAvg Round 56/400 ---\n",
      "Round 56: Mean Global Test Accuracy = 0.9163 | Mean Global Test Loss = 0.1915\n",
      "\n",
      "--- FedAvg Round 57/400 ---\n",
      "Round 56: Mean Global Test Accuracy = 0.9163 | Mean Global Test Loss = 0.1915\n",
      "\n",
      "--- FedAvg Round 57/400 ---\n",
      "Round 57: Mean Global Test Accuracy = 0.9166 | Mean Global Test Loss = 0.1888\n",
      "\n",
      "--- FedAvg Round 58/400 ---\n",
      "Round 57: Mean Global Test Accuracy = 0.9166 | Mean Global Test Loss = 0.1888\n",
      "\n",
      "--- FedAvg Round 58/400 ---\n",
      "Round 58: Mean Global Test Accuracy = 0.9176 | Mean Global Test Loss = 0.1862\n",
      "\n",
      "--- FedAvg Round 59/400 ---\n",
      "Round 58: Mean Global Test Accuracy = 0.9176 | Mean Global Test Loss = 0.1862\n",
      "\n",
      "--- FedAvg Round 59/400 ---\n",
      "Round 59: Mean Global Test Accuracy = 0.9189 | Mean Global Test Loss = 0.1838\n",
      "\n",
      "--- FedAvg Round 60/400 ---\n",
      "Round 59: Mean Global Test Accuracy = 0.9189 | Mean Global Test Loss = 0.1838\n",
      "\n",
      "--- FedAvg Round 60/400 ---\n",
      "Round 60: Mean Global Test Accuracy = 0.9205 | Mean Global Test Loss = 0.1815\n",
      "\n",
      "--- FedAvg Round 61/400 ---\n",
      "Round 60: Mean Global Test Accuracy = 0.9205 | Mean Global Test Loss = 0.1815\n",
      "\n",
      "--- FedAvg Round 61/400 ---\n",
      "Round 61: Mean Global Test Accuracy = 0.9214 | Mean Global Test Loss = 0.1793\n",
      "\n",
      "--- FedAvg Round 62/400 ---\n",
      "Round 61: Mean Global Test Accuracy = 0.9214 | Mean Global Test Loss = 0.1793\n",
      "\n",
      "--- FedAvg Round 62/400 ---\n",
      "Round 62: Mean Global Test Accuracy = 0.9223 | Mean Global Test Loss = 0.1771\n",
      "\n",
      "--- FedAvg Round 63/400 ---\n",
      "Round 62: Mean Global Test Accuracy = 0.9223 | Mean Global Test Loss = 0.1771\n",
      "\n",
      "--- FedAvg Round 63/400 ---\n",
      "Round 63: Mean Global Test Accuracy = 0.9226 | Mean Global Test Loss = 0.1751\n",
      "\n",
      "--- FedAvg Round 64/400 ---\n",
      "Round 63: Mean Global Test Accuracy = 0.9226 | Mean Global Test Loss = 0.1751\n",
      "\n",
      "--- FedAvg Round 64/400 ---\n",
      "Round 64: Mean Global Test Accuracy = 0.9231 | Mean Global Test Loss = 0.1731\n",
      "\n",
      "--- FedAvg Round 65/400 ---\n",
      "Round 64: Mean Global Test Accuracy = 0.9231 | Mean Global Test Loss = 0.1731\n",
      "\n",
      "--- FedAvg Round 65/400 ---\n",
      "Round 65: Mean Global Test Accuracy = 0.9236 | Mean Global Test Loss = 0.1713\n",
      "\n",
      "--- FedAvg Round 66/400 ---\n",
      "Round 65: Mean Global Test Accuracy = 0.9236 | Mean Global Test Loss = 0.1713\n",
      "\n",
      "--- FedAvg Round 66/400 ---\n",
      "Round 66: Mean Global Test Accuracy = 0.9242 | Mean Global Test Loss = 0.1695\n",
      "\n",
      "--- FedAvg Round 67/400 ---\n",
      "Round 66: Mean Global Test Accuracy = 0.9242 | Mean Global Test Loss = 0.1695\n",
      "\n",
      "--- FedAvg Round 67/400 ---\n",
      "Round 67: Mean Global Test Accuracy = 0.9248 | Mean Global Test Loss = 0.1679\n",
      "\n",
      "--- FedAvg Round 68/400 ---\n",
      "Round 67: Mean Global Test Accuracy = 0.9248 | Mean Global Test Loss = 0.1679\n",
      "\n",
      "--- FedAvg Round 68/400 ---\n",
      "Round 68: Mean Global Test Accuracy = 0.9252 | Mean Global Test Loss = 0.1662\n",
      "\n",
      "--- FedAvg Round 69/400 ---\n",
      "Round 68: Mean Global Test Accuracy = 0.9252 | Mean Global Test Loss = 0.1662\n",
      "\n",
      "--- FedAvg Round 69/400 ---\n",
      "Round 69: Mean Global Test Accuracy = 0.9257 | Mean Global Test Loss = 0.1647\n",
      "\n",
      "--- FedAvg Round 70/400 ---\n",
      "Round 69: Mean Global Test Accuracy = 0.9257 | Mean Global Test Loss = 0.1647\n",
      "\n",
      "--- FedAvg Round 70/400 ---\n",
      "Round 70: Mean Global Test Accuracy = 0.9268 | Mean Global Test Loss = 0.1631\n",
      "\n",
      "--- FedAvg Round 71/400 ---\n",
      "Round 70: Mean Global Test Accuracy = 0.9268 | Mean Global Test Loss = 0.1631\n",
      "\n",
      "--- FedAvg Round 71/400 ---\n",
      "Round 71: Mean Global Test Accuracy = 0.9266 | Mean Global Test Loss = 0.1618\n",
      "\n",
      "--- FedAvg Round 72/400 ---\n",
      "Round 71: Mean Global Test Accuracy = 0.9266 | Mean Global Test Loss = 0.1618\n",
      "\n",
      "--- FedAvg Round 72/400 ---\n",
      "Round 72: Mean Global Test Accuracy = 0.9270 | Mean Global Test Loss = 0.1603\n",
      "\n",
      "--- FedAvg Round 73/400 ---\n",
      "Round 72: Mean Global Test Accuracy = 0.9270 | Mean Global Test Loss = 0.1603\n",
      "\n",
      "--- FedAvg Round 73/400 ---\n",
      "Round 73: Mean Global Test Accuracy = 0.9279 | Mean Global Test Loss = 0.1591\n",
      "\n",
      "--- FedAvg Round 74/400 ---\n",
      "Round 73: Mean Global Test Accuracy = 0.9279 | Mean Global Test Loss = 0.1591\n",
      "\n",
      "--- FedAvg Round 74/400 ---\n",
      "Round 74: Mean Global Test Accuracy = 0.9275 | Mean Global Test Loss = 0.1577\n",
      "\n",
      "--- FedAvg Round 75/400 ---\n",
      "Round 74: Mean Global Test Accuracy = 0.9275 | Mean Global Test Loss = 0.1577\n",
      "\n",
      "--- FedAvg Round 75/400 ---\n",
      "Round 75: Mean Global Test Accuracy = 0.9286 | Mean Global Test Loss = 0.1566\n",
      "\n",
      "--- FedAvg Round 76/400 ---\n",
      "Round 75: Mean Global Test Accuracy = 0.9286 | Mean Global Test Loss = 0.1566\n",
      "\n",
      "--- FedAvg Round 76/400 ---\n",
      "Round 76: Mean Global Test Accuracy = 0.9286 | Mean Global Test Loss = 0.1553\n",
      "\n",
      "--- FedAvg Round 77/400 ---\n",
      "Round 76: Mean Global Test Accuracy = 0.9286 | Mean Global Test Loss = 0.1553\n",
      "\n",
      "--- FedAvg Round 77/400 ---\n",
      "Round 77: Mean Global Test Accuracy = 0.9286 | Mean Global Test Loss = 0.1543\n",
      "\n",
      "--- FedAvg Round 78/400 ---\n",
      "Round 77: Mean Global Test Accuracy = 0.9286 | Mean Global Test Loss = 0.1543\n",
      "\n",
      "--- FedAvg Round 78/400 ---\n",
      "Round 78: Mean Global Test Accuracy = 0.9288 | Mean Global Test Loss = 0.1531\n",
      "\n",
      "--- FedAvg Round 79/400 ---\n",
      "Round 78: Mean Global Test Accuracy = 0.9288 | Mean Global Test Loss = 0.1531\n",
      "\n",
      "--- FedAvg Round 79/400 ---\n",
      "Round 79: Mean Global Test Accuracy = 0.9289 | Mean Global Test Loss = 0.1521\n",
      "\n",
      "--- FedAvg Round 80/400 ---\n",
      "Round 79: Mean Global Test Accuracy = 0.9289 | Mean Global Test Loss = 0.1521\n",
      "\n",
      "--- FedAvg Round 80/400 ---\n",
      "Round 80: Mean Global Test Accuracy = 0.9296 | Mean Global Test Loss = 0.1511\n",
      "\n",
      "--- FedAvg Round 81/400 ---\n",
      "Round 80: Mean Global Test Accuracy = 0.9296 | Mean Global Test Loss = 0.1511\n",
      "\n",
      "--- FedAvg Round 81/400 ---\n",
      "Round 81: Mean Global Test Accuracy = 0.9291 | Mean Global Test Loss = 0.1502\n",
      "\n",
      "--- FedAvg Round 82/400 ---\n",
      "Round 81: Mean Global Test Accuracy = 0.9291 | Mean Global Test Loss = 0.1502\n",
      "\n",
      "--- FedAvg Round 82/400 ---\n",
      "Round 82: Mean Global Test Accuracy = 0.9296 | Mean Global Test Loss = 0.1492\n",
      "\n",
      "--- FedAvg Round 83/400 ---\n",
      "Round 82: Mean Global Test Accuracy = 0.9296 | Mean Global Test Loss = 0.1492\n",
      "\n",
      "--- FedAvg Round 83/400 ---\n",
      "Round 83: Mean Global Test Accuracy = 0.9297 | Mean Global Test Loss = 0.1483\n",
      "\n",
      "--- FedAvg Round 84/400 ---\n",
      "Round 83: Mean Global Test Accuracy = 0.9297 | Mean Global Test Loss = 0.1483\n",
      "\n",
      "--- FedAvg Round 84/400 ---\n",
      "Round 84: Mean Global Test Accuracy = 0.9298 | Mean Global Test Loss = 0.1474\n",
      "\n",
      "--- FedAvg Round 85/400 ---\n",
      "Round 84: Mean Global Test Accuracy = 0.9298 | Mean Global Test Loss = 0.1474\n",
      "\n",
      "--- FedAvg Round 85/400 ---\n",
      "Round 85: Mean Global Test Accuracy = 0.9305 | Mean Global Test Loss = 0.1466\n",
      "\n",
      "--- FedAvg Round 86/400 ---\n",
      "Round 85: Mean Global Test Accuracy = 0.9305 | Mean Global Test Loss = 0.1466\n",
      "\n",
      "--- FedAvg Round 86/400 ---\n",
      "Round 86: Mean Global Test Accuracy = 0.9303 | Mean Global Test Loss = 0.1457\n",
      "\n",
      "--- FedAvg Round 87/400 ---\n",
      "Round 86: Mean Global Test Accuracy = 0.9303 | Mean Global Test Loss = 0.1457\n",
      "\n",
      "--- FedAvg Round 87/400 ---\n",
      "Round 87: Mean Global Test Accuracy = 0.9306 | Mean Global Test Loss = 0.1449\n",
      "\n",
      "--- FedAvg Round 88/400 ---\n",
      "Round 87: Mean Global Test Accuracy = 0.9306 | Mean Global Test Loss = 0.1449\n",
      "\n",
      "--- FedAvg Round 88/400 ---\n",
      "Round 88: Mean Global Test Accuracy = 0.9313 | Mean Global Test Loss = 0.1441\n",
      "\n",
      "--- FedAvg Round 89/400 ---\n",
      "Round 88: Mean Global Test Accuracy = 0.9313 | Mean Global Test Loss = 0.1441\n",
      "\n",
      "--- FedAvg Round 89/400 ---\n",
      "Round 89: Mean Global Test Accuracy = 0.9310 | Mean Global Test Loss = 0.1434\n",
      "\n",
      "--- FedAvg Round 90/400 ---\n",
      "Round 89: Mean Global Test Accuracy = 0.9310 | Mean Global Test Loss = 0.1434\n",
      "\n",
      "--- FedAvg Round 90/400 ---\n",
      "Round 90: Mean Global Test Accuracy = 0.9316 | Mean Global Test Loss = 0.1426\n",
      "\n",
      "--- FedAvg Round 91/400 ---\n",
      "Round 90: Mean Global Test Accuracy = 0.9316 | Mean Global Test Loss = 0.1426\n",
      "\n",
      "--- FedAvg Round 91/400 ---\n",
      "Round 91: Mean Global Test Accuracy = 0.9315 | Mean Global Test Loss = 0.1419\n",
      "\n",
      "--- FedAvg Round 92/400 ---\n",
      "Round 91: Mean Global Test Accuracy = 0.9315 | Mean Global Test Loss = 0.1419\n",
      "\n",
      "--- FedAvg Round 92/400 ---\n",
      "Round 92: Mean Global Test Accuracy = 0.9314 | Mean Global Test Loss = 0.1413\n",
      "\n",
      "--- FedAvg Round 93/400 ---\n",
      "Round 92: Mean Global Test Accuracy = 0.9314 | Mean Global Test Loss = 0.1413\n",
      "\n",
      "--- FedAvg Round 93/400 ---\n",
      "Round 93: Mean Global Test Accuracy = 0.9317 | Mean Global Test Loss = 0.1406\n",
      "\n",
      "--- FedAvg Round 94/400 ---\n",
      "Round 93: Mean Global Test Accuracy = 0.9317 | Mean Global Test Loss = 0.1406\n",
      "\n",
      "--- FedAvg Round 94/400 ---\n",
      "Round 94: Mean Global Test Accuracy = 0.9318 | Mean Global Test Loss = 0.1400\n",
      "\n",
      "--- FedAvg Round 95/400 ---\n",
      "Round 94: Mean Global Test Accuracy = 0.9318 | Mean Global Test Loss = 0.1400\n",
      "\n",
      "--- FedAvg Round 95/400 ---\n",
      "Round 95: Mean Global Test Accuracy = 0.9323 | Mean Global Test Loss = 0.1394\n",
      "\n",
      "--- FedAvg Round 96/400 ---\n",
      "Round 95: Mean Global Test Accuracy = 0.9323 | Mean Global Test Loss = 0.1394\n",
      "\n",
      "--- FedAvg Round 96/400 ---\n",
      "Round 96: Mean Global Test Accuracy = 0.9324 | Mean Global Test Loss = 0.1388\n",
      "\n",
      "--- FedAvg Round 97/400 ---\n",
      "Round 96: Mean Global Test Accuracy = 0.9324 | Mean Global Test Loss = 0.1388\n",
      "\n",
      "--- FedAvg Round 97/400 ---\n",
      "Round 97: Mean Global Test Accuracy = 0.9324 | Mean Global Test Loss = 0.1382\n",
      "\n",
      "--- FedAvg Round 98/400 ---\n",
      "Round 97: Mean Global Test Accuracy = 0.9324 | Mean Global Test Loss = 0.1382\n",
      "\n",
      "--- FedAvg Round 98/400 ---\n",
      "Round 98: Mean Global Test Accuracy = 0.9325 | Mean Global Test Loss = 0.1376\n",
      "\n",
      "--- FedAvg Round 99/400 ---\n",
      "Round 98: Mean Global Test Accuracy = 0.9325 | Mean Global Test Loss = 0.1376\n",
      "\n",
      "--- FedAvg Round 99/400 ---\n",
      "Round 99: Mean Global Test Accuracy = 0.9330 | Mean Global Test Loss = 0.1371\n",
      "\n",
      "--- FedAvg Round 100/400 ---\n",
      "Round 99: Mean Global Test Accuracy = 0.9330 | Mean Global Test Loss = 0.1371\n",
      "\n",
      "--- FedAvg Round 100/400 ---\n",
      "Round 100: Mean Global Test Accuracy = 0.9328 | Mean Global Test Loss = 0.1366\n",
      "\n",
      "--- FedAvg Round 101/400 ---\n",
      "Round 100: Mean Global Test Accuracy = 0.9328 | Mean Global Test Loss = 0.1366\n",
      "\n",
      "--- FedAvg Round 101/400 ---\n",
      "Round 101: Mean Global Test Accuracy = 0.9329 | Mean Global Test Loss = 0.1361\n",
      "\n",
      "--- FedAvg Round 102/400 ---\n",
      "Round 101: Mean Global Test Accuracy = 0.9329 | Mean Global Test Loss = 0.1361\n",
      "\n",
      "--- FedAvg Round 102/400 ---\n",
      "Round 102: Mean Global Test Accuracy = 0.9338 | Mean Global Test Loss = 0.1356\n",
      "\n",
      "--- FedAvg Round 103/400 ---\n",
      "Round 102: Mean Global Test Accuracy = 0.9338 | Mean Global Test Loss = 0.1356\n",
      "\n",
      "--- FedAvg Round 103/400 ---\n",
      "Round 103: Mean Global Test Accuracy = 0.9335 | Mean Global Test Loss = 0.1351\n",
      "\n",
      "--- FedAvg Round 104/400 ---\n",
      "Round 103: Mean Global Test Accuracy = 0.9335 | Mean Global Test Loss = 0.1351\n",
      "\n",
      "--- FedAvg Round 104/400 ---\n",
      "Round 104: Mean Global Test Accuracy = 0.9346 | Mean Global Test Loss = 0.1347\n",
      "\n",
      "--- FedAvg Round 105/400 ---\n",
      "Round 104: Mean Global Test Accuracy = 0.9346 | Mean Global Test Loss = 0.1347\n",
      "\n",
      "--- FedAvg Round 105/400 ---\n",
      "Round 105: Mean Global Test Accuracy = 0.9342 | Mean Global Test Loss = 0.1343\n",
      "\n",
      "--- FedAvg Round 106/400 ---\n",
      "Round 105: Mean Global Test Accuracy = 0.9342 | Mean Global Test Loss = 0.1343\n",
      "\n",
      "--- FedAvg Round 106/400 ---\n",
      "Round 106: Mean Global Test Accuracy = 0.9355 | Mean Global Test Loss = 0.1338\n",
      "\n",
      "--- FedAvg Round 107/400 ---\n",
      "Round 106: Mean Global Test Accuracy = 0.9355 | Mean Global Test Loss = 0.1338\n",
      "\n",
      "--- FedAvg Round 107/400 ---\n",
      "Round 107: Mean Global Test Accuracy = 0.9347 | Mean Global Test Loss = 0.1335\n",
      "\n",
      "--- FedAvg Round 108/400 ---\n",
      "Round 107: Mean Global Test Accuracy = 0.9347 | Mean Global Test Loss = 0.1335\n",
      "\n",
      "--- FedAvg Round 108/400 ---\n",
      "Round 108: Mean Global Test Accuracy = 0.9355 | Mean Global Test Loss = 0.1331\n",
      "\n",
      "--- FedAvg Round 109/400 ---\n",
      "Round 108: Mean Global Test Accuracy = 0.9355 | Mean Global Test Loss = 0.1331\n",
      "\n",
      "--- FedAvg Round 109/400 ---\n",
      "Round 109: Mean Global Test Accuracy = 0.9354 | Mean Global Test Loss = 0.1327\n",
      "\n",
      "--- FedAvg Round 110/400 ---\n",
      "Round 109: Mean Global Test Accuracy = 0.9354 | Mean Global Test Loss = 0.1327\n",
      "\n",
      "--- FedAvg Round 110/400 ---\n",
      "Round 110: Mean Global Test Accuracy = 0.9362 | Mean Global Test Loss = 0.1324\n",
      "\n",
      "--- FedAvg Round 111/400 ---\n",
      "Round 110: Mean Global Test Accuracy = 0.9362 | Mean Global Test Loss = 0.1324\n",
      "\n",
      "--- FedAvg Round 111/400 ---\n",
      "Round 111: Mean Global Test Accuracy = 0.9358 | Mean Global Test Loss = 0.1320\n",
      "\n",
      "--- FedAvg Round 112/400 ---\n",
      "Round 111: Mean Global Test Accuracy = 0.9358 | Mean Global Test Loss = 0.1320\n",
      "\n",
      "--- FedAvg Round 112/400 ---\n",
      "Round 112: Mean Global Test Accuracy = 0.9365 | Mean Global Test Loss = 0.1317\n",
      "\n",
      "--- FedAvg Round 113/400 ---\n",
      "Round 112: Mean Global Test Accuracy = 0.9365 | Mean Global Test Loss = 0.1317\n",
      "\n",
      "--- FedAvg Round 113/400 ---\n",
      "Round 113: Mean Global Test Accuracy = 0.9361 | Mean Global Test Loss = 0.1314\n",
      "\n",
      "--- FedAvg Round 114/400 ---\n",
      "Round 113: Mean Global Test Accuracy = 0.9361 | Mean Global Test Loss = 0.1314\n",
      "\n",
      "--- FedAvg Round 114/400 ---\n",
      "Round 114: Mean Global Test Accuracy = 0.9363 | Mean Global Test Loss = 0.1310\n",
      "\n",
      "--- FedAvg Round 115/400 ---\n",
      "Round 114: Mean Global Test Accuracy = 0.9363 | Mean Global Test Loss = 0.1310\n",
      "\n",
      "--- FedAvg Round 115/400 ---\n",
      "Round 115: Mean Global Test Accuracy = 0.9361 | Mean Global Test Loss = 0.1307\n",
      "\n",
      "--- FedAvg Round 116/400 ---\n",
      "Round 115: Mean Global Test Accuracy = 0.9361 | Mean Global Test Loss = 0.1307\n",
      "\n",
      "--- FedAvg Round 116/400 ---\n",
      "Round 116: Mean Global Test Accuracy = 0.9362 | Mean Global Test Loss = 0.1304\n",
      "\n",
      "--- FedAvg Round 117/400 ---\n",
      "Round 116: Mean Global Test Accuracy = 0.9362 | Mean Global Test Loss = 0.1304\n",
      "\n",
      "--- FedAvg Round 117/400 ---\n",
      "Round 117: Mean Global Test Accuracy = 0.9364 | Mean Global Test Loss = 0.1301\n",
      "\n",
      "--- FedAvg Round 118/400 ---\n",
      "Round 117: Mean Global Test Accuracy = 0.9364 | Mean Global Test Loss = 0.1301\n",
      "\n",
      "--- FedAvg Round 118/400 ---\n",
      "Round 118: Mean Global Test Accuracy = 0.9363 | Mean Global Test Loss = 0.1297\n",
      "\n",
      "--- FedAvg Round 119/400 ---\n",
      "Round 118: Mean Global Test Accuracy = 0.9363 | Mean Global Test Loss = 0.1297\n",
      "\n",
      "--- FedAvg Round 119/400 ---\n",
      "Round 119: Mean Global Test Accuracy = 0.9371 | Mean Global Test Loss = 0.1294\n",
      "\n",
      "--- FedAvg Round 120/400 ---\n",
      "Round 119: Mean Global Test Accuracy = 0.9371 | Mean Global Test Loss = 0.1294\n",
      "\n",
      "--- FedAvg Round 120/400 ---\n",
      "Round 120: Mean Global Test Accuracy = 0.9374 | Mean Global Test Loss = 0.1291\n",
      "\n",
      "--- FedAvg Round 121/400 ---\n",
      "Round 120: Mean Global Test Accuracy = 0.9374 | Mean Global Test Loss = 0.1291\n",
      "\n",
      "--- FedAvg Round 121/400 ---\n",
      "Round 121: Mean Global Test Accuracy = 0.9375 | Mean Global Test Loss = 0.1288\n",
      "\n",
      "--- FedAvg Round 122/400 ---\n",
      "Round 121: Mean Global Test Accuracy = 0.9375 | Mean Global Test Loss = 0.1288\n",
      "\n",
      "--- FedAvg Round 122/400 ---\n",
      "Round 122: Mean Global Test Accuracy = 0.9377 | Mean Global Test Loss = 0.1285\n",
      "\n",
      "--- FedAvg Round 123/400 ---\n",
      "Round 122: Mean Global Test Accuracy = 0.9377 | Mean Global Test Loss = 0.1285\n",
      "\n",
      "--- FedAvg Round 123/400 ---\n",
      "Round 123: Mean Global Test Accuracy = 0.9382 | Mean Global Test Loss = 0.1283\n",
      "\n",
      "--- FedAvg Round 124/400 ---\n",
      "Round 123: Mean Global Test Accuracy = 0.9382 | Mean Global Test Loss = 0.1283\n",
      "\n",
      "--- FedAvg Round 124/400 ---\n",
      "Round 124: Mean Global Test Accuracy = 0.9388 | Mean Global Test Loss = 0.1280\n",
      "\n",
      "--- FedAvg Round 125/400 ---\n",
      "Round 124: Mean Global Test Accuracy = 0.9388 | Mean Global Test Loss = 0.1280\n",
      "\n",
      "--- FedAvg Round 125/400 ---\n",
      "Round 125: Mean Global Test Accuracy = 0.9389 | Mean Global Test Loss = 0.1277\n",
      "\n",
      "--- FedAvg Round 126/400 ---\n",
      "Round 125: Mean Global Test Accuracy = 0.9389 | Mean Global Test Loss = 0.1277\n",
      "\n",
      "--- FedAvg Round 126/400 ---\n",
      "Round 126: Mean Global Test Accuracy = 0.9393 | Mean Global Test Loss = 0.1274\n",
      "\n",
      "--- FedAvg Round 127/400 ---\n",
      "Round 126: Mean Global Test Accuracy = 0.9393 | Mean Global Test Loss = 0.1274\n",
      "\n",
      "--- FedAvg Round 127/400 ---\n",
      "Round 127: Mean Global Test Accuracy = 0.9399 | Mean Global Test Loss = 0.1271\n",
      "\n",
      "--- FedAvg Round 128/400 ---\n",
      "Round 127: Mean Global Test Accuracy = 0.9399 | Mean Global Test Loss = 0.1271\n",
      "\n",
      "--- FedAvg Round 128/400 ---\n",
      "Round 128: Mean Global Test Accuracy = 0.9400 | Mean Global Test Loss = 0.1268\n",
      "\n",
      "--- FedAvg Round 129/400 ---\n",
      "Round 128: Mean Global Test Accuracy = 0.9400 | Mean Global Test Loss = 0.1268\n",
      "\n",
      "--- FedAvg Round 129/400 ---\n",
      "Round 129: Mean Global Test Accuracy = 0.9399 | Mean Global Test Loss = 0.1266\n",
      "\n",
      "--- FedAvg Round 130/400 ---\n",
      "Round 129: Mean Global Test Accuracy = 0.9399 | Mean Global Test Loss = 0.1266\n",
      "\n",
      "--- FedAvg Round 130/400 ---\n",
      "Round 130: Mean Global Test Accuracy = 0.9399 | Mean Global Test Loss = 0.1264\n",
      "\n",
      "--- FedAvg Round 131/400 ---\n",
      "Round 130: Mean Global Test Accuracy = 0.9399 | Mean Global Test Loss = 0.1264\n",
      "\n",
      "--- FedAvg Round 131/400 ---\n",
      "Round 131: Mean Global Test Accuracy = 0.9406 | Mean Global Test Loss = 0.1261\n",
      "\n",
      "--- FedAvg Round 132/400 ---\n",
      "Round 131: Mean Global Test Accuracy = 0.9406 | Mean Global Test Loss = 0.1261\n",
      "\n",
      "--- FedAvg Round 132/400 ---\n",
      "Round 132: Mean Global Test Accuracy = 0.9407 | Mean Global Test Loss = 0.1259\n",
      "\n",
      "--- FedAvg Round 133/400 ---\n",
      "Round 132: Mean Global Test Accuracy = 0.9407 | Mean Global Test Loss = 0.1259\n",
      "\n",
      "--- FedAvg Round 133/400 ---\n",
      "Round 133: Mean Global Test Accuracy = 0.9413 | Mean Global Test Loss = 0.1257\n",
      "\n",
      "--- FedAvg Round 134/400 ---\n",
      "Round 133: Mean Global Test Accuracy = 0.9413 | Mean Global Test Loss = 0.1257\n",
      "\n",
      "--- FedAvg Round 134/400 ---\n",
      "Round 134: Mean Global Test Accuracy = 0.9414 | Mean Global Test Loss = 0.1254\n",
      "\n",
      "--- FedAvg Round 135/400 ---\n",
      "Round 134: Mean Global Test Accuracy = 0.9414 | Mean Global Test Loss = 0.1254\n",
      "\n",
      "--- FedAvg Round 135/400 ---\n",
      "Round 135: Mean Global Test Accuracy = 0.9415 | Mean Global Test Loss = 0.1252\n",
      "\n",
      "--- FedAvg Round 136/400 ---\n",
      "Round 135: Mean Global Test Accuracy = 0.9415 | Mean Global Test Loss = 0.1252\n",
      "\n",
      "--- FedAvg Round 136/400 ---\n",
      "Round 136: Mean Global Test Accuracy = 0.9419 | Mean Global Test Loss = 0.1250\n",
      "\n",
      "--- FedAvg Round 137/400 ---\n",
      "Round 136: Mean Global Test Accuracy = 0.9419 | Mean Global Test Loss = 0.1250\n",
      "\n",
      "--- FedAvg Round 137/400 ---\n",
      "Round 137: Mean Global Test Accuracy = 0.9420 | Mean Global Test Loss = 0.1247\n",
      "\n",
      "--- FedAvg Round 138/400 ---\n",
      "Round 137: Mean Global Test Accuracy = 0.9420 | Mean Global Test Loss = 0.1247\n",
      "\n",
      "--- FedAvg Round 138/400 ---\n",
      "Round 138: Mean Global Test Accuracy = 0.9419 | Mean Global Test Loss = 0.1245\n",
      "\n",
      "--- FedAvg Round 139/400 ---\n",
      "Round 138: Mean Global Test Accuracy = 0.9419 | Mean Global Test Loss = 0.1245\n",
      "\n",
      "--- FedAvg Round 139/400 ---\n",
      "Round 139: Mean Global Test Accuracy = 0.9423 | Mean Global Test Loss = 0.1243\n",
      "\n",
      "--- FedAvg Round 140/400 ---\n",
      "Round 139: Mean Global Test Accuracy = 0.9423 | Mean Global Test Loss = 0.1243\n",
      "\n",
      "--- FedAvg Round 140/400 ---\n",
      "Round 140: Mean Global Test Accuracy = 0.9422 | Mean Global Test Loss = 0.1241\n",
      "\n",
      "--- FedAvg Round 141/400 ---\n",
      "Round 140: Mean Global Test Accuracy = 0.9422 | Mean Global Test Loss = 0.1241\n",
      "\n",
      "--- FedAvg Round 141/400 ---\n",
      "Round 141: Mean Global Test Accuracy = 0.9426 | Mean Global Test Loss = 0.1239\n",
      "\n",
      "--- FedAvg Round 142/400 ---\n",
      "Round 141: Mean Global Test Accuracy = 0.9426 | Mean Global Test Loss = 0.1239\n",
      "\n",
      "--- FedAvg Round 142/400 ---\n",
      "Round 142: Mean Global Test Accuracy = 0.9428 | Mean Global Test Loss = 0.1237\n",
      "\n",
      "--- FedAvg Round 143/400 ---\n",
      "Round 142: Mean Global Test Accuracy = 0.9428 | Mean Global Test Loss = 0.1237\n",
      "\n",
      "--- FedAvg Round 143/400 ---\n",
      "Round 143: Mean Global Test Accuracy = 0.9431 | Mean Global Test Loss = 0.1235\n",
      "\n",
      "--- FedAvg Round 144/400 ---\n",
      "Round 143: Mean Global Test Accuracy = 0.9431 | Mean Global Test Loss = 0.1235\n",
      "\n",
      "--- FedAvg Round 144/400 ---\n",
      "Round 144: Mean Global Test Accuracy = 0.9434 | Mean Global Test Loss = 0.1233\n",
      "\n",
      "--- FedAvg Round 145/400 ---\n",
      "Round 144: Mean Global Test Accuracy = 0.9434 | Mean Global Test Loss = 0.1233\n",
      "\n",
      "--- FedAvg Round 145/400 ---\n",
      "Round 145: Mean Global Test Accuracy = 0.9434 | Mean Global Test Loss = 0.1231\n",
      "\n",
      "--- FedAvg Round 146/400 ---\n",
      "Round 145: Mean Global Test Accuracy = 0.9434 | Mean Global Test Loss = 0.1231\n",
      "\n",
      "--- FedAvg Round 146/400 ---\n",
      "Round 146: Mean Global Test Accuracy = 0.9436 | Mean Global Test Loss = 0.1229\n",
      "\n",
      "--- FedAvg Round 147/400 ---\n",
      "Round 146: Mean Global Test Accuracy = 0.9436 | Mean Global Test Loss = 0.1229\n",
      "\n",
      "--- FedAvg Round 147/400 ---\n",
      "Round 147: Mean Global Test Accuracy = 0.9442 | Mean Global Test Loss = 0.1227\n",
      "\n",
      "--- FedAvg Round 148/400 ---\n",
      "Round 147: Mean Global Test Accuracy = 0.9442 | Mean Global Test Loss = 0.1227\n",
      "\n",
      "--- FedAvg Round 148/400 ---\n",
      "Round 148: Mean Global Test Accuracy = 0.9442 | Mean Global Test Loss = 0.1225\n",
      "\n",
      "--- FedAvg Round 149/400 ---\n",
      "Round 148: Mean Global Test Accuracy = 0.9442 | Mean Global Test Loss = 0.1225\n",
      "\n",
      "--- FedAvg Round 149/400 ---\n",
      "Round 149: Mean Global Test Accuracy = 0.9444 | Mean Global Test Loss = 0.1223\n",
      "\n",
      "--- FedAvg Round 150/400 ---\n",
      "Round 149: Mean Global Test Accuracy = 0.9444 | Mean Global Test Loss = 0.1223\n",
      "\n",
      "--- FedAvg Round 150/400 ---\n",
      "Round 150: Mean Global Test Accuracy = 0.9444 | Mean Global Test Loss = 0.1222\n",
      "\n",
      "--- FedAvg Round 151/400 ---\n",
      "Round 150: Mean Global Test Accuracy = 0.9444 | Mean Global Test Loss = 0.1222\n",
      "\n",
      "--- FedAvg Round 151/400 ---\n",
      "Round 151: Mean Global Test Accuracy = 0.9455 | Mean Global Test Loss = 0.1220\n",
      "\n",
      "--- FedAvg Round 152/400 ---\n",
      "Round 151: Mean Global Test Accuracy = 0.9455 | Mean Global Test Loss = 0.1220\n",
      "\n",
      "--- FedAvg Round 152/400 ---\n",
      "Round 152: Mean Global Test Accuracy = 0.9455 | Mean Global Test Loss = 0.1218\n",
      "\n",
      "--- FedAvg Round 153/400 ---\n",
      "Round 152: Mean Global Test Accuracy = 0.9455 | Mean Global Test Loss = 0.1218\n",
      "\n",
      "--- FedAvg Round 153/400 ---\n",
      "Round 153: Mean Global Test Accuracy = 0.9459 | Mean Global Test Loss = 0.1217\n",
      "\n",
      "--- FedAvg Round 154/400 ---\n",
      "Round 153: Mean Global Test Accuracy = 0.9459 | Mean Global Test Loss = 0.1217\n",
      "\n",
      "--- FedAvg Round 154/400 ---\n",
      "Round 154: Mean Global Test Accuracy = 0.9457 | Mean Global Test Loss = 0.1215\n",
      "\n",
      "--- FedAvg Round 155/400 ---\n",
      "Round 154: Mean Global Test Accuracy = 0.9457 | Mean Global Test Loss = 0.1215\n",
      "\n",
      "--- FedAvg Round 155/400 ---\n",
      "Round 155: Mean Global Test Accuracy = 0.9463 | Mean Global Test Loss = 0.1213\n",
      "\n",
      "--- FedAvg Round 156/400 ---\n",
      "Round 155: Mean Global Test Accuracy = 0.9463 | Mean Global Test Loss = 0.1213\n",
      "\n",
      "--- FedAvg Round 156/400 ---\n",
      "Round 156: Mean Global Test Accuracy = 0.9466 | Mean Global Test Loss = 0.1212\n",
      "\n",
      "--- FedAvg Round 157/400 ---\n",
      "Round 156: Mean Global Test Accuracy = 0.9466 | Mean Global Test Loss = 0.1212\n",
      "\n",
      "--- FedAvg Round 157/400 ---\n",
      "Round 157: Mean Global Test Accuracy = 0.9467 | Mean Global Test Loss = 0.1210\n",
      "\n",
      "--- FedAvg Round 158/400 ---\n",
      "Round 157: Mean Global Test Accuracy = 0.9467 | Mean Global Test Loss = 0.1210\n",
      "\n",
      "--- FedAvg Round 158/400 ---\n",
      "Round 158: Mean Global Test Accuracy = 0.9467 | Mean Global Test Loss = 0.1208\n",
      "\n",
      "--- FedAvg Round 159/400 ---\n",
      "Round 158: Mean Global Test Accuracy = 0.9467 | Mean Global Test Loss = 0.1208\n",
      "\n",
      "--- FedAvg Round 159/400 ---\n",
      "Round 159: Mean Global Test Accuracy = 0.9467 | Mean Global Test Loss = 0.1207\n",
      "\n",
      "--- FedAvg Round 160/400 ---\n",
      "Round 159: Mean Global Test Accuracy = 0.9467 | Mean Global Test Loss = 0.1207\n",
      "\n",
      "--- FedAvg Round 160/400 ---\n",
      "Round 160: Mean Global Test Accuracy = 0.9467 | Mean Global Test Loss = 0.1205\n",
      "\n",
      "--- FedAvg Round 161/400 ---\n",
      "Round 160: Mean Global Test Accuracy = 0.9467 | Mean Global Test Loss = 0.1205\n",
      "\n",
      "--- FedAvg Round 161/400 ---\n",
      "Round 161: Mean Global Test Accuracy = 0.9468 | Mean Global Test Loss = 0.1203\n",
      "\n",
      "--- FedAvg Round 162/400 ---\n",
      "Round 161: Mean Global Test Accuracy = 0.9468 | Mean Global Test Loss = 0.1203\n",
      "\n",
      "--- FedAvg Round 162/400 ---\n",
      "Round 162: Mean Global Test Accuracy = 0.9471 | Mean Global Test Loss = 0.1202\n",
      "\n",
      "--- FedAvg Round 163/400 ---\n",
      "Round 162: Mean Global Test Accuracy = 0.9471 | Mean Global Test Loss = 0.1202\n",
      "\n",
      "--- FedAvg Round 163/400 ---\n",
      "Round 163: Mean Global Test Accuracy = 0.9475 | Mean Global Test Loss = 0.1200\n",
      "\n",
      "--- FedAvg Round 164/400 ---\n",
      "Round 163: Mean Global Test Accuracy = 0.9475 | Mean Global Test Loss = 0.1200\n",
      "\n",
      "--- FedAvg Round 164/400 ---\n",
      "Round 164: Mean Global Test Accuracy = 0.9473 | Mean Global Test Loss = 0.1199\n",
      "\n",
      "--- FedAvg Round 165/400 ---\n",
      "Round 164: Mean Global Test Accuracy = 0.9473 | Mean Global Test Loss = 0.1199\n",
      "\n",
      "--- FedAvg Round 165/400 ---\n",
      "Round 165: Mean Global Test Accuracy = 0.9473 | Mean Global Test Loss = 0.1197\n",
      "\n",
      "--- FedAvg Round 166/400 ---\n",
      "Round 165: Mean Global Test Accuracy = 0.9473 | Mean Global Test Loss = 0.1197\n",
      "\n",
      "--- FedAvg Round 166/400 ---\n",
      "Round 166: Mean Global Test Accuracy = 0.9475 | Mean Global Test Loss = 0.1196\n",
      "\n",
      "--- FedAvg Round 167/400 ---\n",
      "Round 166: Mean Global Test Accuracy = 0.9475 | Mean Global Test Loss = 0.1196\n",
      "\n",
      "--- FedAvg Round 167/400 ---\n",
      "Round 167: Mean Global Test Accuracy = 0.9475 | Mean Global Test Loss = 0.1195\n",
      "\n",
      "--- FedAvg Round 168/400 ---\n",
      "Round 167: Mean Global Test Accuracy = 0.9475 | Mean Global Test Loss = 0.1195\n",
      "\n",
      "--- FedAvg Round 168/400 ---\n",
      "Round 168: Mean Global Test Accuracy = 0.9475 | Mean Global Test Loss = 0.1193\n",
      "\n",
      "--- FedAvg Round 169/400 ---\n",
      "Round 168: Mean Global Test Accuracy = 0.9475 | Mean Global Test Loss = 0.1193\n",
      "\n",
      "--- FedAvg Round 169/400 ---\n",
      "Round 169: Mean Global Test Accuracy = 0.9475 | Mean Global Test Loss = 0.1192\n",
      "\n",
      "--- FedAvg Round 170/400 ---\n",
      "Round 169: Mean Global Test Accuracy = 0.9475 | Mean Global Test Loss = 0.1192\n",
      "\n",
      "--- FedAvg Round 170/400 ---\n",
      "Round 170: Mean Global Test Accuracy = 0.9476 | Mean Global Test Loss = 0.1190\n",
      "\n",
      "--- FedAvg Round 171/400 ---\n",
      "Round 170: Mean Global Test Accuracy = 0.9476 | Mean Global Test Loss = 0.1190\n",
      "\n",
      "--- FedAvg Round 171/400 ---\n",
      "Round 171: Mean Global Test Accuracy = 0.9477 | Mean Global Test Loss = 0.1189\n",
      "\n",
      "--- FedAvg Round 172/400 ---\n",
      "Round 171: Mean Global Test Accuracy = 0.9477 | Mean Global Test Loss = 0.1189\n",
      "\n",
      "--- FedAvg Round 172/400 ---\n",
      "Round 172: Mean Global Test Accuracy = 0.9478 | Mean Global Test Loss = 0.1188\n",
      "\n",
      "--- FedAvg Round 173/400 ---\n",
      "Round 172: Mean Global Test Accuracy = 0.9478 | Mean Global Test Loss = 0.1188\n",
      "\n",
      "--- FedAvg Round 173/400 ---\n",
      "Round 173: Mean Global Test Accuracy = 0.9479 | Mean Global Test Loss = 0.1187\n",
      "\n",
      "--- FedAvg Round 174/400 ---\n",
      "Round 173: Mean Global Test Accuracy = 0.9479 | Mean Global Test Loss = 0.1187\n",
      "\n",
      "--- FedAvg Round 174/400 ---\n",
      "Round 174: Mean Global Test Accuracy = 0.9483 | Mean Global Test Loss = 0.1185\n",
      "\n",
      "--- FedAvg Round 175/400 ---\n",
      "Round 174: Mean Global Test Accuracy = 0.9483 | Mean Global Test Loss = 0.1185\n",
      "\n",
      "--- FedAvg Round 175/400 ---\n",
      "Round 175: Mean Global Test Accuracy = 0.9480 | Mean Global Test Loss = 0.1185\n",
      "\n",
      "--- FedAvg Round 176/400 ---\n",
      "Round 175: Mean Global Test Accuracy = 0.9480 | Mean Global Test Loss = 0.1185\n",
      "\n",
      "--- FedAvg Round 176/400 ---\n",
      "Round 176: Mean Global Test Accuracy = 0.9487 | Mean Global Test Loss = 0.1183\n",
      "\n",
      "--- FedAvg Round 177/400 ---\n",
      "Round 176: Mean Global Test Accuracy = 0.9487 | Mean Global Test Loss = 0.1183\n",
      "\n",
      "--- FedAvg Round 177/400 ---\n",
      "Round 177: Mean Global Test Accuracy = 0.9483 | Mean Global Test Loss = 0.1182\n",
      "\n",
      "--- FedAvg Round 178/400 ---\n",
      "Round 177: Mean Global Test Accuracy = 0.9483 | Mean Global Test Loss = 0.1182\n",
      "\n",
      "--- FedAvg Round 178/400 ---\n",
      "Round 178: Mean Global Test Accuracy = 0.9488 | Mean Global Test Loss = 0.1181\n",
      "\n",
      "--- FedAvg Round 179/400 ---\n",
      "Round 178: Mean Global Test Accuracy = 0.9488 | Mean Global Test Loss = 0.1181\n",
      "\n",
      "--- FedAvg Round 179/400 ---\n",
      "Round 179: Mean Global Test Accuracy = 0.9489 | Mean Global Test Loss = 0.1180\n",
      "\n",
      "--- FedAvg Round 180/400 ---\n",
      "Round 179: Mean Global Test Accuracy = 0.9489 | Mean Global Test Loss = 0.1180\n",
      "\n",
      "--- FedAvg Round 180/400 ---\n",
      "Round 180: Mean Global Test Accuracy = 0.9494 | Mean Global Test Loss = 0.1179\n",
      "\n",
      "--- FedAvg Round 181/400 ---\n",
      "Round 180: Mean Global Test Accuracy = 0.9494 | Mean Global Test Loss = 0.1179\n",
      "\n",
      "--- FedAvg Round 181/400 ---\n",
      "Round 181: Mean Global Test Accuracy = 0.9493 | Mean Global Test Loss = 0.1178\n",
      "\n",
      "--- FedAvg Round 182/400 ---\n",
      "Round 181: Mean Global Test Accuracy = 0.9493 | Mean Global Test Loss = 0.1178\n",
      "\n",
      "--- FedAvg Round 182/400 ---\n",
      "Round 182: Mean Global Test Accuracy = 0.9500 | Mean Global Test Loss = 0.1177\n",
      "\n",
      "--- FedAvg Round 183/400 ---\n",
      "Round 182: Mean Global Test Accuracy = 0.9500 | Mean Global Test Loss = 0.1177\n",
      "\n",
      "--- FedAvg Round 183/400 ---\n",
      "Round 183: Mean Global Test Accuracy = 0.9499 | Mean Global Test Loss = 0.1176\n",
      "\n",
      "--- FedAvg Round 184/400 ---\n",
      "Round 183: Mean Global Test Accuracy = 0.9499 | Mean Global Test Loss = 0.1176\n",
      "\n",
      "--- FedAvg Round 184/400 ---\n",
      "Round 184: Mean Global Test Accuracy = 0.9506 | Mean Global Test Loss = 0.1174\n",
      "\n",
      "--- FedAvg Round 185/400 ---\n",
      "Round 184: Mean Global Test Accuracy = 0.9506 | Mean Global Test Loss = 0.1174\n",
      "\n",
      "--- FedAvg Round 185/400 ---\n",
      "Round 185: Mean Global Test Accuracy = 0.9497 | Mean Global Test Loss = 0.1174\n",
      "\n",
      "--- FedAvg Round 186/400 ---\n",
      "Round 185: Mean Global Test Accuracy = 0.9497 | Mean Global Test Loss = 0.1174\n",
      "\n",
      "--- FedAvg Round 186/400 ---\n",
      "Round 186: Mean Global Test Accuracy = 0.9507 | Mean Global Test Loss = 0.1172\n",
      "\n",
      "--- FedAvg Round 187/400 ---\n",
      "Round 186: Mean Global Test Accuracy = 0.9507 | Mean Global Test Loss = 0.1172\n",
      "\n",
      "--- FedAvg Round 187/400 ---\n",
      "Round 187: Mean Global Test Accuracy = 0.9499 | Mean Global Test Loss = 0.1172\n",
      "\n",
      "--- FedAvg Round 188/400 ---\n",
      "Round 187: Mean Global Test Accuracy = 0.9499 | Mean Global Test Loss = 0.1172\n",
      "\n",
      "--- FedAvg Round 188/400 ---\n",
      "Round 188: Mean Global Test Accuracy = 0.9513 | Mean Global Test Loss = 0.1170\n",
      "\n",
      "--- FedAvg Round 189/400 ---\n",
      "Round 188: Mean Global Test Accuracy = 0.9513 | Mean Global Test Loss = 0.1170\n",
      "\n",
      "--- FedAvg Round 189/400 ---\n",
      "Round 189: Mean Global Test Accuracy = 0.9499 | Mean Global Test Loss = 0.1169\n",
      "\n",
      "--- FedAvg Round 190/400 ---\n",
      "Round 189: Mean Global Test Accuracy = 0.9499 | Mean Global Test Loss = 0.1169\n",
      "\n",
      "--- FedAvg Round 190/400 ---\n",
      "Round 190: Mean Global Test Accuracy = 0.9507 | Mean Global Test Loss = 0.1168\n",
      "\n",
      "--- FedAvg Round 191/400 ---\n",
      "Round 190: Mean Global Test Accuracy = 0.9507 | Mean Global Test Loss = 0.1168\n",
      "\n",
      "--- FedAvg Round 191/400 ---\n",
      "Round 191: Mean Global Test Accuracy = 0.9502 | Mean Global Test Loss = 0.1168\n",
      "\n",
      "--- FedAvg Round 192/400 ---\n",
      "Round 191: Mean Global Test Accuracy = 0.9502 | Mean Global Test Loss = 0.1168\n",
      "\n",
      "--- FedAvg Round 192/400 ---\n",
      "Round 192: Mean Global Test Accuracy = 0.9515 | Mean Global Test Loss = 0.1167\n",
      "\n",
      "--- FedAvg Round 193/400 ---\n",
      "Round 192: Mean Global Test Accuracy = 0.9515 | Mean Global Test Loss = 0.1167\n",
      "\n",
      "--- FedAvg Round 193/400 ---\n",
      "Round 193: Mean Global Test Accuracy = 0.9506 | Mean Global Test Loss = 0.1166\n",
      "\n",
      "--- FedAvg Round 194/400 ---\n",
      "Round 193: Mean Global Test Accuracy = 0.9506 | Mean Global Test Loss = 0.1166\n",
      "\n",
      "--- FedAvg Round 194/400 ---\n",
      "Round 194: Mean Global Test Accuracy = 0.9520 | Mean Global Test Loss = 0.1165\n",
      "\n",
      "--- FedAvg Round 195/400 ---\n",
      "Round 194: Mean Global Test Accuracy = 0.9520 | Mean Global Test Loss = 0.1165\n",
      "\n",
      "--- FedAvg Round 195/400 ---\n",
      "Round 195: Mean Global Test Accuracy = 0.9515 | Mean Global Test Loss = 0.1164\n",
      "\n",
      "--- FedAvg Round 196/400 ---\n",
      "Round 195: Mean Global Test Accuracy = 0.9515 | Mean Global Test Loss = 0.1164\n",
      "\n",
      "--- FedAvg Round 196/400 ---\n",
      "Round 196: Mean Global Test Accuracy = 0.9524 | Mean Global Test Loss = 0.1163\n",
      "\n",
      "--- FedAvg Round 197/400 ---\n",
      "Round 196: Mean Global Test Accuracy = 0.9524 | Mean Global Test Loss = 0.1163\n",
      "\n",
      "--- FedAvg Round 197/400 ---\n",
      "Round 197: Mean Global Test Accuracy = 0.9520 | Mean Global Test Loss = 0.1163\n",
      "\n",
      "--- FedAvg Round 198/400 ---\n",
      "Round 197: Mean Global Test Accuracy = 0.9520 | Mean Global Test Loss = 0.1163\n",
      "\n",
      "--- FedAvg Round 198/400 ---\n",
      "Round 198: Mean Global Test Accuracy = 0.9530 | Mean Global Test Loss = 0.1161\n",
      "\n",
      "--- FedAvg Round 199/400 ---\n",
      "Round 198: Mean Global Test Accuracy = 0.9530 | Mean Global Test Loss = 0.1161\n",
      "\n",
      "--- FedAvg Round 199/400 ---\n",
      "Round 199: Mean Global Test Accuracy = 0.9524 | Mean Global Test Loss = 0.1161\n",
      "\n",
      "--- FedAvg Round 200/400 ---\n",
      "Round 199: Mean Global Test Accuracy = 0.9524 | Mean Global Test Loss = 0.1161\n",
      "\n",
      "--- FedAvg Round 200/400 ---\n",
      "Round 200: Mean Global Test Accuracy = 0.9530 | Mean Global Test Loss = 0.1160\n",
      "\n",
      "--- FedAvg Round 201/400 ---\n",
      "Round 200: Mean Global Test Accuracy = 0.9530 | Mean Global Test Loss = 0.1160\n",
      "\n",
      "--- FedAvg Round 201/400 ---\n",
      "Round 201: Mean Global Test Accuracy = 0.9527 | Mean Global Test Loss = 0.1159\n",
      "\n",
      "--- FedAvg Round 202/400 ---\n",
      "Round 201: Mean Global Test Accuracy = 0.9527 | Mean Global Test Loss = 0.1159\n",
      "\n",
      "--- FedAvg Round 202/400 ---\n",
      "Round 202: Mean Global Test Accuracy = 0.9530 | Mean Global Test Loss = 0.1158\n",
      "\n",
      "--- FedAvg Round 203/400 ---\n",
      "Round 202: Mean Global Test Accuracy = 0.9530 | Mean Global Test Loss = 0.1158\n",
      "\n",
      "--- FedAvg Round 203/400 ---\n",
      "Round 203: Mean Global Test Accuracy = 0.9533 | Mean Global Test Loss = 0.1157\n",
      "\n",
      "--- FedAvg Round 204/400 ---\n",
      "Round 203: Mean Global Test Accuracy = 0.9533 | Mean Global Test Loss = 0.1157\n",
      "\n",
      "--- FedAvg Round 204/400 ---\n",
      "Round 204: Mean Global Test Accuracy = 0.9532 | Mean Global Test Loss = 0.1156\n",
      "\n",
      "--- FedAvg Round 205/400 ---\n",
      "Round 204: Mean Global Test Accuracy = 0.9532 | Mean Global Test Loss = 0.1156\n",
      "\n",
      "--- FedAvg Round 205/400 ---\n",
      "Round 205: Mean Global Test Accuracy = 0.9533 | Mean Global Test Loss = 0.1156\n",
      "\n",
      "--- FedAvg Round 206/400 ---\n",
      "Round 205: Mean Global Test Accuracy = 0.9533 | Mean Global Test Loss = 0.1156\n",
      "\n",
      "--- FedAvg Round 206/400 ---\n",
      "Round 206: Mean Global Test Accuracy = 0.9534 | Mean Global Test Loss = 0.1155\n",
      "\n",
      "--- FedAvg Round 207/400 ---\n",
      "Round 206: Mean Global Test Accuracy = 0.9534 | Mean Global Test Loss = 0.1155\n",
      "\n",
      "--- FedAvg Round 207/400 ---\n",
      "Round 207: Mean Global Test Accuracy = 0.9538 | Mean Global Test Loss = 0.1154\n",
      "\n",
      "--- FedAvg Round 208/400 ---\n",
      "Round 207: Mean Global Test Accuracy = 0.9538 | Mean Global Test Loss = 0.1154\n",
      "\n",
      "--- FedAvg Round 208/400 ---\n",
      "Round 208: Mean Global Test Accuracy = 0.9536 | Mean Global Test Loss = 0.1154\n",
      "\n",
      "--- FedAvg Round 209/400 ---\n",
      "Round 208: Mean Global Test Accuracy = 0.9536 | Mean Global Test Loss = 0.1154\n",
      "\n",
      "--- FedAvg Round 209/400 ---\n",
      "Round 209: Mean Global Test Accuracy = 0.9539 | Mean Global Test Loss = 0.1153\n",
      "\n",
      "--- FedAvg Round 210/400 ---\n",
      "Round 209: Mean Global Test Accuracy = 0.9539 | Mean Global Test Loss = 0.1153\n",
      "\n",
      "--- FedAvg Round 210/400 ---\n",
      "Round 210: Mean Global Test Accuracy = 0.9538 | Mean Global Test Loss = 0.1152\n",
      "\n",
      "--- FedAvg Round 211/400 ---\n",
      "Round 210: Mean Global Test Accuracy = 0.9538 | Mean Global Test Loss = 0.1152\n",
      "\n",
      "--- FedAvg Round 211/400 ---\n",
      "Round 211: Mean Global Test Accuracy = 0.9540 | Mean Global Test Loss = 0.1152\n",
      "\n",
      "--- FedAvg Round 212/400 ---\n",
      "Round 211: Mean Global Test Accuracy = 0.9540 | Mean Global Test Loss = 0.1152\n",
      "\n",
      "--- FedAvg Round 212/400 ---\n",
      "Round 212: Mean Global Test Accuracy = 0.9538 | Mean Global Test Loss = 0.1151\n",
      "\n",
      "--- FedAvg Round 213/400 ---\n",
      "Round 212: Mean Global Test Accuracy = 0.9538 | Mean Global Test Loss = 0.1151\n",
      "\n",
      "--- FedAvg Round 213/400 ---\n",
      "Round 213: Mean Global Test Accuracy = 0.9540 | Mean Global Test Loss = 0.1151\n",
      "\n",
      "--- FedAvg Round 214/400 ---\n",
      "Round 213: Mean Global Test Accuracy = 0.9540 | Mean Global Test Loss = 0.1151\n",
      "\n",
      "--- FedAvg Round 214/400 ---\n",
      "Round 214: Mean Global Test Accuracy = 0.9540 | Mean Global Test Loss = 0.1150\n",
      "\n",
      "--- FedAvg Round 215/400 ---\n",
      "Round 214: Mean Global Test Accuracy = 0.9540 | Mean Global Test Loss = 0.1150\n",
      "\n",
      "--- FedAvg Round 215/400 ---\n",
      "Round 215: Mean Global Test Accuracy = 0.9540 | Mean Global Test Loss = 0.1150\n",
      "\n",
      "--- FedAvg Round 216/400 ---\n",
      "Round 215: Mean Global Test Accuracy = 0.9540 | Mean Global Test Loss = 0.1150\n",
      "\n",
      "--- FedAvg Round 216/400 ---\n",
      "Round 216: Mean Global Test Accuracy = 0.9540 | Mean Global Test Loss = 0.1150\n",
      "\n",
      "--- FedAvg Round 217/400 ---\n",
      "Round 216: Mean Global Test Accuracy = 0.9540 | Mean Global Test Loss = 0.1150\n",
      "\n",
      "--- FedAvg Round 217/400 ---\n",
      "Round 217: Mean Global Test Accuracy = 0.9539 | Mean Global Test Loss = 0.1149\n",
      "\n",
      "--- FedAvg Round 218/400 ---\n",
      "Round 217: Mean Global Test Accuracy = 0.9539 | Mean Global Test Loss = 0.1149\n",
      "\n",
      "--- FedAvg Round 218/400 ---\n",
      "Round 218: Mean Global Test Accuracy = 0.9541 | Mean Global Test Loss = 0.1149\n",
      "\n",
      "--- FedAvg Round 219/400 ---\n",
      "Round 218: Mean Global Test Accuracy = 0.9541 | Mean Global Test Loss = 0.1149\n",
      "\n",
      "--- FedAvg Round 219/400 ---\n",
      "Round 219: Mean Global Test Accuracy = 0.9538 | Mean Global Test Loss = 0.1149\n",
      "\n",
      "--- FedAvg Round 220/400 ---\n",
      "Round 219: Mean Global Test Accuracy = 0.9538 | Mean Global Test Loss = 0.1149\n",
      "\n",
      "--- FedAvg Round 220/400 ---\n",
      "Round 220: Mean Global Test Accuracy = 0.9541 | Mean Global Test Loss = 0.1148\n",
      "\n",
      "--- FedAvg Round 221/400 ---\n",
      "Round 220: Mean Global Test Accuracy = 0.9541 | Mean Global Test Loss = 0.1148\n",
      "\n",
      "--- FedAvg Round 221/400 ---\n",
      "Round 221: Mean Global Test Accuracy = 0.9543 | Mean Global Test Loss = 0.1148\n",
      "\n",
      "--- FedAvg Round 222/400 ---\n",
      "Round 221: Mean Global Test Accuracy = 0.9543 | Mean Global Test Loss = 0.1148\n",
      "\n",
      "--- FedAvg Round 222/400 ---\n",
      "Round 222: Mean Global Test Accuracy = 0.9542 | Mean Global Test Loss = 0.1148\n",
      "\n",
      "--- FedAvg Round 223/400 ---\n",
      "Round 222: Mean Global Test Accuracy = 0.9542 | Mean Global Test Loss = 0.1148\n",
      "\n",
      "--- FedAvg Round 223/400 ---\n",
      "Round 223: Mean Global Test Accuracy = 0.9541 | Mean Global Test Loss = 0.1148\n",
      "\n",
      "--- FedAvg Round 224/400 ---\n",
      "Round 223: Mean Global Test Accuracy = 0.9541 | Mean Global Test Loss = 0.1148\n",
      "\n",
      "--- FedAvg Round 224/400 ---\n",
      "Round 224: Mean Global Test Accuracy = 0.9543 | Mean Global Test Loss = 0.1147\n",
      "\n",
      "--- FedAvg Round 225/400 ---\n",
      "Round 224: Mean Global Test Accuracy = 0.9543 | Mean Global Test Loss = 0.1147\n",
      "\n",
      "--- FedAvg Round 225/400 ---\n",
      "Round 225: Mean Global Test Accuracy = 0.9544 | Mean Global Test Loss = 0.1147\n",
      "\n",
      "--- FedAvg Round 226/400 ---\n",
      "Round 225: Mean Global Test Accuracy = 0.9544 | Mean Global Test Loss = 0.1147\n",
      "\n",
      "--- FedAvg Round 226/400 ---\n",
      "Round 226: Mean Global Test Accuracy = 0.9545 | Mean Global Test Loss = 0.1147\n",
      "\n",
      "--- FedAvg Round 227/400 ---\n",
      "Round 226: Mean Global Test Accuracy = 0.9545 | Mean Global Test Loss = 0.1147\n",
      "\n",
      "--- FedAvg Round 227/400 ---\n",
      "Round 227: Mean Global Test Accuracy = 0.9546 | Mean Global Test Loss = 0.1147\n",
      "\n",
      "--- FedAvg Round 228/400 ---\n",
      "Round 227: Mean Global Test Accuracy = 0.9546 | Mean Global Test Loss = 0.1147\n",
      "\n",
      "--- FedAvg Round 228/400 ---\n",
      "Round 228: Mean Global Test Accuracy = 0.9546 | Mean Global Test Loss = 0.1146\n",
      "\n",
      "--- FedAvg Round 229/400 ---\n",
      "Round 228: Mean Global Test Accuracy = 0.9546 | Mean Global Test Loss = 0.1146\n",
      "\n",
      "--- FedAvg Round 229/400 ---\n",
      "Round 229: Mean Global Test Accuracy = 0.9550 | Mean Global Test Loss = 0.1146\n",
      "\n",
      "--- FedAvg Round 230/400 ---\n",
      "Round 229: Mean Global Test Accuracy = 0.9550 | Mean Global Test Loss = 0.1146\n",
      "\n",
      "--- FedAvg Round 230/400 ---\n",
      "Round 230: Mean Global Test Accuracy = 0.9546 | Mean Global Test Loss = 0.1146\n",
      "\n",
      "--- FedAvg Round 231/400 ---\n",
      "Round 230: Mean Global Test Accuracy = 0.9546 | Mean Global Test Loss = 0.1146\n",
      "\n",
      "--- FedAvg Round 231/400 ---\n",
      "Round 231: Mean Global Test Accuracy = 0.9553 | Mean Global Test Loss = 0.1146\n",
      "\n",
      "--- FedAvg Round 232/400 ---\n",
      "Round 231: Mean Global Test Accuracy = 0.9553 | Mean Global Test Loss = 0.1146\n",
      "\n",
      "--- FedAvg Round 232/400 ---\n",
      "Round 232: Mean Global Test Accuracy = 0.9547 | Mean Global Test Loss = 0.1145\n",
      "\n",
      "--- FedAvg Round 233/400 ---\n",
      "Round 232: Mean Global Test Accuracy = 0.9547 | Mean Global Test Loss = 0.1145\n",
      "\n",
      "--- FedAvg Round 233/400 ---\n",
      "Round 233: Mean Global Test Accuracy = 0.9556 | Mean Global Test Loss = 0.1146\n",
      "\n",
      "--- FedAvg Round 234/400 ---\n",
      "Round 233: Mean Global Test Accuracy = 0.9556 | Mean Global Test Loss = 0.1146\n",
      "\n",
      "--- FedAvg Round 234/400 ---\n",
      "Round 234: Mean Global Test Accuracy = 0.9551 | Mean Global Test Loss = 0.1145\n",
      "\n",
      "--- FedAvg Round 235/400 ---\n",
      "Round 234: Mean Global Test Accuracy = 0.9551 | Mean Global Test Loss = 0.1145\n",
      "\n",
      "--- FedAvg Round 235/400 ---\n",
      "Round 235: Mean Global Test Accuracy = 0.9559 | Mean Global Test Loss = 0.1145\n",
      "\n",
      "--- FedAvg Round 236/400 ---\n",
      "Round 235: Mean Global Test Accuracy = 0.9559 | Mean Global Test Loss = 0.1145\n",
      "\n",
      "--- FedAvg Round 236/400 ---\n",
      "Round 236: Mean Global Test Accuracy = 0.9555 | Mean Global Test Loss = 0.1144\n",
      "\n",
      "--- FedAvg Round 237/400 ---\n",
      "Round 236: Mean Global Test Accuracy = 0.9555 | Mean Global Test Loss = 0.1144\n",
      "\n",
      "--- FedAvg Round 237/400 ---\n",
      "Round 237: Mean Global Test Accuracy = 0.9558 | Mean Global Test Loss = 0.1144\n",
      "\n",
      "--- FedAvg Round 238/400 ---\n",
      "Round 237: Mean Global Test Accuracy = 0.9558 | Mean Global Test Loss = 0.1144\n",
      "\n",
      "--- FedAvg Round 238/400 ---\n",
      "Round 238: Mean Global Test Accuracy = 0.9555 | Mean Global Test Loss = 0.1144\n",
      "\n",
      "--- FedAvg Round 239/400 ---\n",
      "Round 238: Mean Global Test Accuracy = 0.9555 | Mean Global Test Loss = 0.1144\n",
      "\n",
      "--- FedAvg Round 239/400 ---\n",
      "Round 239: Mean Global Test Accuracy = 0.9558 | Mean Global Test Loss = 0.1144\n",
      "\n",
      "--- FedAvg Round 240/400 ---\n",
      "Round 239: Mean Global Test Accuracy = 0.9558 | Mean Global Test Loss = 0.1144\n",
      "\n",
      "--- FedAvg Round 240/400 ---\n",
      "Round 240: Mean Global Test Accuracy = 0.9555 | Mean Global Test Loss = 0.1144\n",
      "\n",
      "--- FedAvg Round 241/400 ---\n",
      "Round 240: Mean Global Test Accuracy = 0.9555 | Mean Global Test Loss = 0.1144\n",
      "\n",
      "--- FedAvg Round 241/400 ---\n",
      "Round 241: Mean Global Test Accuracy = 0.9559 | Mean Global Test Loss = 0.1144\n",
      "\n",
      "--- FedAvg Round 242/400 ---\n",
      "Round 241: Mean Global Test Accuracy = 0.9559 | Mean Global Test Loss = 0.1144\n",
      "\n",
      "--- FedAvg Round 242/400 ---\n",
      "Round 242: Mean Global Test Accuracy = 0.9557 | Mean Global Test Loss = 0.1143\n",
      "\n",
      "--- FedAvg Round 243/400 ---\n",
      "Round 242: Mean Global Test Accuracy = 0.9557 | Mean Global Test Loss = 0.1143\n",
      "\n",
      "--- FedAvg Round 243/400 ---\n",
      "Round 243: Mean Global Test Accuracy = 0.9559 | Mean Global Test Loss = 0.1143\n",
      "\n",
      "--- FedAvg Round 244/400 ---\n",
      "Round 243: Mean Global Test Accuracy = 0.9559 | Mean Global Test Loss = 0.1143\n",
      "\n",
      "--- FedAvg Round 244/400 ---\n",
      "Round 244: Mean Global Test Accuracy = 0.9562 | Mean Global Test Loss = 0.1143\n",
      "\n",
      "--- FedAvg Round 245/400 ---\n",
      "Round 244: Mean Global Test Accuracy = 0.9562 | Mean Global Test Loss = 0.1143\n",
      "\n",
      "--- FedAvg Round 245/400 ---\n",
      "Round 245: Mean Global Test Accuracy = 0.9559 | Mean Global Test Loss = 0.1143\n",
      "\n",
      "--- FedAvg Round 246/400 ---\n",
      "Round 245: Mean Global Test Accuracy = 0.9559 | Mean Global Test Loss = 0.1143\n",
      "\n",
      "--- FedAvg Round 246/400 ---\n",
      "Round 246: Mean Global Test Accuracy = 0.9561 | Mean Global Test Loss = 0.1142\n",
      "\n",
      "--- FedAvg Round 247/400 ---\n",
      "Round 246: Mean Global Test Accuracy = 0.9561 | Mean Global Test Loss = 0.1142\n",
      "\n",
      "--- FedAvg Round 247/400 ---\n",
      "Round 247: Mean Global Test Accuracy = 0.9562 | Mean Global Test Loss = 0.1142\n",
      "\n",
      "--- FedAvg Round 248/400 ---\n",
      "Round 247: Mean Global Test Accuracy = 0.9562 | Mean Global Test Loss = 0.1142\n",
      "\n",
      "--- FedAvg Round 248/400 ---\n",
      "Round 248: Mean Global Test Accuracy = 0.9562 | Mean Global Test Loss = 0.1141\n",
      "\n",
      "--- FedAvg Round 249/400 ---\n",
      "Round 248: Mean Global Test Accuracy = 0.9562 | Mean Global Test Loss = 0.1141\n",
      "\n",
      "--- FedAvg Round 249/400 ---\n",
      "Round 249: Mean Global Test Accuracy = 0.9559 | Mean Global Test Loss = 0.1142\n",
      "\n",
      "--- FedAvg Round 250/400 ---\n",
      "Round 249: Mean Global Test Accuracy = 0.9559 | Mean Global Test Loss = 0.1142\n",
      "\n",
      "--- FedAvg Round 250/400 ---\n",
      "Round 250: Mean Global Test Accuracy = 0.9562 | Mean Global Test Loss = 0.1141\n",
      "\n",
      "--- FedAvg Round 251/400 ---\n",
      "Round 250: Mean Global Test Accuracy = 0.9562 | Mean Global Test Loss = 0.1141\n",
      "\n",
      "--- FedAvg Round 251/400 ---\n",
      "Round 251: Mean Global Test Accuracy = 0.9558 | Mean Global Test Loss = 0.1141\n",
      "\n",
      "--- FedAvg Round 252/400 ---\n",
      "Round 251: Mean Global Test Accuracy = 0.9558 | Mean Global Test Loss = 0.1141\n",
      "\n",
      "--- FedAvg Round 252/400 ---\n",
      "Round 252: Mean Global Test Accuracy = 0.9561 | Mean Global Test Loss = 0.1140\n",
      "\n",
      "--- FedAvg Round 253/400 ---\n",
      "Round 252: Mean Global Test Accuracy = 0.9561 | Mean Global Test Loss = 0.1140\n",
      "\n",
      "--- FedAvg Round 253/400 ---\n",
      "Round 253: Mean Global Test Accuracy = 0.9559 | Mean Global Test Loss = 0.1141\n",
      "\n",
      "--- FedAvg Round 254/400 ---\n",
      "Round 253: Mean Global Test Accuracy = 0.9559 | Mean Global Test Loss = 0.1141\n",
      "\n",
      "--- FedAvg Round 254/400 ---\n",
      "Round 254: Mean Global Test Accuracy = 0.9557 | Mean Global Test Loss = 0.1140\n",
      "\n",
      "--- FedAvg Round 255/400 ---\n",
      "Round 254: Mean Global Test Accuracy = 0.9557 | Mean Global Test Loss = 0.1140\n",
      "\n",
      "--- FedAvg Round 255/400 ---\n",
      "Round 255: Mean Global Test Accuracy = 0.9562 | Mean Global Test Loss = 0.1140\n",
      "\n",
      "--- FedAvg Round 256/400 ---\n",
      "Round 255: Mean Global Test Accuracy = 0.9562 | Mean Global Test Loss = 0.1140\n",
      "\n",
      "--- FedAvg Round 256/400 ---\n",
      "Round 256: Mean Global Test Accuracy = 0.9557 | Mean Global Test Loss = 0.1139\n",
      "\n",
      "--- FedAvg Round 257/400 ---\n",
      "Round 256: Mean Global Test Accuracy = 0.9557 | Mean Global Test Loss = 0.1139\n",
      "\n",
      "--- FedAvg Round 257/400 ---\n",
      "Round 257: Mean Global Test Accuracy = 0.9564 | Mean Global Test Loss = 0.1139\n",
      "\n",
      "--- FedAvg Round 258/400 ---\n",
      "Round 257: Mean Global Test Accuracy = 0.9564 | Mean Global Test Loss = 0.1139\n",
      "\n",
      "--- FedAvg Round 258/400 ---\n",
      "Round 258: Mean Global Test Accuracy = 0.9557 | Mean Global Test Loss = 0.1138\n",
      "\n",
      "--- FedAvg Round 259/400 ---\n",
      "Round 258: Mean Global Test Accuracy = 0.9557 | Mean Global Test Loss = 0.1138\n",
      "\n",
      "--- FedAvg Round 259/400 ---\n",
      "Round 259: Mean Global Test Accuracy = 0.9567 | Mean Global Test Loss = 0.1139\n",
      "\n",
      "--- FedAvg Round 260/400 ---\n",
      "Round 259: Mean Global Test Accuracy = 0.9567 | Mean Global Test Loss = 0.1139\n",
      "\n",
      "--- FedAvg Round 260/400 ---\n",
      "Round 260: Mean Global Test Accuracy = 0.9555 | Mean Global Test Loss = 0.1137\n",
      "\n",
      "--- FedAvg Round 261/400 ---\n",
      "Round 260: Mean Global Test Accuracy = 0.9555 | Mean Global Test Loss = 0.1137\n",
      "\n",
      "--- FedAvg Round 261/400 ---\n",
      "Round 261: Mean Global Test Accuracy = 0.9571 | Mean Global Test Loss = 0.1138\n",
      "\n",
      "--- FedAvg Round 262/400 ---\n",
      "Round 261: Mean Global Test Accuracy = 0.9571 | Mean Global Test Loss = 0.1138\n",
      "\n",
      "--- FedAvg Round 262/400 ---\n",
      "Round 262: Mean Global Test Accuracy = 0.9555 | Mean Global Test Loss = 0.1136\n",
      "\n",
      "--- FedAvg Round 263/400 ---\n",
      "Round 262: Mean Global Test Accuracy = 0.9555 | Mean Global Test Loss = 0.1136\n",
      "\n",
      "--- FedAvg Round 263/400 ---\n",
      "Round 263: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1137\n",
      "\n",
      "--- FedAvg Round 264/400 ---\n",
      "Round 263: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1137\n",
      "\n",
      "--- FedAvg Round 264/400 ---\n",
      "Round 264: Mean Global Test Accuracy = 0.9556 | Mean Global Test Loss = 0.1135\n",
      "\n",
      "--- FedAvg Round 265/400 ---\n",
      "Round 264: Mean Global Test Accuracy = 0.9556 | Mean Global Test Loss = 0.1135\n",
      "\n",
      "--- FedAvg Round 265/400 ---\n",
      "Round 265: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1136\n",
      "\n",
      "--- FedAvg Round 266/400 ---\n",
      "Round 265: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1136\n",
      "\n",
      "--- FedAvg Round 266/400 ---\n",
      "Round 266: Mean Global Test Accuracy = 0.9558 | Mean Global Test Loss = 0.1134\n",
      "\n",
      "--- FedAvg Round 267/400 ---\n",
      "Round 266: Mean Global Test Accuracy = 0.9558 | Mean Global Test Loss = 0.1134\n",
      "\n",
      "--- FedAvg Round 267/400 ---\n",
      "Round 267: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1135\n",
      "\n",
      "--- FedAvg Round 268/400 ---\n",
      "Round 267: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1135\n",
      "\n",
      "--- FedAvg Round 268/400 ---\n",
      "Round 268: Mean Global Test Accuracy = 0.9562 | Mean Global Test Loss = 0.1133\n",
      "\n",
      "--- FedAvg Round 269/400 ---\n",
      "Round 268: Mean Global Test Accuracy = 0.9562 | Mean Global Test Loss = 0.1133\n",
      "\n",
      "--- FedAvg Round 269/400 ---\n",
      "Round 269: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1134\n",
      "\n",
      "--- FedAvg Round 270/400 ---\n",
      "Round 269: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1134\n",
      "\n",
      "--- FedAvg Round 270/400 ---\n",
      "Round 270: Mean Global Test Accuracy = 0.9562 | Mean Global Test Loss = 0.1132\n",
      "\n",
      "--- FedAvg Round 271/400 ---\n",
      "Round 270: Mean Global Test Accuracy = 0.9562 | Mean Global Test Loss = 0.1132\n",
      "\n",
      "--- FedAvg Round 271/400 ---\n",
      "Round 271: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1133\n",
      "\n",
      "--- FedAvg Round 272/400 ---\n",
      "Round 271: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1133\n",
      "\n",
      "--- FedAvg Round 272/400 ---\n",
      "Round 272: Mean Global Test Accuracy = 0.9563 | Mean Global Test Loss = 0.1131\n",
      "\n",
      "--- FedAvg Round 273/400 ---\n",
      "Round 272: Mean Global Test Accuracy = 0.9563 | Mean Global Test Loss = 0.1131\n",
      "\n",
      "--- FedAvg Round 273/400 ---\n",
      "Round 273: Mean Global Test Accuracy = 0.9572 | Mean Global Test Loss = 0.1133\n",
      "\n",
      "--- FedAvg Round 274/400 ---\n",
      "Round 273: Mean Global Test Accuracy = 0.9572 | Mean Global Test Loss = 0.1133\n",
      "\n",
      "--- FedAvg Round 274/400 ---\n",
      "Round 274: Mean Global Test Accuracy = 0.9563 | Mean Global Test Loss = 0.1130\n",
      "\n",
      "--- FedAvg Round 275/400 ---\n",
      "Round 274: Mean Global Test Accuracy = 0.9563 | Mean Global Test Loss = 0.1130\n",
      "\n",
      "--- FedAvg Round 275/400 ---\n",
      "Round 275: Mean Global Test Accuracy = 0.9572 | Mean Global Test Loss = 0.1132\n",
      "\n",
      "--- FedAvg Round 276/400 ---\n",
      "Round 275: Mean Global Test Accuracy = 0.9572 | Mean Global Test Loss = 0.1132\n",
      "\n",
      "--- FedAvg Round 276/400 ---\n",
      "Round 276: Mean Global Test Accuracy = 0.9565 | Mean Global Test Loss = 0.1129\n",
      "\n",
      "--- FedAvg Round 277/400 ---\n",
      "Round 276: Mean Global Test Accuracy = 0.9565 | Mean Global Test Loss = 0.1129\n",
      "\n",
      "--- FedAvg Round 277/400 ---\n",
      "Round 277: Mean Global Test Accuracy = 0.9575 | Mean Global Test Loss = 0.1131\n",
      "\n",
      "--- FedAvg Round 278/400 ---\n",
      "Round 277: Mean Global Test Accuracy = 0.9575 | Mean Global Test Loss = 0.1131\n",
      "\n",
      "--- FedAvg Round 278/400 ---\n",
      "Round 278: Mean Global Test Accuracy = 0.9564 | Mean Global Test Loss = 0.1129\n",
      "\n",
      "--- FedAvg Round 279/400 ---\n",
      "Round 278: Mean Global Test Accuracy = 0.9564 | Mean Global Test Loss = 0.1129\n",
      "\n",
      "--- FedAvg Round 279/400 ---\n",
      "Round 279: Mean Global Test Accuracy = 0.9575 | Mean Global Test Loss = 0.1130\n",
      "\n",
      "--- FedAvg Round 280/400 ---\n",
      "Round 279: Mean Global Test Accuracy = 0.9575 | Mean Global Test Loss = 0.1130\n",
      "\n",
      "--- FedAvg Round 280/400 ---\n",
      "Round 280: Mean Global Test Accuracy = 0.9564 | Mean Global Test Loss = 0.1128\n",
      "\n",
      "--- FedAvg Round 281/400 ---\n",
      "Round 280: Mean Global Test Accuracy = 0.9564 | Mean Global Test Loss = 0.1128\n",
      "\n",
      "--- FedAvg Round 281/400 ---\n",
      "Round 281: Mean Global Test Accuracy = 0.9573 | Mean Global Test Loss = 0.1129\n",
      "\n",
      "--- FedAvg Round 282/400 ---\n",
      "Round 281: Mean Global Test Accuracy = 0.9573 | Mean Global Test Loss = 0.1129\n",
      "\n",
      "--- FedAvg Round 282/400 ---\n",
      "Round 282: Mean Global Test Accuracy = 0.9565 | Mean Global Test Loss = 0.1127\n",
      "\n",
      "--- FedAvg Round 283/400 ---\n",
      "Round 282: Mean Global Test Accuracy = 0.9565 | Mean Global Test Loss = 0.1127\n",
      "\n",
      "--- FedAvg Round 283/400 ---\n",
      "Round 283: Mean Global Test Accuracy = 0.9572 | Mean Global Test Loss = 0.1128\n",
      "\n",
      "--- FedAvg Round 284/400 ---\n",
      "Round 283: Mean Global Test Accuracy = 0.9572 | Mean Global Test Loss = 0.1128\n",
      "\n",
      "--- FedAvg Round 284/400 ---\n",
      "Round 284: Mean Global Test Accuracy = 0.9563 | Mean Global Test Loss = 0.1125\n",
      "\n",
      "--- FedAvg Round 285/400 ---\n",
      "Round 284: Mean Global Test Accuracy = 0.9563 | Mean Global Test Loss = 0.1125\n",
      "\n",
      "--- FedAvg Round 285/400 ---\n",
      "Round 285: Mean Global Test Accuracy = 0.9573 | Mean Global Test Loss = 0.1127\n",
      "\n",
      "--- FedAvg Round 286/400 ---\n",
      "Round 285: Mean Global Test Accuracy = 0.9573 | Mean Global Test Loss = 0.1127\n",
      "\n",
      "--- FedAvg Round 286/400 ---\n",
      "Round 286: Mean Global Test Accuracy = 0.9559 | Mean Global Test Loss = 0.1124\n",
      "\n",
      "--- FedAvg Round 287/400 ---\n",
      "Round 286: Mean Global Test Accuracy = 0.9559 | Mean Global Test Loss = 0.1124\n",
      "\n",
      "--- FedAvg Round 287/400 ---\n",
      "Round 287: Mean Global Test Accuracy = 0.9573 | Mean Global Test Loss = 0.1126\n",
      "\n",
      "--- FedAvg Round 288/400 ---\n",
      "Round 287: Mean Global Test Accuracy = 0.9573 | Mean Global Test Loss = 0.1126\n",
      "\n",
      "--- FedAvg Round 288/400 ---\n",
      "Round 288: Mean Global Test Accuracy = 0.9563 | Mean Global Test Loss = 0.1123\n",
      "\n",
      "--- FedAvg Round 289/400 ---\n",
      "Round 288: Mean Global Test Accuracy = 0.9563 | Mean Global Test Loss = 0.1123\n",
      "\n",
      "--- FedAvg Round 289/400 ---\n",
      "Round 289: Mean Global Test Accuracy = 0.9573 | Mean Global Test Loss = 0.1126\n",
      "\n",
      "--- FedAvg Round 290/400 ---\n",
      "Round 289: Mean Global Test Accuracy = 0.9573 | Mean Global Test Loss = 0.1126\n",
      "\n",
      "--- FedAvg Round 290/400 ---\n",
      "Round 290: Mean Global Test Accuracy = 0.9564 | Mean Global Test Loss = 0.1123\n",
      "\n",
      "--- FedAvg Round 291/400 ---\n",
      "Round 290: Mean Global Test Accuracy = 0.9564 | Mean Global Test Loss = 0.1123\n",
      "\n",
      "--- FedAvg Round 291/400 ---\n",
      "Round 291: Mean Global Test Accuracy = 0.9574 | Mean Global Test Loss = 0.1125\n",
      "\n",
      "--- FedAvg Round 292/400 ---\n",
      "Round 291: Mean Global Test Accuracy = 0.9574 | Mean Global Test Loss = 0.1125\n",
      "\n",
      "--- FedAvg Round 292/400 ---\n",
      "Round 292: Mean Global Test Accuracy = 0.9566 | Mean Global Test Loss = 0.1122\n",
      "\n",
      "--- FedAvg Round 293/400 ---\n",
      "Round 292: Mean Global Test Accuracy = 0.9566 | Mean Global Test Loss = 0.1122\n",
      "\n",
      "--- FedAvg Round 293/400 ---\n",
      "Round 293: Mean Global Test Accuracy = 0.9574 | Mean Global Test Loss = 0.1124\n",
      "\n",
      "--- FedAvg Round 294/400 ---\n",
      "Round 293: Mean Global Test Accuracy = 0.9574 | Mean Global Test Loss = 0.1124\n",
      "\n",
      "--- FedAvg Round 294/400 ---\n",
      "Round 294: Mean Global Test Accuracy = 0.9566 | Mean Global Test Loss = 0.1121\n",
      "\n",
      "--- FedAvg Round 295/400 ---\n",
      "Round 294: Mean Global Test Accuracy = 0.9566 | Mean Global Test Loss = 0.1121\n",
      "\n",
      "--- FedAvg Round 295/400 ---\n",
      "Round 295: Mean Global Test Accuracy = 0.9574 | Mean Global Test Loss = 0.1123\n",
      "\n",
      "--- FedAvg Round 296/400 ---\n",
      "Round 295: Mean Global Test Accuracy = 0.9574 | Mean Global Test Loss = 0.1123\n",
      "\n",
      "--- FedAvg Round 296/400 ---\n",
      "Round 296: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1121\n",
      "\n",
      "--- FedAvg Round 297/400 ---\n",
      "Round 296: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1121\n",
      "\n",
      "--- FedAvg Round 297/400 ---\n",
      "Round 297: Mean Global Test Accuracy = 0.9571 | Mean Global Test Loss = 0.1122\n",
      "\n",
      "--- FedAvg Round 298/400 ---\n",
      "Round 297: Mean Global Test Accuracy = 0.9571 | Mean Global Test Loss = 0.1122\n",
      "\n",
      "--- FedAvg Round 298/400 ---\n",
      "Round 298: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1120\n",
      "\n",
      "--- FedAvg Round 299/400 ---\n",
      "Round 298: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1120\n",
      "\n",
      "--- FedAvg Round 299/400 ---\n",
      "Round 299: Mean Global Test Accuracy = 0.9571 | Mean Global Test Loss = 0.1121\n",
      "\n",
      "--- FedAvg Round 300/400 ---\n",
      "Round 299: Mean Global Test Accuracy = 0.9571 | Mean Global Test Loss = 0.1121\n",
      "\n",
      "--- FedAvg Round 300/400 ---\n",
      "Round 300: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1120\n",
      "\n",
      "--- FedAvg Round 301/400 ---\n",
      "Round 300: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1120\n",
      "\n",
      "--- FedAvg Round 301/400 ---\n",
      "Round 301: Mean Global Test Accuracy = 0.9571 | Mean Global Test Loss = 0.1120\n",
      "\n",
      "--- FedAvg Round 302/400 ---\n",
      "Round 301: Mean Global Test Accuracy = 0.9571 | Mean Global Test Loss = 0.1120\n",
      "\n",
      "--- FedAvg Round 302/400 ---\n",
      "Round 302: Mean Global Test Accuracy = 0.9568 | Mean Global Test Loss = 0.1119\n",
      "\n",
      "--- FedAvg Round 303/400 ---\n",
      "Round 302: Mean Global Test Accuracy = 0.9568 | Mean Global Test Loss = 0.1119\n",
      "\n",
      "--- FedAvg Round 303/400 ---\n",
      "Round 303: Mean Global Test Accuracy = 0.9572 | Mean Global Test Loss = 0.1120\n",
      "\n",
      "--- FedAvg Round 304/400 ---\n",
      "Round 303: Mean Global Test Accuracy = 0.9572 | Mean Global Test Loss = 0.1120\n",
      "\n",
      "--- FedAvg Round 304/400 ---\n",
      "Round 304: Mean Global Test Accuracy = 0.9567 | Mean Global Test Loss = 0.1118\n",
      "\n",
      "--- FedAvg Round 305/400 ---\n",
      "Round 304: Mean Global Test Accuracy = 0.9567 | Mean Global Test Loss = 0.1118\n",
      "\n",
      "--- FedAvg Round 305/400 ---\n",
      "Round 305: Mean Global Test Accuracy = 0.9571 | Mean Global Test Loss = 0.1118\n",
      "\n",
      "--- FedAvg Round 306/400 ---\n",
      "Round 305: Mean Global Test Accuracy = 0.9571 | Mean Global Test Loss = 0.1118\n",
      "\n",
      "--- FedAvg Round 306/400 ---\n",
      "Round 306: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 307/400 ---\n",
      "Round 306: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 307/400 ---\n",
      "Round 307: Mean Global Test Accuracy = 0.9572 | Mean Global Test Loss = 0.1118\n",
      "\n",
      "--- FedAvg Round 308/400 ---\n",
      "Round 307: Mean Global Test Accuracy = 0.9572 | Mean Global Test Loss = 0.1118\n",
      "\n",
      "--- FedAvg Round 308/400 ---\n",
      "Round 308: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 309/400 ---\n",
      "Round 308: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 309/400 ---\n",
      "Round 309: Mean Global Test Accuracy = 0.9568 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 310/400 ---\n",
      "Round 309: Mean Global Test Accuracy = 0.9568 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 310/400 ---\n",
      "Round 310: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 311/400 ---\n",
      "Round 310: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 311/400 ---\n",
      "Round 311: Mean Global Test Accuracy = 0.9566 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 312/400 ---\n",
      "Round 311: Mean Global Test Accuracy = 0.9566 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 312/400 ---\n",
      "Round 312: Mean Global Test Accuracy = 0.9568 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 313/400 ---\n",
      "Round 312: Mean Global Test Accuracy = 0.9568 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 313/400 ---\n",
      "Round 313: Mean Global Test Accuracy = 0.9567 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 314/400 ---\n",
      "Round 313: Mean Global Test Accuracy = 0.9567 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 314/400 ---\n",
      "Round 314: Mean Global Test Accuracy = 0.9571 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 315/400 ---\n",
      "Round 314: Mean Global Test Accuracy = 0.9571 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 315/400 ---\n",
      "Round 315: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 316/400 ---\n",
      "Round 315: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 316/400 ---\n",
      "Round 316: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 317/400 ---\n",
      "Round 316: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 317/400 ---\n",
      "Round 317: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 318/400 ---\n",
      "Round 317: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 318/400 ---\n",
      "Round 318: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 319/400 ---\n",
      "Round 318: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 319/400 ---\n",
      "Round 319: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 320/400 ---\n",
      "Round 319: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 320/400 ---\n",
      "Round 320: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 321/400 ---\n",
      "Round 320: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 321/400 ---\n",
      "Round 321: Mean Global Test Accuracy = 0.9565 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 322/400 ---\n",
      "Round 321: Mean Global Test Accuracy = 0.9565 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 322/400 ---\n",
      "Round 322: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 323/400 ---\n",
      "Round 322: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 323/400 ---\n",
      "Round 323: Mean Global Test Accuracy = 0.9565 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 324/400 ---\n",
      "Round 323: Mean Global Test Accuracy = 0.9565 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 324/400 ---\n",
      "Round 324: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 325/400 ---\n",
      "Round 324: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 325/400 ---\n",
      "Round 325: Mean Global Test Accuracy = 0.9566 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 326/400 ---\n",
      "Round 325: Mean Global Test Accuracy = 0.9566 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 326/400 ---\n",
      "Round 326: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 327/400 ---\n",
      "Round 326: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 327/400 ---\n",
      "Round 327: Mean Global Test Accuracy = 0.9567 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 328/400 ---\n",
      "Round 327: Mean Global Test Accuracy = 0.9567 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 328/400 ---\n",
      "Round 328: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 329/400 ---\n",
      "Round 328: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 329/400 ---\n",
      "Round 329: Mean Global Test Accuracy = 0.9568 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 330/400 ---\n",
      "Round 329: Mean Global Test Accuracy = 0.9568 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 330/400 ---\n",
      "Round 330: Mean Global Test Accuracy = 0.9568 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 331/400 ---\n",
      "Round 330: Mean Global Test Accuracy = 0.9568 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 331/400 ---\n",
      "Round 331: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 332/400 ---\n",
      "Round 331: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 332/400 ---\n",
      "Round 332: Mean Global Test Accuracy = 0.9568 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 333/400 ---\n",
      "Round 332: Mean Global Test Accuracy = 0.9568 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 333/400 ---\n",
      "Round 333: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 334/400 ---\n",
      "Round 333: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 334/400 ---\n",
      "Round 334: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 335/400 ---\n",
      "Round 334: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 335/400 ---\n",
      "Round 335: Mean Global Test Accuracy = 0.9571 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 336/400 ---\n",
      "Round 335: Mean Global Test Accuracy = 0.9571 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 336/400 ---\n",
      "Round 336: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 337/400 ---\n",
      "Round 336: Mean Global Test Accuracy = 0.9569 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 337/400 ---\n",
      "Round 337: Mean Global Test Accuracy = 0.9572 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 338/400 ---\n",
      "Round 337: Mean Global Test Accuracy = 0.9572 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 338/400 ---\n",
      "Round 338: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 339/400 ---\n",
      "Round 338: Mean Global Test Accuracy = 0.9570 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 339/400 ---\n",
      "Round 339: Mean Global Test Accuracy = 0.9573 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 340/400 ---\n",
      "Round 339: Mean Global Test Accuracy = 0.9573 | Mean Global Test Loss = 0.1117\n",
      "\n",
      "--- FedAvg Round 340/400 ---\n",
      "Round 340: Mean Global Test Accuracy = 0.9574 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 341/400 ---\n",
      "Round 340: Mean Global Test Accuracy = 0.9574 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 341/400 ---\n",
      "Round 341: Mean Global Test Accuracy = 0.9573 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 342/400 ---\n",
      "Round 341: Mean Global Test Accuracy = 0.9573 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 342/400 ---\n",
      "Round 342: Mean Global Test Accuracy = 0.9576 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 343/400 ---\n",
      "Round 342: Mean Global Test Accuracy = 0.9576 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 343/400 ---\n",
      "Round 343: Mean Global Test Accuracy = 0.9574 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 344/400 ---\n",
      "Round 343: Mean Global Test Accuracy = 0.9574 | Mean Global Test Loss = 0.1116\n",
      "\n",
      "--- FedAvg Round 344/400 ---\n",
      "Round 344: Mean Global Test Accuracy = 0.9577 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 345/400 ---\n",
      "Round 344: Mean Global Test Accuracy = 0.9577 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 345/400 ---\n",
      "Round 345: Mean Global Test Accuracy = 0.9575 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 346/400 ---\n",
      "Round 345: Mean Global Test Accuracy = 0.9575 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 346/400 ---\n",
      "Round 346: Mean Global Test Accuracy = 0.9578 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 347/400 ---\n",
      "Round 346: Mean Global Test Accuracy = 0.9578 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 347/400 ---\n",
      "Round 347: Mean Global Test Accuracy = 0.9575 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 348/400 ---\n",
      "Round 347: Mean Global Test Accuracy = 0.9575 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 348/400 ---\n",
      "Round 348: Mean Global Test Accuracy = 0.9578 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 349/400 ---\n",
      "Round 348: Mean Global Test Accuracy = 0.9578 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 349/400 ---\n",
      "Round 349: Mean Global Test Accuracy = 0.9575 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 350/400 ---\n",
      "Round 349: Mean Global Test Accuracy = 0.9575 | Mean Global Test Loss = 0.1115\n",
      "\n",
      "--- FedAvg Round 350/400 ---\n",
      "Round 350: Mean Global Test Accuracy = 0.9578 | Mean Global Test Loss = 0.1113\n",
      "\n",
      "--- FedAvg Round 351/400 ---\n",
      "Round 350: Mean Global Test Accuracy = 0.9578 | Mean Global Test Loss = 0.1113\n",
      "\n",
      "--- FedAvg Round 351/400 ---\n",
      "Round 351: Mean Global Test Accuracy = 0.9576 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 352/400 ---\n",
      "Round 351: Mean Global Test Accuracy = 0.9576 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 352/400 ---\n",
      "Round 352: Mean Global Test Accuracy = 0.9580 | Mean Global Test Loss = 0.1112\n",
      "\n",
      "--- FedAvg Round 353/400 ---\n",
      "Round 352: Mean Global Test Accuracy = 0.9580 | Mean Global Test Loss = 0.1112\n",
      "\n",
      "--- FedAvg Round 353/400 ---\n",
      "Round 353: Mean Global Test Accuracy = 0.9577 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 354/400 ---\n",
      "Round 353: Mean Global Test Accuracy = 0.9577 | Mean Global Test Loss = 0.1114\n",
      "\n",
      "--- FedAvg Round 354/400 ---\n",
      "Round 354: Mean Global Test Accuracy = 0.9580 | Mean Global Test Loss = 0.1112\n",
      "\n",
      "--- FedAvg Round 355/400 ---\n",
      "Round 354: Mean Global Test Accuracy = 0.9580 | Mean Global Test Loss = 0.1112\n",
      "\n",
      "--- FedAvg Round 355/400 ---\n",
      "Round 355: Mean Global Test Accuracy = 0.9579 | Mean Global Test Loss = 0.1113\n",
      "\n",
      "--- FedAvg Round 356/400 ---\n",
      "Round 355: Mean Global Test Accuracy = 0.9579 | Mean Global Test Loss = 0.1113\n",
      "\n",
      "--- FedAvg Round 356/400 ---\n",
      "Round 356: Mean Global Test Accuracy = 0.9580 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 357/400 ---\n",
      "Round 356: Mean Global Test Accuracy = 0.9580 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 357/400 ---\n",
      "Round 357: Mean Global Test Accuracy = 0.9579 | Mean Global Test Loss = 0.1113\n",
      "\n",
      "--- FedAvg Round 358/400 ---\n",
      "Round 357: Mean Global Test Accuracy = 0.9579 | Mean Global Test Loss = 0.1113\n",
      "\n",
      "--- FedAvg Round 358/400 ---\n",
      "Round 358: Mean Global Test Accuracy = 0.9580 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 359/400 ---\n",
      "Round 358: Mean Global Test Accuracy = 0.9580 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 359/400 ---\n",
      "Round 359: Mean Global Test Accuracy = 0.9577 | Mean Global Test Loss = 0.1113\n",
      "\n",
      "--- FedAvg Round 360/400 ---\n",
      "Round 359: Mean Global Test Accuracy = 0.9577 | Mean Global Test Loss = 0.1113\n",
      "\n",
      "--- FedAvg Round 360/400 ---\n",
      "Round 360: Mean Global Test Accuracy = 0.9580 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 361/400 ---\n",
      "Round 360: Mean Global Test Accuracy = 0.9580 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 361/400 ---\n",
      "Round 361: Mean Global Test Accuracy = 0.9577 | Mean Global Test Loss = 0.1112\n",
      "\n",
      "--- FedAvg Round 362/400 ---\n",
      "Round 361: Mean Global Test Accuracy = 0.9577 | Mean Global Test Loss = 0.1112\n",
      "\n",
      "--- FedAvg Round 362/400 ---\n",
      "Round 362: Mean Global Test Accuracy = 0.9579 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 363/400 ---\n",
      "Round 362: Mean Global Test Accuracy = 0.9579 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 363/400 ---\n",
      "Round 363: Mean Global Test Accuracy = 0.9577 | Mean Global Test Loss = 0.1112\n",
      "\n",
      "--- FedAvg Round 364/400 ---\n",
      "Round 363: Mean Global Test Accuracy = 0.9577 | Mean Global Test Loss = 0.1112\n",
      "\n",
      "--- FedAvg Round 364/400 ---\n",
      "Round 364: Mean Global Test Accuracy = 0.9580 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 365/400 ---\n",
      "Round 364: Mean Global Test Accuracy = 0.9580 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 365/400 ---\n",
      "Round 365: Mean Global Test Accuracy = 0.9577 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 366/400 ---\n",
      "Round 365: Mean Global Test Accuracy = 0.9577 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 366/400 ---\n",
      "Round 366: Mean Global Test Accuracy = 0.9581 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 367/400 ---\n",
      "Round 366: Mean Global Test Accuracy = 0.9581 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 367/400 ---\n",
      "Round 367: Mean Global Test Accuracy = 0.9578 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 368/400 ---\n",
      "Round 367: Mean Global Test Accuracy = 0.9578 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 368/400 ---\n",
      "Round 368: Mean Global Test Accuracy = 0.9581 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 369/400 ---\n",
      "Round 368: Mean Global Test Accuracy = 0.9581 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 369/400 ---\n",
      "Round 369: Mean Global Test Accuracy = 0.9578 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 370/400 ---\n",
      "Round 369: Mean Global Test Accuracy = 0.9578 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 370/400 ---\n",
      "Round 370: Mean Global Test Accuracy = 0.9583 | Mean Global Test Loss = 0.1109\n",
      "\n",
      "--- FedAvg Round 371/400 ---\n",
      "Round 370: Mean Global Test Accuracy = 0.9583 | Mean Global Test Loss = 0.1109\n",
      "\n",
      "--- FedAvg Round 371/400 ---\n",
      "Round 371: Mean Global Test Accuracy = 0.9580 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 372/400 ---\n",
      "Round 371: Mean Global Test Accuracy = 0.9580 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 372/400 ---\n",
      "Round 372: Mean Global Test Accuracy = 0.9583 | Mean Global Test Loss = 0.1109\n",
      "\n",
      "--- FedAvg Round 373/400 ---\n",
      "Round 372: Mean Global Test Accuracy = 0.9583 | Mean Global Test Loss = 0.1109\n",
      "\n",
      "--- FedAvg Round 373/400 ---\n",
      "Round 373: Mean Global Test Accuracy = 0.9578 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 374/400 ---\n",
      "Round 373: Mean Global Test Accuracy = 0.9578 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 374/400 ---\n",
      "Round 374: Mean Global Test Accuracy = 0.9586 | Mean Global Test Loss = 0.1109\n",
      "\n",
      "--- FedAvg Round 375/400 ---\n",
      "Round 374: Mean Global Test Accuracy = 0.9586 | Mean Global Test Loss = 0.1109\n",
      "\n",
      "--- FedAvg Round 375/400 ---\n",
      "Round 375: Mean Global Test Accuracy = 0.9582 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 376/400 ---\n",
      "Round 375: Mean Global Test Accuracy = 0.9582 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 376/400 ---\n",
      "Round 376: Mean Global Test Accuracy = 0.9586 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 377/400 ---\n",
      "Round 376: Mean Global Test Accuracy = 0.9586 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 377/400 ---\n",
      "Round 377: Mean Global Test Accuracy = 0.9582 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 378/400 ---\n",
      "Round 377: Mean Global Test Accuracy = 0.9582 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 378/400 ---\n",
      "Round 378: Mean Global Test Accuracy = 0.9587 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 379/400 ---\n",
      "Round 378: Mean Global Test Accuracy = 0.9587 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 379/400 ---\n",
      "Round 379: Mean Global Test Accuracy = 0.9581 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 380/400 ---\n",
      "Round 379: Mean Global Test Accuracy = 0.9581 | Mean Global Test Loss = 0.1111\n",
      "\n",
      "--- FedAvg Round 380/400 ---\n",
      "Round 380: Mean Global Test Accuracy = 0.9588 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 381/400 ---\n",
      "Round 380: Mean Global Test Accuracy = 0.9588 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 381/400 ---\n",
      "Round 381: Mean Global Test Accuracy = 0.9583 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 382/400 ---\n",
      "Round 381: Mean Global Test Accuracy = 0.9583 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 382/400 ---\n",
      "Round 382: Mean Global Test Accuracy = 0.9588 | Mean Global Test Loss = 0.1109\n",
      "\n",
      "--- FedAvg Round 383/400 ---\n",
      "Round 382: Mean Global Test Accuracy = 0.9588 | Mean Global Test Loss = 0.1109\n",
      "\n",
      "--- FedAvg Round 383/400 ---\n",
      "Round 383: Mean Global Test Accuracy = 0.9582 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 384/400 ---\n",
      "Round 383: Mean Global Test Accuracy = 0.9582 | Mean Global Test Loss = 0.1110\n",
      "\n",
      "--- FedAvg Round 384/400 ---\n",
      "Round 384: Mean Global Test Accuracy = 0.9587 | Mean Global Test Loss = 0.1108\n",
      "\n",
      "--- FedAvg Round 385/400 ---\n",
      "Round 384: Mean Global Test Accuracy = 0.9587 | Mean Global Test Loss = 0.1108\n",
      "\n",
      "--- FedAvg Round 385/400 ---\n",
      "Round 385: Mean Global Test Accuracy = 0.9581 | Mean Global Test Loss = 0.1109\n",
      "\n",
      "--- FedAvg Round 386/400 ---\n",
      "Round 385: Mean Global Test Accuracy = 0.9581 | Mean Global Test Loss = 0.1109\n",
      "\n",
      "--- FedAvg Round 386/400 ---\n",
      "Round 386: Mean Global Test Accuracy = 0.9586 | Mean Global Test Loss = 0.1107\n",
      "\n",
      "--- FedAvg Round 387/400 ---\n",
      "Round 386: Mean Global Test Accuracy = 0.9586 | Mean Global Test Loss = 0.1107\n",
      "\n",
      "--- FedAvg Round 387/400 ---\n",
      "Round 387: Mean Global Test Accuracy = 0.9586 | Mean Global Test Loss = 0.1108\n",
      "\n",
      "--- FedAvg Round 388/400 ---\n",
      "Round 387: Mean Global Test Accuracy = 0.9586 | Mean Global Test Loss = 0.1108\n",
      "\n",
      "--- FedAvg Round 388/400 ---\n",
      "Round 388: Mean Global Test Accuracy = 0.9584 | Mean Global Test Loss = 0.1106\n",
      "\n",
      "--- FedAvg Round 389/400 ---\n",
      "Round 388: Mean Global Test Accuracy = 0.9584 | Mean Global Test Loss = 0.1106\n",
      "\n",
      "--- FedAvg Round 389/400 ---\n",
      "Round 389: Mean Global Test Accuracy = 0.9586 | Mean Global Test Loss = 0.1108\n",
      "\n",
      "--- FedAvg Round 390/400 ---\n",
      "Round 389: Mean Global Test Accuracy = 0.9586 | Mean Global Test Loss = 0.1108\n",
      "\n",
      "--- FedAvg Round 390/400 ---\n",
      "Round 390: Mean Global Test Accuracy = 0.9584 | Mean Global Test Loss = 0.1106\n",
      "\n",
      "--- FedAvg Round 391/400 ---\n",
      "Round 390: Mean Global Test Accuracy = 0.9584 | Mean Global Test Loss = 0.1106\n",
      "\n",
      "--- FedAvg Round 391/400 ---\n",
      "Round 391: Mean Global Test Accuracy = 0.9588 | Mean Global Test Loss = 0.1107\n",
      "\n",
      "--- FedAvg Round 392/400 ---\n",
      "Round 391: Mean Global Test Accuracy = 0.9588 | Mean Global Test Loss = 0.1107\n",
      "\n",
      "--- FedAvg Round 392/400 ---\n",
      "Round 392: Mean Global Test Accuracy = 0.9588 | Mean Global Test Loss = 0.1105\n",
      "\n",
      "--- FedAvg Round 393/400 ---\n",
      "Round 392: Mean Global Test Accuracy = 0.9588 | Mean Global Test Loss = 0.1105\n",
      "\n",
      "--- FedAvg Round 393/400 ---\n",
      "Round 393: Mean Global Test Accuracy = 0.9591 | Mean Global Test Loss = 0.1106\n",
      "\n",
      "--- FedAvg Round 394/400 ---\n",
      "Round 393: Mean Global Test Accuracy = 0.9591 | Mean Global Test Loss = 0.1106\n",
      "\n",
      "--- FedAvg Round 394/400 ---\n",
      "Round 394: Mean Global Test Accuracy = 0.9589 | Mean Global Test Loss = 0.1104\n",
      "\n",
      "--- FedAvg Round 395/400 ---\n",
      "Round 394: Mean Global Test Accuracy = 0.9589 | Mean Global Test Loss = 0.1104\n",
      "\n",
      "--- FedAvg Round 395/400 ---\n",
      "Round 395: Mean Global Test Accuracy = 0.9592 | Mean Global Test Loss = 0.1106\n",
      "\n",
      "--- FedAvg Round 396/400 ---\n",
      "Round 395: Mean Global Test Accuracy = 0.9592 | Mean Global Test Loss = 0.1106\n",
      "\n",
      "--- FedAvg Round 396/400 ---\n",
      "Round 396: Mean Global Test Accuracy = 0.9588 | Mean Global Test Loss = 0.1104\n",
      "\n",
      "--- FedAvg Round 397/400 ---\n",
      "Round 396: Mean Global Test Accuracy = 0.9588 | Mean Global Test Loss = 0.1104\n",
      "\n",
      "--- FedAvg Round 397/400 ---\n",
      "Round 397: Mean Global Test Accuracy = 0.9592 | Mean Global Test Loss = 0.1105\n",
      "\n",
      "--- FedAvg Round 398/400 ---\n",
      "Round 397: Mean Global Test Accuracy = 0.9592 | Mean Global Test Loss = 0.1105\n",
      "\n",
      "--- FedAvg Round 398/400 ---\n",
      "Round 398: Mean Global Test Accuracy = 0.9588 | Mean Global Test Loss = 0.1103\n",
      "\n",
      "--- FedAvg Round 399/400 ---\n",
      "Round 398: Mean Global Test Accuracy = 0.9588 | Mean Global Test Loss = 0.1103\n",
      "\n",
      "--- FedAvg Round 399/400 ---\n",
      "Round 399: Mean Global Test Accuracy = 0.9592 | Mean Global Test Loss = 0.1104\n",
      "\n",
      "--- FedAvg Round 400/400 ---\n",
      "Round 399: Mean Global Test Accuracy = 0.9592 | Mean Global Test Loss = 0.1104\n",
      "\n",
      "--- FedAvg Round 400/400 ---\n",
      "Round 400: Mean Global Test Accuracy = 0.9588 | Mean Global Test Loss = 0.1102\n",
      "Round 400: Mean Global Test Accuracy = 0.9588 | Mean Global Test Loss = 0.1102\n"
     ]
    }
   ],
   "source": [
    "# 7. Vòng lặp huấn luyện liên kết\n",
    "num_rounds = 400\n",
    "local_epochs = 10\n",
    "model_params = {'input_size': input_size, 'hidden_size': 64, 'num_classes': len(df['label'].unique())}\n",
    "\n",
    "# *** SỬA ĐỔI QUAN TRỌNG: Sử dụng quantity_skewed_clients đã được oversample ***\n",
    "clients = []\n",
    "# Chọn bộ dữ liệu đã được cân bằng\n",
    "client_dataset_to_use = quantity_skewed_clients \n",
    "\n",
    "for client_id, df_client in client_dataset_to_use.items():\n",
    "    if len(df_client) > 0:\n",
    "        clients.append(FederatedClient(client_id, df_client, BiLSTM, model_params))\n",
    "\n",
    "server = FederatedServer(BiLSTM, model_params)\n",
    "global_loss_history = []\n",
    "global_acc_history = []\n",
    "criterion_eval = nn.CrossEntropyLoss()\n",
    "\n",
    "for rnd in range(num_rounds):\n",
    "    print(f'\\n--- FedAvg Round {rnd+1}/{num_rounds} ---')\n",
    "    local_weights = []\n",
    "    client_sample_sizes = [len(client.raw_data) for client in clients]\n",
    "    for client in clients:\n",
    "        updated_weights = client.local_train(server.global_weights, local_epochs=local_epochs)\n",
    "        local_weights.append(updated_weights)\n",
    "    server.global_weights = server.aggregate_weights(local_weights, client_sample_sizes=client_sample_sizes)\n",
    "    \n",
    "    # Đánh giá global model trên tất cả client test set\n",
    "    accs = []\n",
    "    losses = []\n",
    "    total_samples = 0\n",
    "    for client in clients:\n",
    "        model = BiLSTM(**model_params).to(client.device)\n",
    "        model.load_state_dict(server.global_weights)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(client.X_test_tensor.unsqueeze(1))\n",
    "            loss = criterion_eval(outputs, client.y_test_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            acc = (predicted == client.y_test_tensor).float().mean().item()\n",
    "            accs.append(acc * len(client.y_test_tensor))\n",
    "            losses.append(loss.item() * len(client.y_test_tensor))\n",
    "            total_samples += len(client.y_test_tensor)\n",
    "\n",
    "    mean_acc = sum(accs) / total_samples\n",
    "    mean_loss = sum(losses) / total_samples\n",
    "    global_acc_history.append(mean_acc)\n",
    "    global_loss_history.append(mean_loss)\n",
    "    print(f'Round {rnd+1}: Mean Global Test Accuracy = {mean_acc:.4f} | Mean Global Test Loss = {mean_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22e08cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Global Model Evaluation ---\n",
      "\n",
      "Overall Accuracy: 0.9587585919600083\n",
      "Overall F1-Score (Macro): 0.9588324843115563\n",
      "\n",
      "Classification Report (Per Class):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "BenignTraffic       1.00      0.99      0.99      1177\n",
      "  Brute_Force       0.81      0.90      0.85      1206\n",
      "         DDoS       0.99      0.98      0.99      1220\n",
      "          DoS       1.00      1.00      1.00      1273\n",
      "        Mirai       1.00      1.00      1.00      1148\n",
      "        Recon       1.00      1.00      1.00      1181\n",
      "     Spoofing       0.89      0.80      0.84      1197\n",
      "    Web-based       1.00      1.00      1.00      1200\n",
      "\n",
      "     accuracy                           0.96      9602\n",
      "    macro avg       0.96      0.96      0.96      9602\n",
      " weighted avg       0.96      0.96      0.96      9602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Đánh giá mô hình toàn cục\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "final_model = BiLSTM(**model_params).to(device)\n",
    "final_model.load_state_dict(server.global_weights)\n",
    "final_model.eval()\n",
    "\n",
    "for client in clients:\n",
    "    with torch.no_grad():\n",
    "        outputs = final_model(client.X_test_tensor.unsqueeze(1))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(client.y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Lấy tên các lớp (class names) từ label_encoder\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "# Tính toán và in báo cáo chi tiết\n",
    "print(\"--- Final Global Model Evaluation ---\")\n",
    "print(\"\\nOverall Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(\"Overall F1-Score (Macro):\", f1_score(all_labels, all_preds, average='macro', zero_division=0))\n",
    "print(\"\\nClassification Report (Per Class):\")\n",
    "# Lấy tên các lớp tương ứng với các nhãn có trong all_labels và all_preds\n",
    "unique_labels_indices = sorted(list(set(all_labels) | set(all_preds)))\n",
    "target_names = [class_names[i] for i in unique_labels_indices]\n",
    "print(classification_report(all_labels, all_preds, target_names=target_names, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284c893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 9. Phân tích & trực quan hóa kết quả\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Biểu đồ Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_rounds + 1), global_acc_history, marker='o', linestyle='-', label='Global Test Accuracy')\n",
    "plt.title('Global Test Accuracy qua các vòng FedAvg')\n",
    "plt.xlabel('FedAvg Round')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Biểu đồ Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_rounds + 1), global_loss_history, marker='x', linestyle='--', color='r', label='Global Test Loss')\n",
    "plt.title('Global Test Loss qua các vòng FedAvg')\n",
    "plt.xlabel('FedAvg Round')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
